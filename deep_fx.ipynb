{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from logging import getLogger, StreamHandler, DEBUG, INFO\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from debug_tools import DebugTools\n",
    "from hist_data import HistData\n",
    "from episode_logger import EpisodeLogger\n",
    "from model_saver import ModelSaver\n",
    "from my_tensor_board import MyTensorBoard\n",
    "from fx_trade import FXTrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepFX:\n",
    "    def __init__(self, env, steps=50000,\n",
    "              log_directory='./logs', model_directory='./models',\n",
    "              model_filename='Keras-RL_DQN_FX_model_meanq{mean_q:e}_episode{episode:05d}',\n",
    "              prepared_model_filename=None,\n",
    "              weights_filename='Keras-RL_DQN_FX_weights.h5',\n",
    "              logger=None):\n",
    "\n",
    "        self._log_directory = log_directory\n",
    "        self._model_directory = model_directory\n",
    "        self._model_filename = model_filename\n",
    "        self._prepared_model_filename = prepared_model_filename\n",
    "        self._weights_filename = weights_filename\n",
    "        self._load_model_path = self._relative_path(model_directory, prepared_model_filename) \n",
    "        self._save_model_path = self._relative_path(model_directory, model_filename)\n",
    "        self._env = env\n",
    "        self.steps = steps\n",
    "        self._logger = logger\n",
    "        \n",
    "\n",
    "    def setup(self):\n",
    "        self._agent, self._model, self._memory, self._policy = self._initialize_agent()\n",
    "        self._agent.compile('adam')\n",
    "        self._logger.info(self._model.summary())\n",
    "\n",
    "    def train(self, is_for_time_measurement=False, wipe_instance_variables_after=True):\n",
    "        self.setup()\n",
    "        self._callbacks = self._get_callbacks()\n",
    "        self._fit(self._agent, is_for_time_measurement, self._env, self._callbacks)\n",
    "        if wipe_instance_variables_after:\n",
    "            self._wipe_instance_variables()\n",
    "\n",
    "    def test(self, episodes, callbacks=[], wipe_instance_variables_after=True):\n",
    "        self.setup()\n",
    "        self._agent.test(self._env, nb_episodes=episodes, visualize=False, callbacks=callbacks)\n",
    "\n",
    "        %matplotlib inline\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        for obs in callbacks[0].rewards.values():\n",
    "            plt.plot([o for o in obs])\n",
    "        plt.xlabel(\"step\")\n",
    "        plt.ylabel(\"reward\")\n",
    "        if wipe_instance_variables_after:\n",
    "            self._wipe_instance_variables()\n",
    "        \n",
    "    def _wipe_instance_variables(self):\n",
    "         self._callbacks, self._agent, self._model, \\\n",
    "                self._memory, self._policy, self.env = [None] * 6\n",
    "        \n",
    "    def _relative_path(self, directory, filename):\n",
    "        if directory is None or filename is None:\n",
    "            return None\n",
    "        return os.path.join(directory, filename)\n",
    "\n",
    "    def _get_model(self, load_model_path, observation_space_shape, nb_actions):\n",
    "        if load_model_path is None:\n",
    "            # DQNのネットワーク定義\n",
    "            model = Sequential()\n",
    "            model.add(Flatten(input_shape=(1,) + observation_space_shape))\n",
    "            #model.add(Flatten(input_shape=observation_space_shape))\n",
    "        #    model.add(Dense(4))\n",
    "        #    model.add(Activation('relu'))\n",
    "        #    model.add(Dense(4))\n",
    "        #    model.add(Activation('relu'))\n",
    "            model.add(Dense(nb_actions))\n",
    "            model.add(Activation('relu'))\n",
    "        else:\n",
    "            model = keras.models.load_model(load_model_path)\n",
    "        return model\n",
    "\n",
    "    def _initialize_agent(self):\n",
    "        nb_actions = self._env.action_space.n\n",
    "        observation_space_shape = self._env.observation_space.shape\n",
    "        model = self._get_model(self._load_model_path, observation_space_shape, nb_actions)\n",
    "        \n",
    "        # experience replay用のmemory\n",
    "        memory = SequentialMemory(limit=500000, window_length=1)\n",
    "        # 行動方策はオーソドックスなepsilon-greedy。ほかに、各行動のQ値によって確率を決定するBoltzmannQPolicyが利用可能\n",
    "        policy = EpsGreedyQPolicy(eps=0.1) \n",
    "        dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "                       policy=policy)\n",
    "                       #target_model_update=1e-2, policy=policy)\n",
    "        #dqn.compile(Adam(lr=1e-3))\n",
    "        return (dqn, model, memory, policy)\n",
    "        \n",
    "    def _get_callbacks(self):\n",
    "        tensor_board_callback = MyTensorBoard(log_dir=self._log_directory, histogram_freq=1, embeddings_layer_names=True, write_graph=True)\n",
    "        model_saver_callback = ModelSaver(self._save_model_path, monitor='mean_q', mode='max')\n",
    "        callbacks = [tensor_board_callback, model_saver_callback]\n",
    "        return callbacks\n",
    "\n",
    "    def _fit(self, agent, is_for_time_measurement, env, callbacks=[]):\n",
    "        if is_for_time_measurement:\n",
    "            start = time.time()\n",
    "            self._logger.info(DebugTools.now_str())\n",
    "            history = agent.fit(env, nb_steps=self.steps, visualize=False, verbose=2, nb_max_episode_steps=None, \\\n",
    "                             callbacks=callbacks)\n",
    "            elapsed_time = time.time() - start\n",
    "            self._logger.warn((\"elapsed_time:{0}\".format(elapsed_time)) + \"[sec]\")\n",
    "            self._logger.info(DebugTools.now_str())\n",
    "        else:\n",
    "            history = agent.fit(env, nb_steps=50000, visualize=True, verbose=2, nb_max_episode_steps=None)\n",
    "        #学習の様子を描画したいときは、Envに_render()を実装して、visualize=True にします,\n",
    "        \n",
    "    def _render(self, mode='human', close=False):\n",
    "        import pdb; pdb.set_trace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
