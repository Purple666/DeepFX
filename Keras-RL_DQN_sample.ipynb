{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Python] Keras-RLで簡単に強化学習(DQN)を試す](http://qiita.com/inoory/items/e63ade6f21766c7c2393)を試す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行時に、以下の警告ログが出た。TensorFlowを高速化できるようである。\n",
    "\n",
    "[How to compile Tensorflow with SSE4.2 and AVX instructions?](http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions)\n",
    "\n",
    "```\n",
    "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "\n",
    "# 直線上を動く点の速度を操作し、目標(原点)に移動させることを目標とする環境\n",
    "class PointOnLine(gym.core.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = gym.spaces.Discrete(3) # 行動空間。速度を下げる、そのまま、上げるの3種\n",
    "\n",
    "        high = np.array([1.0, 1.0]) # 観測空間(state)の次元 (位置と速度の2次元) とそれらの最大値\n",
    "        self.observation_space = gym.spaces.Box(low=-high, high=high) # 最小値は、最大値のマイナスがけ\n",
    "\n",
    "    # 各stepごとに呼ばれる\n",
    "    # actionを受け取り、次のstateとreward、episodeが終了したかどうかを返すように実装\n",
    "    def _step(self, action):\n",
    "        # actionを受け取り、次のstateを決定\n",
    "        dt = 0.1\n",
    "        acc = (action - 1) * 0.1\n",
    "        self._vel += acc * dt\n",
    "        self._vel = max(-1.0,  min(self._vel, 1.0))\n",
    "        self._pos += self._vel * dt\n",
    "        self._pos = max(-1.0,  min(self._pos, 1.0))\n",
    "\n",
    "        # 位置と速度の絶対値が十分小さくなったらepisode終了\n",
    "        done = abs(self._pos) < 0.1 and abs(self._vel) < 0.1\n",
    "\n",
    "        if done:\n",
    "            # 終了したときに正の報酬\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            # 時間経過ごとに負の報酬\n",
    "            # ゴールに近づくように、距離が近くなるほど絶対値を減らしておくと、学習が早く進む\n",
    "            reward = -0.01 * abs(self._pos)\n",
    "\n",
    "        # 次のstate、reward、終了したかどうか、追加情報の順に返す\n",
    "        # 追加情報は特にないので空dict\n",
    "        return np.array([self._pos, self._vel]), reward, done, {}\n",
    "\n",
    "    # 各episodeの開始時に呼ばれ、初期stateを返すように実装\n",
    "    def _reset(self):\n",
    "        # 初期stateは、位置はランダム、速度ゼロ\n",
    "        self._pos = np.random.rand()*2 - 1\n",
    "        self._vel = 0.0\n",
    "        return np.array([self._pos, self._vel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 643.0\n",
      "Trainable params: 643.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 50000 steps ...\n",
      "   300/50000: episode: 1, duration: 1.058s, episode steps: 300, steps per second: 284, episode reward: -2.711, mean reward: -0.009 [-0.010, -0.000], mean action: 0.933 [0.000, 2.000], mean observation: -0.489 [-1.000, 0.983], loss: 0.000264, mean_absolute_error: 0.070288, mean_q: -0.044590\n",
      "   600/50000: episode: 2, duration: 1.125s, episode steps: 300, steps per second: 267, episode reward: -1.191, mean reward: -0.004 [-0.004, -0.003], mean action: 1.000 [0.000, 2.000], mean observation: 0.200 [-0.050, 0.446], loss: 0.000019, mean_absolute_error: 0.054933, mean_q: -0.055805\n",
      "   900/50000: episode: 3, duration: 0.946s, episode steps: 300, steps per second: 317, episode reward: -2.656, mean reward: -0.009 [-0.010, -0.000], mean action: 0.990 [0.000, 2.000], mean observation: 0.526 [-0.793, 1.000], loss: 0.000042, mean_absolute_error: 0.044402, mean_q: -0.026780\n",
      "   937/50000: episode: 4, duration: 0.120s, episode steps: 37, steps per second: 308, episode reward: 0.960, mean reward: 0.026 [-0.001, 1.000], mean action: 0.865 [0.000, 1.000], mean observation: 0.053 [-0.050, 0.111], loss: 0.000069, mean_absolute_error: 0.044820, mean_q: -0.018140\n",
      "  1237/50000: episode: 5, duration: 0.945s, episode steps: 300, steps per second: 317, episode reward: -2.180, mean reward: -0.007 [-0.010, -0.000], mean action: 0.987 [0.000, 2.000], mean observation: -0.247 [-1.000, 0.829], loss: 0.000536, mean_absolute_error: 0.047604, mean_q: -0.019380\n",
      "  1537/50000: episode: 6, duration: 0.973s, episode steps: 300, steps per second: 308, episode reward: -1.754, mean reward: -0.006 [-0.010, -0.000], mean action: 0.933 [0.000, 2.000], mean observation: -0.271 [-1.000, 0.487], loss: 0.000113, mean_absolute_error: 0.056382, mean_q: -0.039386\n",
      "  1837/50000: episode: 7, duration: 0.968s, episode steps: 300, steps per second: 310, episode reward: -2.138, mean reward: -0.007 [-0.010, -0.000], mean action: 1.397 [0.000, 2.000], mean observation: 0.249 [-0.990, 1.000], loss: 0.000156, mean_absolute_error: 0.059749, mean_q: -0.052744\n",
      "  2137/50000: episode: 8, duration: 0.990s, episode steps: 300, steps per second: 303, episode reward: -1.755, mean reward: -0.006 [-0.010, -0.000], mean action: 0.987 [0.000, 2.000], mean observation: -0.288 [-1.000, 0.300], loss: 0.000265, mean_absolute_error: 0.065273, mean_q: -0.060104\n",
      "  2437/50000: episode: 9, duration: 1.012s, episode steps: 300, steps per second: 297, episode reward: -2.138, mean reward: -0.007 [-0.010, -0.000], mean action: 1.267 [0.000, 2.000], mean observation: 0.068 [-1.000, 1.000], loss: 0.000161, mean_absolute_error: 0.072838, mean_q: -0.071967\n",
      "  2737/50000: episode: 10, duration: 0.987s, episode steps: 300, steps per second: 304, episode reward: -2.265, mean reward: -0.008 [-0.010, -0.000], mean action: 1.270 [0.000, 2.000], mean observation: 0.187 [-1.000, 1.000], loss: 0.000207, mean_absolute_error: 0.079385, mean_q: -0.079373\n",
      "  3037/50000: episode: 11, duration: 1.075s, episode steps: 300, steps per second: 279, episode reward: -1.020, mean reward: -0.003 [-0.005, -0.002], mean action: 0.970 [0.000, 2.000], mean observation: -0.170 [-0.534, 0.040], loss: 0.000206, mean_absolute_error: 0.085426, mean_q: -0.087153\n",
      "  3337/50000: episode: 12, duration: 1.434s, episode steps: 300, steps per second: 209, episode reward: -2.151, mean reward: -0.007 [-0.010, -0.000], mean action: 1.240 [0.000, 2.000], mean observation: 0.202 [-1.000, 1.000], loss: 0.000119, mean_absolute_error: 0.090002, mean_q: -0.093922\n",
      "  3338/50000: episode: 13, duration: 0.007s, episode steps: 1, steps per second: 142, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.030 [-0.050, -0.010], loss: 0.000014, mean_absolute_error: 0.093867, mean_q: -0.068678\n",
      "  3359/50000: episode: 14, duration: 0.097s, episode steps: 21, steps per second: 217, episode reward: 0.974, mean reward: 0.046 [-0.002, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.025 [-0.110, 0.190], loss: 0.000031, mean_absolute_error: 0.089250, mean_q: -0.080764\n",
      "  3427/50000: episode: 15, duration: 0.239s, episode steps: 68, steps per second: 285, episode reward: 0.786, mean reward: 0.012 [-0.004, 1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.132 [-0.412, 0.180], loss: 0.000808, mean_absolute_error: 0.098769, mean_q: -0.099593\n",
      "  3428/50000: episode: 16, duration: 0.006s, episode steps: 1, steps per second: 157, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.018 [-0.027, -0.010], loss: 0.000124, mean_absolute_error: 0.081756, mean_q: -0.064544\n",
      "  3728/50000: episode: 17, duration: 1.340s, episode steps: 300, steps per second: 224, episode reward: -0.737, mean reward: -0.002 [-0.004, -0.000], mean action: 1.017 [0.000, 2.000], mean observation: -0.103 [-0.365, 0.329], loss: 0.000353, mean_absolute_error: 0.096608, mean_q: -0.090748\n",
      "  4028/50000: episode: 18, duration: 0.974s, episode steps: 300, steps per second: 308, episode reward: -0.496, mean reward: -0.002 [-0.003, -0.001], mean action: 0.973 [0.000, 2.000], mean observation: -0.081 [-0.318, 0.080], loss: 0.000276, mean_absolute_error: 0.096288, mean_q: -0.082698\n",
      "  4029/50000: episode: 19, duration: 0.006s, episode steps: 1, steps per second: 166, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.008 [-0.010, 0.025], loss: 0.000052, mean_absolute_error: 0.100364, mean_q: -0.106289\n",
      "  4079/50000: episode: 20, duration: 0.152s, episode steps: 50, steps per second: 329, episode reward: 0.899, mean reward: 0.018 [-0.004, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.062 [-0.130, 0.360], loss: 0.000408, mean_absolute_error: 0.099612, mean_q: -0.072961\n",
      "  4183/50000: episode: 21, duration: 0.379s, episode steps: 104, steps per second: 274, episode reward: 0.833, mean reward: 0.008 [-0.003, 1.000], mean action: 1.019 [0.000, 2.000], mean observation: -0.069 [-0.336, 0.140], loss: 0.000445, mean_absolute_error: 0.098940, mean_q: -0.063473\n",
      "  4246/50000: episode: 22, duration: 0.198s, episode steps: 63, steps per second: 318, episode reward: 0.827, mean reward: 0.013 [-0.005, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.094 [-0.140, 0.538], loss: 0.000712, mean_absolute_error: 0.101273, mean_q: -0.056764\n",
      "  4310/50000: episode: 23, duration: 0.198s, episode steps: 64, steps per second: 324, episode reward: 0.811, mean reward: 0.013 [-0.006, 1.000], mean action: 1.062 [0.000, 2.000], mean observation: -0.112 [-0.566, 0.180], loss: 0.001388, mean_absolute_error: 0.110602, mean_q: -0.053526\n",
      "  4465/50000: episode: 24, duration: 0.536s, episode steps: 155, steps per second: 289, episode reward: 0.690, mean reward: 0.004 [-0.006, 1.000], mean action: 1.006 [0.000, 2.000], mean observation: -0.085 [-0.585, 0.160], loss: 0.000575, mean_absolute_error: 0.110100, mean_q: -0.046626\n",
      "  4545/50000: episode: 25, duration: 0.323s, episode steps: 80, steps per second: 248, episode reward: 0.623, mean reward: 0.008 [-0.009, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.188 [-0.120, 0.865], loss: 0.000404, mean_absolute_error: 0.110910, mean_q: -0.031423\n",
      "  4636/50000: episode: 26, duration: 0.296s, episode steps: 91, steps per second: 308, episode reward: 0.596, mean reward: 0.007 [-0.008, 1.000], mean action: 0.912 [0.000, 2.000], mean observation: 0.182 [-0.130, 0.824], loss: 0.000646, mean_absolute_error: 0.116738, mean_q: -0.029581\n",
      "  4685/50000: episode: 27, duration: 0.211s, episode steps: 49, steps per second: 232, episode reward: 0.886, mean reward: 0.018 [-0.004, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.090 [-0.090, 0.359], loss: 0.000616, mean_absolute_error: 0.114541, mean_q: -0.015406\n",
      "  4727/50000: episode: 28, duration: 0.211s, episode steps: 42, steps per second: 199, episode reward: 0.885, mean reward: 0.021 [-0.004, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.097 [-0.130, 0.440], loss: 0.000723, mean_absolute_error: 0.120865, mean_q: -0.018439\n",
      "  4813/50000: episode: 29, duration: 0.408s, episode steps: 86, steps per second: 211, episode reward: 0.881, mean reward: 0.010 [-0.003, 1.000], mean action: 1.012 [0.000, 2.000], mean observation: -0.059 [-0.290, 0.110], loss: 0.000514, mean_absolute_error: 0.119837, mean_q: -0.010803\n",
      "  4814/50000: episode: 30, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.022 [-0.034, -0.010], loss: 0.000050, mean_absolute_error: 0.123873, mean_q: -0.060511\n",
      "  4879/50000: episode: 31, duration: 0.284s, episode steps: 65, steps per second: 229, episode reward: 0.662, mean reward: 0.010 [-0.010, 1.000], mean action: 1.046 [0.000, 2.000], mean observation: -0.194 [-0.969, 0.230], loss: 0.000510, mean_absolute_error: 0.125222, mean_q: -0.007074\n",
      "  5072/50000: episode: 32, duration: 0.686s, episode steps: 193, steps per second: 281, episode reward: 0.590, mean reward: 0.003 [-0.008, 1.000], mean action: 1.005 [0.000, 2.000], mean observation: -0.089 [-0.752, 0.190], loss: 0.000519, mean_absolute_error: 0.132080, mean_q: 0.004418\n",
      "  5123/50000: episode: 33, duration: 0.149s, episode steps: 51, steps per second: 342, episode reward: 0.944, mean reward: 0.019 [-0.001, 1.000], mean action: 1.020 [0.000, 2.000], mean observation: -0.051 [-0.143, 0.050], loss: 0.000871, mean_absolute_error: 0.138149, mean_q: 0.025625\n",
      "  5215/50000: episode: 34, duration: 0.302s, episode steps: 92, steps per second: 305, episode reward: 0.770, mean reward: 0.008 [-0.005, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.095 [-0.140, 0.503], loss: 0.001096, mean_absolute_error: 0.146473, mean_q: 0.031628\n",
      "  5279/50000: episode: 35, duration: 0.267s, episode steps: 64, steps per second: 240, episode reward: 0.837, mean reward: 0.013 [-0.004, 1.000], mean action: 0.859 [0.000, 2.000], mean observation: 0.099 [-0.120, 0.379], loss: 0.000560, mean_absolute_error: 0.147951, mean_q: 0.046905\n",
      "  5339/50000: episode: 36, duration: 0.177s, episode steps: 60, steps per second: 339, episode reward: 0.857, mean reward: 0.014 [-0.004, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.088 [-0.110, 0.406], loss: 0.000837, mean_absolute_error: 0.151921, mean_q: 0.047495\n",
      "  5639/50000: episode: 37, duration: 1.041s, episode steps: 300, steps per second: 288, episode reward: -2.497, mean reward: -0.008 [-0.010, -0.000], mean action: 0.897 [0.000, 2.000], mean observation: 0.404 [-0.816, 1.000], loss: 0.000398, mean_absolute_error: 0.157804, mean_q: 0.053500\n",
      "  5716/50000: episode: 38, duration: 0.234s, episode steps: 77, steps per second: 329, episode reward: 0.656, mean reward: 0.009 [-0.009, 1.000], mean action: 1.026 [0.000, 2.000], mean observation: -0.171 [-0.912, 0.220], loss: 0.000358, mean_absolute_error: 0.164225, mean_q: 0.052646\n",
      "  5782/50000: episode: 39, duration: 0.212s, episode steps: 66, steps per second: 312, episode reward: 0.707, mean reward: 0.011 [-0.008, 1.000], mean action: 1.030 [0.000, 2.000], mean observation: -0.168 [-0.820, 0.190], loss: 0.000290, mean_absolute_error: 0.166750, mean_q: 0.055044\n",
      "  5812/50000: episode: 40, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 0.941, mean reward: 0.031 [-0.003, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.067 [-0.303, 0.120], loss: 0.000182, mean_absolute_error: 0.169451, mean_q: 0.059078\n",
      "  5861/50000: episode: 41, duration: 0.263s, episode steps: 49, steps per second: 186, episode reward: 0.842, mean reward: 0.017 [-0.006, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.115 [-0.160, 0.561], loss: 0.000240, mean_absolute_error: 0.176545, mean_q: 0.078251\n",
      "  5905/50000: episode: 42, duration: 0.176s, episode steps: 44, steps per second: 250, episode reward: 0.897, mean reward: 0.020 [-0.004, 1.000], mean action: 1.045 [0.000, 2.000], mean observation: -0.081 [-0.419, 0.140], loss: 0.000224, mean_absolute_error: 0.172529, mean_q: 0.071742\n",
      "  5964/50000: episode: 43, duration: 0.246s, episode steps: 59, steps per second: 240, episode reward: 0.820, mean reward: 0.014 [-0.006, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.113 [-0.150, 0.557], loss: 0.000307, mean_absolute_error: 0.176618, mean_q: 0.076165\n",
      "  6000/50000: episode: 44, duration: 0.153s, episode steps: 36, steps per second: 235, episode reward: 0.932, mean reward: 0.026 [-0.003, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.057 [-0.120, 0.302], loss: 0.000465, mean_absolute_error: 0.176503, mean_q: 0.086326\n",
      "  6035/50000: episode: 45, duration: 0.138s, episode steps: 35, steps per second: 254, episode reward: 0.908, mean reward: 0.026 [-0.004, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.086 [-0.421, 0.140], loss: 0.000329, mean_absolute_error: 0.182546, mean_q: 0.101031\n",
      "  6095/50000: episode: 46, duration: 0.211s, episode steps: 60, steps per second: 284, episode reward: 0.750, mean reward: 0.012 [-0.007, 1.000], mean action: 1.033 [0.000, 2.000], mean observation: -0.156 [-0.738, 0.190], loss: 0.000458, mean_absolute_error: 0.184948, mean_q: 0.102004\n",
      "  6147/50000: episode: 47, duration: 0.168s, episode steps: 52, steps per second: 310, episode reward: 0.739, mean reward: 0.014 [-0.009, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.171 [-0.270, 0.885], loss: 0.000451, mean_absolute_error: 0.188493, mean_q: 0.107712\n",
      "  6148/50000: episode: 48, duration: 0.007s, episode steps: 1, steps per second: 136, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.032 [-0.074, 0.010], loss: 0.000056, mean_absolute_error: 0.190664, mean_q: 0.090201\n",
      "  6208/50000: episode: 49, duration: 0.304s, episode steps: 60, steps per second: 197, episode reward: 0.789, mean reward: 0.013 [-0.006, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.132 [-0.150, 0.631], loss: 0.000457, mean_absolute_error: 0.193324, mean_q: 0.112260\n",
      "  6209/50000: episode: 50, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.040 [0.000, 0.079], loss: 0.000088, mean_absolute_error: 0.179390, mean_q: 0.049528\n",
      "  6284/50000: episode: 51, duration: 0.281s, episode steps: 75, steps per second: 267, episode reward: 0.666, mean reward: 0.009 [-0.008, 1.000], mean action: 1.040 [0.000, 2.000], mean observation: -0.177 [-0.798, 0.160], loss: 0.000300, mean_absolute_error: 0.197108, mean_q: 0.121669\n",
      "  6326/50000: episode: 52, duration: 0.136s, episode steps: 42, steps per second: 308, episode reward: 0.895, mean reward: 0.021 [-0.004, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.092 [-0.100, 0.374], loss: 0.000538, mean_absolute_error: 0.205101, mean_q: 0.139586\n",
      "  6368/50000: episode: 53, duration: 0.145s, episode steps: 42, steps per second: 290, episode reward: 0.845, mean reward: 0.020 [-0.006, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.121 [-0.230, 0.622], loss: 0.000250, mean_absolute_error: 0.197712, mean_q: 0.136423\n",
      "  6421/50000: episode: 54, duration: 0.179s, episode steps: 53, steps per second: 296, episode reward: 0.814, mean reward: 0.015 [-0.006, 1.000], mean action: 0.868 [0.000, 2.000], mean observation: 0.125 [-0.200, 0.641], loss: 0.000365, mean_absolute_error: 0.210421, mean_q: 0.155601\n",
      "  6466/50000: episode: 55, duration: 0.158s, episode steps: 45, steps per second: 284, episode reward: 0.889, mean reward: 0.020 [-0.004, 1.000], mean action: 0.844 [0.000, 2.000], mean observation: 0.094 [-0.080, 0.372], loss: 0.000253, mean_absolute_error: 0.207890, mean_q: 0.142783\n",
      "  6482/50000: episode: 56, duration: 0.051s, episode steps: 16, steps per second: 315, episode reward: 0.978, mean reward: 0.061 [-0.002, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.044 [-0.182, 0.090], loss: 0.000214, mean_absolute_error: 0.208550, mean_q: 0.150419\n",
      "  6523/50000: episode: 57, duration: 0.129s, episode steps: 41, steps per second: 318, episode reward: 0.906, mean reward: 0.022 [-0.004, 1.000], mean action: 0.805 [0.000, 2.000], mean observation: 0.085 [-0.080, 0.353], loss: 0.000381, mean_absolute_error: 0.208547, mean_q: 0.141416\n",
      "  6550/50000: episode: 58, duration: 0.093s, episode steps: 27, steps per second: 290, episode reward: 0.945, mean reward: 0.035 [-0.003, 1.000], mean action: 0.704 [0.000, 2.000], mean observation: 0.066 [-0.110, 0.301], loss: 0.000216, mean_absolute_error: 0.223358, mean_q: 0.154096\n",
      "  6571/50000: episode: 59, duration: 0.062s, episode steps: 21, steps per second: 338, episode reward: 0.967, mean reward: 0.046 [-0.002, 1.000], mean action: 1.238 [0.000, 2.000], mean observation: -0.051 [-0.226, 0.100], loss: 0.000201, mean_absolute_error: 0.211782, mean_q: 0.185018\n",
      "  6587/50000: episode: 60, duration: 0.050s, episode steps: 16, steps per second: 323, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.047 [-0.187, 0.090], loss: 0.000175, mean_absolute_error: 0.214552, mean_q: 0.177193\n",
      "  6619/50000: episode: 61, duration: 0.101s, episode steps: 32, steps per second: 318, episode reward: 0.933, mean reward: 0.029 [-0.003, 1.000], mean action: 1.156 [0.000, 2.000], mean observation: -0.069 [-0.329, 0.120], loss: 0.000158, mean_absolute_error: 0.225311, mean_q: 0.167615\n",
      "  6664/50000: episode: 62, duration: 0.148s, episode steps: 45, steps per second: 304, episode reward: 0.846, mean reward: 0.019 [-0.006, 1.000], mean action: 1.178 [0.000, 2.000], mean observation: -0.121 [-0.553, 0.160], loss: 0.000274, mean_absolute_error: 0.227325, mean_q: 0.168739\n",
      "  6704/50000: episode: 63, duration: 0.138s, episode steps: 40, steps per second: 289, episode reward: 0.903, mean reward: 0.023 [-0.004, 1.000], mean action: 0.825 [0.000, 2.000], mean observation: 0.089 [-0.100, 0.371], loss: 0.000166, mean_absolute_error: 0.233521, mean_q: 0.188482\n",
      "  6716/50000: episode: 64, duration: 0.037s, episode steps: 12, steps per second: 325, episode reward: 0.988, mean reward: 0.082 [-0.001, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.048 [-0.060, 0.117], loss: 0.000227, mean_absolute_error: 0.225535, mean_q: 0.178209\n",
      "  6723/50000: episode: 65, duration: 0.022s, episode steps: 7, steps per second: 311, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.040 [-0.120, 0.050], loss: 0.000097, mean_absolute_error: 0.238554, mean_q: 0.216983\n",
      "  6754/50000: episode: 66, duration: 0.092s, episode steps: 31, steps per second: 338, episode reward: 0.935, mean reward: 0.030 [-0.003, 1.000], mean action: 0.806 [0.000, 2.000], mean observation: 0.069 [-0.110, 0.326], loss: 0.000145, mean_absolute_error: 0.232740, mean_q: 0.204805\n",
      "  6773/50000: episode: 67, duration: 0.061s, episode steps: 19, steps per second: 313, episode reward: 0.970, mean reward: 0.051 [-0.002, 1.000], mean action: 1.263 [0.000, 2.000], mean observation: -0.051 [-0.217, 0.110], loss: 0.000132, mean_absolute_error: 0.228613, mean_q: 0.167821\n",
      "  6820/50000: episode: 68, duration: 0.145s, episode steps: 47, steps per second: 325, episode reward: 0.846, mean reward: 0.018 [-0.005, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.117 [-0.547, 0.160], loss: 0.000237, mean_absolute_error: 0.237785, mean_q: 0.188499\n",
      "  6821/50000: episode: 69, duration: 0.006s, episode steps: 1, steps per second: 172, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.048 [0.000, 0.096], loss: 0.000053, mean_absolute_error: 0.196143, mean_q: 0.098137\n",
      "  6893/50000: episode: 70, duration: 0.240s, episode steps: 72, steps per second: 301, episode reward: 0.661, mean reward: 0.009 [-0.009, 1.000], mean action: 1.097 [0.000, 2.000], mean observation: -0.181 [-0.892, 0.200], loss: 0.000219, mean_absolute_error: 0.243046, mean_q: 0.209300\n",
      "  6894/50000: episode: 71, duration: 0.006s, episode steps: 1, steps per second: 164, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [-0.010, 0.074], loss: 0.000050, mean_absolute_error: 0.208262, mean_q: 0.222823\n",
      "  6917/50000: episode: 72, duration: 0.083s, episode steps: 23, steps per second: 278, episode reward: 0.961, mean reward: 0.042 [-0.002, 1.000], mean action: 1.261 [0.000, 2.000], mean observation: -0.060 [-0.223, 0.100], loss: 0.000135, mean_absolute_error: 0.246032, mean_q: 0.222388\n",
      "  6943/50000: episode: 73, duration: 0.081s, episode steps: 26, steps per second: 321, episode reward: 0.951, mean reward: 0.037 [-0.003, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.062 [-0.090, 0.276], loss: 0.000120, mean_absolute_error: 0.239187, mean_q: 0.189964\n",
      "  7000/50000: episode: 74, duration: 0.188s, episode steps: 57, steps per second: 304, episode reward: 0.768, mean reward: 0.013 [-0.007, 1.000], mean action: 1.088 [0.000, 2.000], mean observation: -0.148 [-0.736, 0.190], loss: 0.000076, mean_absolute_error: 0.251915, mean_q: 0.227871\n",
      "  7015/50000: episode: 75, duration: 0.045s, episode steps: 15, steps per second: 331, episode reward: 0.980, mean reward: 0.065 [-0.002, 1.000], mean action: 0.533 [0.000, 2.000], mean observation: 0.047 [-0.080, 0.169], loss: 0.000088, mean_absolute_error: 0.252858, mean_q: 0.211705\n",
      "  7029/50000: episode: 76, duration: 0.043s, episode steps: 14, steps per second: 327, episode reward: 0.982, mean reward: 0.070 [-0.002, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.042 [-0.080, 0.170], loss: 0.000543, mean_absolute_error: 0.246862, mean_q: 0.206633\n",
      "  7098/50000: episode: 77, duration: 0.223s, episode steps: 69, steps per second: 309, episode reward: 0.606, mean reward: 0.009 [-0.010, 1.000], mean action: 0.913 [0.000, 2.000], mean observation: 0.223 [-0.240, 0.968], loss: 0.000138, mean_absolute_error: 0.254278, mean_q: 0.238034\n",
      "  7099/50000: episode: 78, duration: 0.007s, episode steps: 1, steps per second: 153, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.042 [-0.094, 0.010], loss: 0.000066, mean_absolute_error: 0.288579, mean_q: 0.295574\n",
      "  7159/50000: episode: 79, duration: 0.184s, episode steps: 60, steps per second: 326, episode reward: 0.764, mean reward: 0.013 [-0.007, 1.000], mean action: 1.083 [0.000, 2.000], mean observation: -0.146 [-0.710, 0.180], loss: 0.000087, mean_absolute_error: 0.261645, mean_q: 0.235364\n",
      "  7162/50000: episode: 80, duration: 0.014s, episode steps: 3, steps per second: 222, episode reward: 0.998, mean reward: 0.333 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.040 [-0.103, 0.030], loss: 0.000080, mean_absolute_error: 0.249433, mean_q: 0.180618\n",
      "  7208/50000: episode: 81, duration: 0.148s, episode steps: 46, steps per second: 310, episode reward: 0.858, mean reward: 0.019 [-0.005, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.103 [-0.521, 0.160], loss: 0.000263, mean_absolute_error: 0.259751, mean_q: 0.253100\n",
      "  7261/50000: episode: 82, duration: 0.178s, episode steps: 53, steps per second: 298, episode reward: 0.800, mean reward: 0.015 [-0.007, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.136 [-0.200, 0.665], loss: 0.000116, mean_absolute_error: 0.265850, mean_q: 0.244344\n",
      "  7325/50000: episode: 83, duration: 0.198s, episode steps: 64, steps per second: 324, episode reward: 0.713, mean reward: 0.011 [-0.008, 1.000], mean action: 0.922 [0.000, 2.000], mean observation: 0.169 [-0.200, 0.808], loss: 0.000247, mean_absolute_error: 0.270075, mean_q: 0.271634\n",
      "  7387/50000: episode: 84, duration: 0.196s, episode steps: 62, steps per second: 317, episode reward: 0.659, mean reward: 0.011 [-0.009, 1.000], mean action: 0.871 [0.000, 2.000], mean observation: 0.213 [-0.230, 0.868], loss: 0.000091, mean_absolute_error: 0.275950, mean_q: 0.283193\n",
      "  7408/50000: episode: 85, duration: 0.062s, episode steps: 21, steps per second: 338, episode reward: 0.963, mean reward: 0.046 [-0.002, 1.000], mean action: 1.381 [0.000, 2.000], mean observation: -0.054 [-0.247, 0.100], loss: 0.000665, mean_absolute_error: 0.276777, mean_q: 0.292977\n",
      "  7473/50000: episode: 86, duration: 0.208s, episode steps: 65, steps per second: 312, episode reward: 0.661, mean reward: 0.010 [-0.009, 1.000], mean action: 1.108 [0.000, 2.000], mean observation: -0.197 [-0.930, 0.180], loss: 0.000179, mean_absolute_error: 0.275860, mean_q: 0.272509\n",
      "  7542/50000: episode: 87, duration: 0.198s, episode steps: 69, steps per second: 348, episode reward: 0.659, mean reward: 0.010 [-0.009, 1.000], mean action: 1.058 [0.000, 2.000], mean observation: -0.187 [-0.927, 0.190], loss: 0.000079, mean_absolute_error: 0.276432, mean_q: 0.265885\n",
      "  7611/50000: episode: 88, duration: 0.221s, episode steps: 69, steps per second: 312, episode reward: 0.720, mean reward: 0.010 [-0.007, 1.000], mean action: 0.899 [0.000, 2.000], mean observation: 0.159 [-0.120, 0.705], loss: 0.000291, mean_absolute_error: 0.291938, mean_q: 0.302469\n",
      "  7667/50000: episode: 89, duration: 0.162s, episode steps: 56, steps per second: 346, episode reward: 0.764, mean reward: 0.014 [-0.007, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.153 [-0.746, 0.180], loss: 0.000074, mean_absolute_error: 0.285582, mean_q: 0.290193\n",
      "  7706/50000: episode: 90, duration: 0.124s, episode steps: 39, steps per second: 313, episode reward: 0.888, mean reward: 0.023 [-0.005, 1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.097 [-0.465, 0.150], loss: 0.000126, mean_absolute_error: 0.287894, mean_q: 0.293438\n",
      "  7749/50000: episode: 91, duration: 0.135s, episode steps: 43, steps per second: 319, episode reward: 0.849, mean reward: 0.020 [-0.006, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.102 [-0.240, 0.632], loss: 0.000299, mean_absolute_error: 0.291402, mean_q: 0.296984\n",
      "  7810/50000: episode: 92, duration: 0.207s, episode steps: 61, steps per second: 295, episode reward: 0.684, mean reward: 0.011 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.186 [-0.260, 0.911], loss: 0.000181, mean_absolute_error: 0.296135, mean_q: 0.312112\n",
      "  7875/50000: episode: 93, duration: 0.210s, episode steps: 65, steps per second: 309, episode reward: 0.658, mean reward: 0.010 [-0.009, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.192 [-0.270, 0.915], loss: 0.000119, mean_absolute_error: 0.293723, mean_q: 0.299408\n",
      "  7930/50000: episode: 94, duration: 0.178s, episode steps: 55, steps per second: 310, episode reward: 0.794, mean reward: 0.014 [-0.006, 1.000], mean action: 1.109 [0.000, 2.000], mean observation: -0.141 [-0.616, 0.160], loss: 0.000103, mean_absolute_error: 0.311944, mean_q: 0.353200\n",
      "  7967/50000: episode: 95, duration: 0.122s, episode steps: 37, steps per second: 304, episode reward: 0.904, mean reward: 0.024 [-0.004, 1.000], mean action: 1.135 [0.000, 2.000], mean observation: -0.087 [-0.419, 0.140], loss: 0.000242, mean_absolute_error: 0.312177, mean_q: 0.363173\n",
      "  8010/50000: episode: 96, duration: 0.125s, episode steps: 43, steps per second: 343, episode reward: 0.885, mean reward: 0.021 [-0.004, 1.000], mean action: 0.907 [0.000, 2.000], mean observation: 0.096 [-0.110, 0.424], loss: 0.000109, mean_absolute_error: 0.304319, mean_q: 0.336258\n",
      "  8011/50000: episode: 97, duration: 0.006s, episode steps: 1, steps per second: 164, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [-0.010, 0.043], loss: 0.000135, mean_absolute_error: 0.344904, mean_q: 0.315066\n",
      "  8067/50000: episode: 98, duration: 0.176s, episode steps: 56, steps per second: 318, episode reward: 0.785, mean reward: 0.014 [-0.007, 1.000], mean action: 1.089 [0.000, 2.000], mean observation: -0.142 [-0.671, 0.160], loss: 0.000092, mean_absolute_error: 0.317577, mean_q: 0.355339\n",
      "  8117/50000: episode: 99, duration: 0.184s, episode steps: 50, steps per second: 272, episode reward: 0.816, mean reward: 0.016 [-0.006, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.130 [-0.642, 0.180], loss: 0.000121, mean_absolute_error: 0.310389, mean_q: 0.339418\n",
      "  8177/50000: episode: 100, duration: 0.207s, episode steps: 60, steps per second: 290, episode reward: 0.803, mean reward: 0.013 [-0.005, 1.000], mean action: 0.917 [0.000, 2.000], mean observation: 0.128 [-0.110, 0.542], loss: 0.000053, mean_absolute_error: 0.314730, mean_q: 0.353757\n",
      "  8236/50000: episode: 101, duration: 0.176s, episode steps: 59, steps per second: 335, episode reward: 0.820, mean reward: 0.014 [-0.005, 1.000], mean action: 0.966 [0.000, 2.000], mean observation: 0.118 [-0.110, 0.516], loss: 0.000064, mean_absolute_error: 0.323663, mean_q: 0.366462\n",
      "  8296/50000: episode: 102, duration: 0.195s, episode steps: 60, steps per second: 308, episode reward: 0.765, mean reward: 0.013 [-0.007, 1.000], mean action: 1.083 [0.000, 2.000], mean observation: -0.146 [-0.706, 0.160], loss: 0.000206, mean_absolute_error: 0.336102, mean_q: 0.394828\n",
      "  8333/50000: episode: 103, duration: 0.109s, episode steps: 37, steps per second: 340, episode reward: 0.914, mean reward: 0.025 [-0.004, 1.000], mean action: 1.135 [0.000, 2.000], mean observation: -0.083 [-0.352, 0.130], loss: 0.000115, mean_absolute_error: 0.332814, mean_q: 0.371262\n",
      "  8352/50000: episode: 104, duration: 0.063s, episode steps: 19, steps per second: 303, episode reward: 0.972, mean reward: 0.051 [-0.002, 1.000], mean action: 1.316 [0.000, 2.000], mean observation: -0.052 [-0.188, 0.080], loss: 0.000074, mean_absolute_error: 0.332413, mean_q: 0.371837\n",
      "  8428/50000: episode: 105, duration: 0.244s, episode steps: 76, steps per second: 312, episode reward: 0.586, mean reward: 0.008 [-0.009, 1.000], mean action: 0.908 [0.000, 2.000], mean observation: 0.222 [-0.220, 0.871], loss: 0.000166, mean_absolute_error: 0.329145, mean_q: 0.378439\n",
      "  8451/50000: episode: 106, duration: 0.073s, episode steps: 23, steps per second: 315, episode reward: 0.963, mean reward: 0.042 [-0.002, 1.000], mean action: 1.217 [0.000, 2.000], mean observation: -0.054 [-0.229, 0.090], loss: 0.000096, mean_absolute_error: 0.339621, mean_q: 0.390524\n",
      "  8519/50000: episode: 107, duration: 0.221s, episode steps: 68, steps per second: 308, episode reward: 0.682, mean reward: 0.010 [-0.008, 1.000], mean action: 1.088 [0.000, 2.000], mean observation: -0.183 [-0.799, 0.190], loss: 0.000079, mean_absolute_error: 0.335469, mean_q: 0.384374\n",
      "  8520/50000: episode: 108, duration: 0.005s, episode steps: 1, steps per second: 183, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [-0.010, 0.066], loss: 0.000024, mean_absolute_error: 0.275456, mean_q: 0.334649\n",
      "  8521/50000: episode: 109, duration: 0.007s, episode steps: 1, steps per second: 150, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.031 [-0.072, 0.010], loss: 0.000048, mean_absolute_error: 0.317093, mean_q: 0.425023\n",
      "  8560/50000: episode: 110, duration: 0.116s, episode steps: 39, steps per second: 336, episode reward: 0.898, mean reward: 0.023 [-0.004, 1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.090 [-0.433, 0.140], loss: 0.000055, mean_absolute_error: 0.334216, mean_q: 0.374498\n",
      "  8659/50000: episode: 111, duration: 0.302s, episode steps: 99, steps per second: 328, episode reward: 0.551, mean reward: 0.006 [-0.007, 1.000], mean action: 0.970 [0.000, 2.000], mean observation: 0.197 [-0.180, 0.699], loss: 0.000063, mean_absolute_error: 0.346523, mean_q: 0.405376\n",
      "  8680/50000: episode: 112, duration: 0.065s, episode steps: 21, steps per second: 324, episode reward: 0.965, mean reward: 0.046 [-0.002, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: -0.053 [-0.232, 0.100], loss: 0.000082, mean_absolute_error: 0.348257, mean_q: 0.406111\n",
      "  8742/50000: episode: 113, duration: 0.209s, episode steps: 62, steps per second: 297, episode reward: 0.739, mean reward: 0.012 [-0.008, 1.000], mean action: 1.081 [0.000, 2.000], mean observation: -0.158 [-0.767, 0.190], loss: 0.000113, mean_absolute_error: 0.355375, mean_q: 0.429290\n",
      "  8809/50000: episode: 114, duration: 0.193s, episode steps: 67, steps per second: 347, episode reward: 0.687, mean reward: 0.010 [-0.009, 1.000], mean action: 1.075 [0.000, 2.000], mean observation: -0.177 [-0.866, 0.210], loss: 0.000101, mean_absolute_error: 0.354725, mean_q: 0.426769\n",
      "  8886/50000: episode: 115, duration: 0.253s, episode steps: 77, steps per second: 304, episode reward: 0.732, mean reward: 0.010 [-0.006, 1.000], mean action: 0.948 [0.000, 2.000], mean observation: 0.143 [-0.090, 0.577], loss: 0.000304, mean_absolute_error: 0.356052, mean_q: 0.432977\n",
      "  8928/50000: episode: 116, duration: 0.124s, episode steps: 42, steps per second: 339, episode reward: 0.886, mean reward: 0.021 [-0.004, 1.000], mean action: 1.119 [0.000, 2.000], mean observation: -0.094 [-0.449, 0.150], loss: 0.000245, mean_absolute_error: 0.350763, mean_q: 0.430674\n",
      "  8939/50000: episode: 117, duration: 0.035s, episode steps: 11, steps per second: 314, episode reward: 0.988, mean reward: 0.090 [-0.001, 1.000], mean action: 1.364 [0.000, 2.000], mean observation: -0.045 [-0.128, 0.040], loss: 0.000056, mean_absolute_error: 0.341496, mean_q: 0.417918\n",
      "  9021/50000: episode: 118, duration: 0.266s, episode steps: 82, steps per second: 309, episode reward: 0.730, mean reward: 0.009 [-0.006, 1.000], mean action: 0.976 [0.000, 2.000], mean observation: 0.135 [-0.120, 0.592], loss: 0.000311, mean_absolute_error: 0.363746, mean_q: 0.442856\n",
      "  9081/50000: episode: 119, duration: 0.184s, episode steps: 60, steps per second: 327, episode reward: 0.853, mean reward: 0.014 [-0.004, 1.000], mean action: 0.950 [0.000, 2.000], mean observation: 0.095 [-0.100, 0.442], loss: 0.000080, mean_absolute_error: 0.373260, mean_q: 0.450391\n",
      "  9123/50000: episode: 120, duration: 0.136s, episode steps: 42, steps per second: 310, episode reward: 0.860, mean reward: 0.020 [-0.005, 1.000], mean action: 1.190 [0.000, 2.000], mean observation: -0.115 [-0.547, 0.190], loss: 0.000050, mean_absolute_error: 0.383020, mean_q: 0.485474\n",
      "  9187/50000: episode: 121, duration: 0.204s, episode steps: 64, steps per second: 314, episode reward: 0.784, mean reward: 0.012 [-0.007, 1.000], mean action: 0.969 [0.000, 2.000], mean observation: 0.126 [-0.210, 0.654], loss: 0.000044, mean_absolute_error: 0.376229, mean_q: 0.469018\n",
      "  9216/50000: episode: 122, duration: 0.106s, episode steps: 29, steps per second: 274, episode reward: 0.943, mean reward: 0.033 [-0.003, 1.000], mean action: 1.172 [0.000, 2.000], mean observation: -0.067 [-0.289, 0.090], loss: 0.000083, mean_absolute_error: 0.369522, mean_q: 0.455183\n",
      "  9271/50000: episode: 123, duration: 0.181s, episode steps: 55, steps per second: 305, episode reward: 0.856, mean reward: 0.016 [-0.005, 1.000], mean action: 0.945 [0.000, 2.000], mean observation: 0.094 [-0.170, 0.506], loss: 0.000329, mean_absolute_error: 0.380322, mean_q: 0.474837\n",
      "  9347/50000: episode: 124, duration: 0.254s, episode steps: 76, steps per second: 299, episode reward: 0.782, mean reward: 0.010 [-0.007, 1.000], mean action: 0.961 [0.000, 2.000], mean observation: 0.106 [-0.190, 0.668], loss: 0.000427, mean_absolute_error: 0.375651, mean_q: 0.456253\n",
      "  9488/50000: episode: 125, duration: 0.423s, episode steps: 141, steps per second: 334, episode reward: -0.068, mean reward: -0.000 [-0.010, 1.000], mean action: 1.035 [0.000, 2.000], mean observation: -0.354 [-1.000, 0.210], loss: 0.000056, mean_absolute_error: 0.386505, mean_q: 0.483897\n",
      "  9531/50000: episode: 126, duration: 0.130s, episode steps: 43, steps per second: 330, episode reward: 0.877, mean reward: 0.020 [-0.005, 1.000], mean action: 1.163 [0.000, 2.000], mean observation: -0.099 [-0.485, 0.150], loss: 0.000279, mean_absolute_error: 0.392744, mean_q: 0.500160\n",
      "  9532/50000: episode: 127, duration: 0.006s, episode steps: 1, steps per second: 166, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.033 [-0.077, 0.010], loss: 0.000103, mean_absolute_error: 0.384722, mean_q: 0.448585\n",
      "  9584/50000: episode: 128, duration: 0.164s, episode steps: 52, steps per second: 317, episode reward: 0.831, mean reward: 0.016 [-0.006, 1.000], mean action: 0.923 [0.000, 2.000], mean observation: 0.116 [-0.150, 0.589], loss: 0.000122, mean_absolute_error: 0.392608, mean_q: 0.488859\n",
      "  9597/50000: episode: 129, duration: 0.041s, episode steps: 13, steps per second: 316, episode reward: 0.985, mean reward: 0.076 [-0.002, 1.000], mean action: 1.385 [0.000, 2.000], mean observation: -0.042 [-0.150, 0.050], loss: 0.000043, mean_absolute_error: 0.388916, mean_q: 0.488581\n",
      "  9648/50000: episode: 130, duration: 0.160s, episode steps: 51, steps per second: 319, episode reward: 0.823, mean reward: 0.016 [-0.006, 1.000], mean action: 0.961 [0.000, 2.000], mean observation: 0.121 [-0.200, 0.637], loss: 0.000046, mean_absolute_error: 0.391151, mean_q: 0.492787\n",
      "  9702/50000: episode: 131, duration: 0.194s, episode steps: 54, steps per second: 278, episode reward: 0.800, mean reward: 0.015 [-0.007, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.134 [-0.656, 0.180], loss: 0.000084, mean_absolute_error: 0.393004, mean_q: 0.495237\n",
      "  9771/50000: episode: 132, duration: 0.233s, episode steps: 69, steps per second: 296, episode reward: 0.667, mean reward: 0.010 [-0.009, 1.000], mean action: 1.087 [0.000, 2.000], mean observation: -0.186 [-0.881, 0.190], loss: 0.000053, mean_absolute_error: 0.397457, mean_q: 0.497072\n",
      "  9816/50000: episode: 133, duration: 0.138s, episode steps: 45, steps per second: 326, episode reward: 0.888, mean reward: 0.020 [-0.004, 1.000], mean action: 0.933 [0.000, 2.000], mean observation: 0.092 [-0.120, 0.405], loss: 0.000210, mean_absolute_error: 0.389123, mean_q: 0.474416\n",
      "  9817/50000: episode: 134, duration: 0.006s, episode steps: 1, steps per second: 175, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.013 [0.010, 0.017], loss: 0.000036, mean_absolute_error: 0.481995, mean_q: 0.642429\n",
      "  9865/50000: episode: 135, duration: 0.161s, episode steps: 48, steps per second: 298, episode reward: 0.782, mean reward: 0.016 [-0.008, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.157 [-0.250, 0.771], loss: 0.000060, mean_absolute_error: 0.398218, mean_q: 0.508377\n",
      "  9875/50000: episode: 136, duration: 0.035s, episode steps: 10, steps per second: 286, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.041 [-0.132, 0.060], loss: 0.000647, mean_absolute_error: 0.402723, mean_q: 0.525338\n",
      "  9917/50000: episode: 137, duration: 0.131s, episode steps: 42, steps per second: 321, episode reward: 0.887, mean reward: 0.021 [-0.005, 1.000], mean action: 1.119 [0.000, 2.000], mean observation: -0.093 [-0.453, 0.140], loss: 0.000419, mean_absolute_error: 0.393654, mean_q: 0.488423\n",
      "  9956/50000: episode: 138, duration: 0.142s, episode steps: 39, steps per second: 275, episode reward: 0.900, mean reward: 0.023 [-0.004, 1.000], mean action: 0.897 [0.000, 2.000], mean observation: 0.087 [-0.150, 0.427], loss: 0.000413, mean_absolute_error: 0.407644, mean_q: 0.521518\n",
      " 10008/50000: episode: 139, duration: 0.182s, episode steps: 52, steps per second: 286, episode reward: 0.838, mean reward: 0.016 [-0.006, 1.000], mean action: 0.885 [0.000, 2.000], mean observation: 0.110 [-0.190, 0.577], loss: 0.000202, mean_absolute_error: 0.401299, mean_q: 0.509736\n",
      " 10064/50000: episode: 140, duration: 0.193s, episode steps: 56, steps per second: 291, episode reward: 0.835, mean reward: 0.015 [-0.005, 1.000], mean action: 1.071 [0.000, 2.000], mean observation: -0.110 [-0.520, 0.140], loss: 0.000289, mean_absolute_error: 0.398800, mean_q: 0.499100\n",
      " 10065/50000: episode: 141, duration: 0.006s, episode steps: 1, steps per second: 169, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.016 [0.000, 0.033], loss: 0.000072, mean_absolute_error: 0.452388, mean_q: 0.538519\n",
      " 10088/50000: episode: 142, duration: 0.076s, episode steps: 23, steps per second: 302, episode reward: 0.965, mean reward: 0.042 [-0.002, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.052 [-0.070, 0.215], loss: 0.000126, mean_absolute_error: 0.410753, mean_q: 0.521394\n",
      " 10149/50000: episode: 143, duration: 0.202s, episode steps: 61, steps per second: 302, episode reward: 0.765, mean reward: 0.013 [-0.007, 1.000], mean action: 1.049 [0.000, 2.000], mean observation: -0.142 [-0.723, 0.170], loss: 0.000194, mean_absolute_error: 0.400887, mean_q: 0.511524\n",
      " 10207/50000: episode: 144, duration: 0.184s, episode steps: 58, steps per second: 315, episode reward: 0.781, mean reward: 0.013 [-0.007, 1.000], mean action: 1.103 [0.000, 2.000], mean observation: -0.138 [-0.694, 0.200], loss: 0.000187, mean_absolute_error: 0.417006, mean_q: 0.534732\n",
      " 10269/50000: episode: 145, duration: 0.204s, episode steps: 62, steps per second: 304, episode reward: 0.803, mean reward: 0.013 [-0.006, 1.000], mean action: 1.065 [0.000, 2.000], mean observation: -0.119 [-0.599, 0.150], loss: 0.000316, mean_absolute_error: 0.408676, mean_q: 0.519830\n",
      " 10323/50000: episode: 146, duration: 0.174s, episode steps: 54, steps per second: 311, episode reward: 0.821, mean reward: 0.015 [-0.006, 1.000], mean action: 1.093 [0.000, 2.000], mean observation: -0.121 [-0.589, 0.160], loss: 0.000059, mean_absolute_error: 0.415609, mean_q: 0.539525\n",
      " 10382/50000: episode: 147, duration: 0.199s, episode steps: 59, steps per second: 297, episode reward: 0.829, mean reward: 0.014 [-0.005, 1.000], mean action: 1.085 [0.000, 2.000], mean observation: -0.108 [-0.544, 0.150], loss: 0.000434, mean_absolute_error: 0.419949, mean_q: 0.547059\n",
      " 10446/50000: episode: 148, duration: 0.247s, episode steps: 64, steps per second: 259, episode reward: 0.761, mean reward: 0.012 [-0.007, 1.000], mean action: 1.062 [0.000, 2.000], mean observation: -0.138 [-0.722, 0.190], loss: 0.000381, mean_absolute_error: 0.413957, mean_q: 0.538841\n",
      " 10494/50000: episode: 149, duration: 0.207s, episode steps: 48, steps per second: 232, episode reward: 0.830, mean reward: 0.017 [-0.006, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.124 [-0.190, 0.613], loss: 0.000081, mean_absolute_error: 0.417712, mean_q: 0.549452\n",
      " 10547/50000: episode: 150, duration: 0.235s, episode steps: 53, steps per second: 226, episode reward: 0.720, mean reward: 0.014 [-0.009, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.189 [-0.250, 0.903], loss: 0.000045, mean_absolute_error: 0.420205, mean_q: 0.558712\n",
      " 10619/50000: episode: 151, duration: 0.311s, episode steps: 72, steps per second: 232, episode reward: 0.675, mean reward: 0.009 [-0.009, 1.000], mean action: 1.056 [0.000, 2.000], mean observation: -0.172 [-0.886, 0.190], loss: 0.000187, mean_absolute_error: 0.424910, mean_q: 0.564319\n",
      " 10655/50000: episode: 152, duration: 0.132s, episode steps: 36, steps per second: 273, episode reward: 0.928, mean reward: 0.026 [-0.003, 1.000], mean action: 1.139 [0.000, 2.000], mean observation: -0.072 [-0.308, 0.080], loss: 0.000295, mean_absolute_error: 0.434051, mean_q: 0.565638\n",
      " 10675/50000: episode: 153, duration: 0.063s, episode steps: 20, steps per second: 316, episode reward: 0.970, mean reward: 0.049 [-0.002, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.049 [-0.205, 0.080], loss: 0.000095, mean_absolute_error: 0.426305, mean_q: 0.559259\n",
      " 10683/50000: episode: 154, duration: 0.038s, episode steps: 8, steps per second: 209, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: -0.041 [-0.119, 0.040], loss: 0.000049, mean_absolute_error: 0.413703, mean_q: 0.535319\n",
      " 10710/50000: episode: 155, duration: 0.093s, episode steps: 27, steps per second: 290, episode reward: 0.946, mean reward: 0.035 [-0.003, 1.000], mean action: 0.704 [0.000, 2.000], mean observation: 0.066 [-0.100, 0.286], loss: 0.000068, mean_absolute_error: 0.418610, mean_q: 0.554312\n",
      " 10778/50000: episode: 156, duration: 0.221s, episode steps: 68, steps per second: 308, episode reward: 0.709, mean reward: 0.010 [-0.008, 1.000], mean action: 0.897 [0.000, 2.000], mean observation: 0.162 [-0.220, 0.809], loss: 0.000155, mean_absolute_error: 0.427084, mean_q: 0.566745\n",
      " 10835/50000: episode: 157, duration: 0.183s, episode steps: 57, steps per second: 312, episode reward: 0.671, mean reward: 0.012 [-0.010, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.212 [-0.260, 0.980], loss: 0.000204, mean_absolute_error: 0.428731, mean_q: 0.564266\n",
      " 10913/50000: episode: 158, duration: 0.262s, episode steps: 78, steps per second: 298, episode reward: 0.586, mean reward: 0.008 [-0.010, 1.000], mean action: 1.077 [0.000, 2.000], mean observation: -0.208 [-0.991, 0.220], loss: 0.000149, mean_absolute_error: 0.427796, mean_q: 0.571744\n",
      " 10966/50000: episode: 159, duration: 0.171s, episode steps: 53, steps per second: 310, episode reward: 0.796, mean reward: 0.015 [-0.006, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.141 [-0.130, 0.649], loss: 0.000273, mean_absolute_error: 0.437504, mean_q: 0.583712\n",
      " 11020/50000: episode: 160, duration: 0.183s, episode steps: 54, steps per second: 295, episode reward: 0.810, mean reward: 0.015 [-0.006, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.127 [-0.631, 0.170], loss: 0.000131, mean_absolute_error: 0.430877, mean_q: 0.581442\n",
      " 11085/50000: episode: 161, duration: 0.230s, episode steps: 65, steps per second: 283, episode reward: 0.689, mean reward: 0.011 [-0.009, 1.000], mean action: 0.908 [0.000, 2.000], mean observation: 0.176 [-0.240, 0.929], loss: 0.000034, mean_absolute_error: 0.432545, mean_q: 0.583048\n",
      " 11148/50000: episode: 162, duration: 0.203s, episode steps: 63, steps per second: 310, episode reward: 0.679, mean reward: 0.011 [-0.009, 1.000], mean action: 0.873 [0.000, 2.000], mean observation: 0.188 [-0.230, 0.940], loss: 0.000037, mean_absolute_error: 0.430595, mean_q: 0.574174\n",
      " 11191/50000: episode: 163, duration: 0.179s, episode steps: 43, steps per second: 240, episode reward: 0.879, mean reward: 0.020 [-0.005, 1.000], mean action: 1.116 [0.000, 2.000], mean observation: -0.099 [-0.473, 0.130], loss: 0.000110, mean_absolute_error: 0.427781, mean_q: 0.552719\n",
      " 11192/50000: episode: 164, duration: 0.006s, episode steps: 1, steps per second: 168, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [-0.010, 0.063], loss: 0.000081, mean_absolute_error: 0.427507, mean_q: 0.524859\n",
      " 11239/50000: episode: 165, duration: 0.166s, episode steps: 47, steps per second: 283, episode reward: 0.854, mean reward: 0.018 [-0.005, 1.000], mean action: 1.128 [0.000, 2.000], mean observation: -0.110 [-0.534, 0.150], loss: 0.000083, mean_absolute_error: 0.436143, mean_q: 0.592934\n",
      " 11296/50000: episode: 166, duration: 0.217s, episode steps: 57, steps per second: 262, episode reward: 0.761, mean reward: 0.013 [-0.007, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.153 [-0.190, 0.723], loss: 0.000473, mean_absolute_error: 0.437351, mean_q: 0.589400\n",
      " 11355/50000: episode: 167, duration: 0.246s, episode steps: 59, steps per second: 240, episode reward: 0.788, mean reward: 0.013 [-0.006, 1.000], mean action: 0.915 [0.000, 2.000], mean observation: 0.136 [-0.140, 0.617], loss: 0.000055, mean_absolute_error: 0.443295, mean_q: 0.600143\n",
      " 11427/50000: episode: 168, duration: 0.259s, episode steps: 72, steps per second: 278, episode reward: 0.676, mean reward: 0.009 [-0.008, 1.000], mean action: 1.056 [0.000, 2.000], mean observation: -0.174 [-0.839, 0.170], loss: 0.000189, mean_absolute_error: 0.439257, mean_q: 0.596789\n",
      " 11453/50000: episode: 169, duration: 0.093s, episode steps: 26, steps per second: 278, episode reward: 0.960, mean reward: 0.037 [-0.002, 1.000], mean action: 0.808 [0.000, 2.000], mean observation: 0.059 [-0.050, 0.205], loss: 0.000075, mean_absolute_error: 0.444950, mean_q: 0.607444\n",
      " 11496/50000: episode: 170, duration: 0.137s, episode steps: 43, steps per second: 313, episode reward: 0.858, mean reward: 0.020 [-0.006, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.113 [-0.170, 0.552], loss: 0.000041, mean_absolute_error: 0.442114, mean_q: 0.593995\n",
      " 11554/50000: episode: 171, duration: 0.177s, episode steps: 58, steps per second: 327, episode reward: 0.791, mean reward: 0.014 [-0.006, 1.000], mean action: 1.086 [0.000, 2.000], mean observation: -0.133 [-0.649, 0.150], loss: 0.000109, mean_absolute_error: 0.440559, mean_q: 0.603626\n",
      " 11614/50000: episode: 172, duration: 0.209s, episode steps: 60, steps per second: 286, episode reward: 0.730, mean reward: 0.012 [-0.008, 1.000], mean action: 0.900 [0.000, 2.000], mean observation: 0.166 [-0.210, 0.808], loss: 0.000053, mean_absolute_error: 0.435954, mean_q: 0.580208\n",
      " 11672/50000: episode: 173, duration: 0.177s, episode steps: 58, steps per second: 328, episode reward: 0.779, mean reward: 0.013 [-0.007, 1.000], mean action: 0.879 [0.000, 2.000], mean observation: 0.141 [-0.180, 0.680], loss: 0.000283, mean_absolute_error: 0.446622, mean_q: 0.603441\n",
      " 11731/50000: episode: 174, duration: 0.190s, episode steps: 59, steps per second: 310, episode reward: 0.745, mean reward: 0.013 [-0.008, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.159 [-0.771, 0.180], loss: 0.000698, mean_absolute_error: 0.442152, mean_q: 0.603677\n",
      " 11803/50000: episode: 175, duration: 0.229s, episode steps: 72, steps per second: 314, episode reward: 0.723, mean reward: 0.010 [-0.008, 1.000], mean action: 1.042 [0.000, 2.000], mean observation: -0.147 [-0.755, 0.150], loss: 0.000247, mean_absolute_error: 0.431406, mean_q: 0.578552\n",
      " 11849/50000: episode: 176, duration: 0.144s, episode steps: 46, steps per second: 320, episode reward: 0.863, mean reward: 0.019 [-0.005, 1.000], mean action: 1.109 [0.000, 2.000], mean observation: -0.107 [-0.497, 0.110], loss: 0.000079, mean_absolute_error: 0.439245, mean_q: 0.590827\n",
      " 11900/50000: episode: 177, duration: 0.172s, episode steps: 51, steps per second: 296, episode reward: 0.857, mean reward: 0.017 [-0.005, 1.000], mean action: 1.098 [0.000, 2.000], mean observation: -0.102 [-0.497, 0.130], loss: 0.000414, mean_absolute_error: 0.456388, mean_q: 0.624169\n",
      " 11959/50000: episode: 178, duration: 0.193s, episode steps: 59, steps per second: 306, episode reward: 0.754, mean reward: 0.013 [-0.008, 1.000], mean action: 1.085 [0.000, 2.000], mean observation: -0.154 [-0.751, 0.170], loss: 0.000208, mean_absolute_error: 0.439617, mean_q: 0.590030\n",
      " 11981/50000: episode: 179, duration: 0.088s, episode steps: 22, steps per second: 249, episode reward: 0.964, mean reward: 0.044 [-0.002, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.053 [-0.090, 0.233], loss: 0.000210, mean_absolute_error: 0.449640, mean_q: 0.606977\n",
      " 12026/50000: episode: 180, duration: 0.143s, episode steps: 45, steps per second: 316, episode reward: 0.836, mean reward: 0.019 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.128 [-0.599, 0.140], loss: 0.001165, mean_absolute_error: 0.440443, mean_q: 0.588095\n",
      " 12079/50000: episode: 181, duration: 0.233s, episode steps: 53, steps per second: 228, episode reward: 0.763, mean reward: 0.014 [-0.008, 1.000], mean action: 0.849 [0.000, 2.000], mean observation: 0.161 [-0.240, 0.774], loss: 0.000085, mean_absolute_error: 0.436484, mean_q: 0.587039\n",
      " 12113/50000: episode: 182, duration: 0.104s, episode steps: 34, steps per second: 327, episode reward: 0.903, mean reward: 0.027 [-0.004, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.090 [-0.180, 0.448], loss: 0.000175, mean_absolute_error: 0.443701, mean_q: 0.604871\n",
      " 12190/50000: episode: 183, duration: 0.250s, episode steps: 77, steps per second: 308, episode reward: 0.635, mean reward: 0.008 [-0.009, 1.000], mean action: 1.039 [0.000, 2.000], mean observation: -0.187 [-0.886, 0.180], loss: 0.000101, mean_absolute_error: 0.444139, mean_q: 0.605702\n",
      " 12239/50000: episode: 184, duration: 0.193s, episode steps: 49, steps per second: 254, episode reward: 0.874, mean reward: 0.018 [-0.004, 1.000], mean action: 1.122 [0.000, 2.000], mean observation: -0.095 [-0.429, 0.100], loss: 0.000127, mean_absolute_error: 0.449450, mean_q: 0.616862\n",
      " 12302/50000: episode: 185, duration: 0.209s, episode steps: 63, steps per second: 302, episode reward: 0.771, mean reward: 0.012 [-0.007, 1.000], mean action: 1.048 [0.000, 2.000], mean observation: -0.137 [-0.678, 0.160], loss: 0.000065, mean_absolute_error: 0.445187, mean_q: 0.601582\n",
      " 12333/50000: episode: 186, duration: 0.098s, episode steps: 31, steps per second: 316, episode reward: 0.941, mean reward: 0.030 [-0.003, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.066 [-0.291, 0.080], loss: 0.000797, mean_absolute_error: 0.445210, mean_q: 0.612925\n",
      " 12393/50000: episode: 187, duration: 0.209s, episode steps: 60, steps per second: 287, episode reward: 0.752, mean reward: 0.013 [-0.007, 1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.154 [-0.745, 0.180], loss: 0.000232, mean_absolute_error: 0.443676, mean_q: 0.605310\n",
      " 12451/50000: episode: 188, duration: 0.170s, episode steps: 58, steps per second: 342, episode reward: 0.680, mean reward: 0.012 [-0.010, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.202 [-0.240, 0.968], loss: 0.000072, mean_absolute_error: 0.442832, mean_q: 0.599823\n",
      " 12531/50000: episode: 189, duration: 0.265s, episode steps: 80, steps per second: 302, episode reward: 0.587, mean reward: 0.007 [-0.010, 1.000], mean action: 1.075 [0.000, 2.000], mean observation: -0.203 [-0.986, 0.170], loss: 0.000052, mean_absolute_error: 0.446210, mean_q: 0.610620\n",
      " 12600/50000: episode: 190, duration: 0.198s, episode steps: 69, steps per second: 348, episode reward: 0.671, mean reward: 0.010 [-0.009, 1.000], mean action: 0.928 [0.000, 2.000], mean observation: 0.179 [-0.240, 0.928], loss: 0.000319, mean_absolute_error: 0.450162, mean_q: 0.620469\n",
      " 12662/50000: episode: 191, duration: 0.205s, episode steps: 62, steps per second: 303, episode reward: 0.835, mean reward: 0.013 [-0.005, 1.000], mean action: 0.952 [0.000, 2.000], mean observation: 0.104 [-0.100, 0.463], loss: 0.000064, mean_absolute_error: 0.436902, mean_q: 0.592008\n",
      " 12663/50000: episode: 192, duration: 0.006s, episode steps: 1, steps per second: 175, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [-0.010, 0.064], loss: 0.000037, mean_absolute_error: 0.400822, mean_q: 0.551116\n",
      " 12698/50000: episode: 193, duration: 0.102s, episode steps: 35, steps per second: 344, episode reward: 0.932, mean reward: 0.027 [-0.003, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.075 [-0.259, 0.090], loss: 0.000079, mean_absolute_error: 0.436183, mean_q: 0.597005\n",
      " 12763/50000: episode: 194, duration: 0.205s, episode steps: 65, steps per second: 317, episode reward: 0.726, mean reward: 0.011 [-0.008, 1.000], mean action: 1.077 [0.000, 2.000], mean observation: -0.159 [-0.781, 0.170], loss: 0.000085, mean_absolute_error: 0.447772, mean_q: 0.614670\n",
      " 12829/50000: episode: 195, duration: 0.192s, episode steps: 66, steps per second: 343, episode reward: 0.718, mean reward: 0.011 [-0.008, 1.000], mean action: 1.061 [0.000, 2.000], mean observation: -0.161 [-0.801, 0.190], loss: 0.000090, mean_absolute_error: 0.446130, mean_q: 0.614205\n",
      " 12886/50000: episode: 196, duration: 0.212s, episode steps: 57, steps per second: 269, episode reward: 0.687, mean reward: 0.012 [-0.010, 1.000], mean action: 0.895 [0.000, 2.000], mean observation: 0.197 [-0.280, 0.987], loss: 0.000122, mean_absolute_error: 0.448829, mean_q: 0.619803\n",
      " 12926/50000: episode: 197, duration: 0.128s, episode steps: 40, steps per second: 312, episode reward: 0.878, mean reward: 0.022 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.103 [-0.190, 0.500], loss: 0.000072, mean_absolute_error: 0.440902, mean_q: 0.600272\n",
      " 12975/50000: episode: 198, duration: 0.180s, episode steps: 49, steps per second: 272, episode reward: 0.897, mean reward: 0.018 [-0.003, 1.000], mean action: 1.102 [0.000, 2.000], mean observation: -0.083 [-0.321, 0.050], loss: 0.000409, mean_absolute_error: 0.445924, mean_q: 0.618778\n",
      " 13033/50000: episode: 199, duration: 0.168s, episode steps: 58, steps per second: 345, episode reward: 0.772, mean reward: 0.013 [-0.008, 1.000], mean action: 0.983 [0.000, 2.000], mean observation: 0.141 [-0.230, 0.752], loss: 0.000298, mean_absolute_error: 0.443997, mean_q: 0.611936\n",
      " 13063/50000: episode: 200, duration: 0.110s, episode steps: 30, steps per second: 272, episode reward: 0.951, mean reward: 0.032 [-0.002, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: 0.062 [-0.050, 0.224], loss: 0.000522, mean_absolute_error: 0.447996, mean_q: 0.627134\n",
      " 13106/50000: episode: 201, duration: 0.145s, episode steps: 43, steps per second: 297, episode reward: 0.870, mean reward: 0.020 [-0.005, 1.000], mean action: 1.163 [0.000, 2.000], mean observation: -0.106 [-0.494, 0.150], loss: 0.000148, mean_absolute_error: 0.441887, mean_q: 0.613777\n",
      " 13136/50000: episode: 202, duration: 0.154s, episode steps: 30, steps per second: 195, episode reward: 0.964, mean reward: 0.032 [-0.001, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.058 [-0.133, 0.050], loss: 0.000071, mean_absolute_error: 0.447484, mean_q: 0.611596\n",
      " 13177/50000: episode: 203, duration: 0.132s, episode steps: 41, steps per second: 310, episode reward: 0.929, mean reward: 0.023 [-0.002, 1.000], mean action: 1.098 [0.000, 2.000], mean observation: -0.070 [-0.240, 0.050], loss: 0.000046, mean_absolute_error: 0.450261, mean_q: 0.628748\n",
      " 13216/50000: episode: 204, duration: 0.128s, episode steps: 39, steps per second: 304, episode reward: 0.899, mean reward: 0.023 [-0.004, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.091 [-0.399, 0.120], loss: 0.000381, mean_absolute_error: 0.443909, mean_q: 0.616793\n",
      " 13259/50000: episode: 205, duration: 0.138s, episode steps: 43, steps per second: 312, episode reward: 0.851, mean reward: 0.020 [-0.006, 1.000], mean action: 0.907 [0.000, 2.000], mean observation: 0.118 [-0.210, 0.586], loss: 0.000067, mean_absolute_error: 0.449867, mean_q: 0.629489\n",
      " 13327/50000: episode: 206, duration: 0.219s, episode steps: 68, steps per second: 310, episode reward: 0.659, mean reward: 0.010 [-0.010, 1.000], mean action: 0.941 [0.000, 2.000], mean observation: 0.188 [-0.230, 0.958], loss: 0.000060, mean_absolute_error: 0.446594, mean_q: 0.613916\n",
      " 13370/50000: episode: 207, duration: 0.128s, episode steps: 43, steps per second: 336, episode reward: 0.882, mean reward: 0.021 [-0.005, 1.000], mean action: 1.116 [0.000, 2.000], mean observation: -0.096 [-0.461, 0.120], loss: 0.000057, mean_absolute_error: 0.442418, mean_q: 0.612834\n",
      " 13435/50000: episode: 208, duration: 0.244s, episode steps: 65, steps per second: 267, episode reward: 0.698, mean reward: 0.011 [-0.009, 1.000], mean action: 1.046 [0.000, 2.000], mean observation: -0.174 [-0.858, 0.180], loss: 0.000109, mean_absolute_error: 0.452074, mean_q: 0.629526\n",
      " 13472/50000: episode: 209, duration: 0.120s, episode steps: 37, steps per second: 309, episode reward: 0.891, mean reward: 0.024 [-0.005, 1.000], mean action: 0.784 [0.000, 2.000], mean observation: 0.098 [-0.180, 0.473], loss: 0.000069, mean_absolute_error: 0.439918, mean_q: 0.609178\n",
      " 13506/50000: episode: 210, duration: 0.104s, episode steps: 34, steps per second: 327, episode reward: 0.941, mean reward: 0.028 [-0.002, 1.000], mean action: 1.235 [0.000, 2.000], mean observation: -0.074 [-0.196, 0.110], loss: 0.000462, mean_absolute_error: 0.447776, mean_q: 0.625989\n",
      " 13557/50000: episode: 211, duration: 0.180s, episode steps: 51, steps per second: 283, episode reward: 0.789, mean reward: 0.015 [-0.007, 1.000], mean action: 0.922 [0.000, 2.000], mean observation: 0.150 [-0.210, 0.684], loss: 0.000209, mean_absolute_error: 0.445165, mean_q: 0.614018\n",
      " 13624/50000: episode: 212, duration: 0.218s, episode steps: 67, steps per second: 308, episode reward: 0.661, mean reward: 0.010 [-0.010, 1.000], mean action: 0.955 [0.000, 2.000], mean observation: 0.189 [-0.260, 0.969], loss: 0.000179, mean_absolute_error: 0.436630, mean_q: 0.603663\n",
      " 13684/50000: episode: 213, duration: 0.177s, episode steps: 60, steps per second: 339, episode reward: 0.686, mean reward: 0.011 [-0.009, 1.000], mean action: 0.883 [0.000, 2.000], mean observation: 0.192 [-0.220, 0.941], loss: 0.000272, mean_absolute_error: 0.445066, mean_q: 0.625836\n",
      " 13722/50000: episode: 214, duration: 0.134s, episode steps: 38, steps per second: 283, episode reward: 0.892, mean reward: 0.023 [-0.005, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.094 [-0.459, 0.140], loss: 0.000720, mean_absolute_error: 0.443620, mean_q: 0.610070\n",
      " 13739/50000: episode: 215, duration: 0.058s, episode steps: 17, steps per second: 294, episode reward: 0.975, mean reward: 0.057 [-0.002, 1.000], mean action: 1.294 [0.000, 2.000], mean observation: -0.046 [-0.201, 0.100], loss: 0.000338, mean_absolute_error: 0.453885, mean_q: 0.643954\n",
      " 13768/50000: episode: 216, duration: 0.085s, episode steps: 29, steps per second: 342, episode reward: 0.953, mean reward: 0.033 [-0.002, 1.000], mean action: 0.828 [0.000, 1.000], mean observation: 0.062 [-0.050, 0.219], loss: 0.000187, mean_absolute_error: 0.438838, mean_q: 0.607057\n",
      " 13815/50000: episode: 217, duration: 0.155s, episode steps: 47, steps per second: 304, episode reward: 0.789, mean reward: 0.017 [-0.008, 1.000], mean action: 0.851 [0.000, 2.000], mean observation: 0.155 [-0.240, 0.764], loss: 0.000476, mean_absolute_error: 0.446346, mean_q: 0.622376\n",
      " 13847/50000: episode: 218, duration: 0.095s, episode steps: 32, steps per second: 337, episode reward: 0.936, mean reward: 0.029 [-0.003, 1.000], mean action: 1.219 [0.000, 2.000], mean observation: -0.077 [-0.252, 0.100], loss: 0.000077, mean_absolute_error: 0.440304, mean_q: 0.619193\n",
      " 13848/50000: episode: 219, duration: 0.006s, episode steps: 1, steps per second: 173, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.054 [0.010, 0.099], loss: 0.006494, mean_absolute_error: 0.444480, mean_q: 0.535311\n",
      " 13882/50000: episode: 220, duration: 0.115s, episode steps: 34, steps per second: 296, episode reward: 0.940, mean reward: 0.028 [-0.003, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.066 [-0.255, 0.060], loss: 0.000089, mean_absolute_error: 0.446267, mean_q: 0.618128\n",
      " 13927/50000: episode: 221, duration: 0.145s, episode steps: 45, steps per second: 311, episode reward: 0.905, mean reward: 0.020 [-0.003, 1.000], mean action: 0.933 [0.000, 2.000], mean observation: 0.082 [-0.070, 0.321], loss: 0.000261, mean_absolute_error: 0.450826, mean_q: 0.635891\n",
      " 13986/50000: episode: 222, duration: 0.190s, episode steps: 59, steps per second: 310, episode reward: 0.696, mean reward: 0.012 [-0.009, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.188 [-0.240, 0.925], loss: 0.000176, mean_absolute_error: 0.437875, mean_q: 0.615287\n",
      " 14029/50000: episode: 223, duration: 0.129s, episode steps: 43, steps per second: 334, episode reward: 0.853, mean reward: 0.020 [-0.006, 1.000], mean action: 0.884 [0.000, 2.000], mean observation: 0.118 [-0.190, 0.567], loss: 0.000061, mean_absolute_error: 0.453030, mean_q: 0.645760\n",
      " 14053/50000: episode: 224, duration: 0.120s, episode steps: 24, steps per second: 200, episode reward: 0.957, mean reward: 0.040 [-0.003, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.058 [-0.252, 0.100], loss: 0.000046, mean_absolute_error: 0.445447, mean_q: 0.622948\n",
      " 14096/50000: episode: 225, duration: 0.193s, episode steps: 43, steps per second: 223, episode reward: 0.869, mean reward: 0.020 [-0.005, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.103 [-0.531, 0.190], loss: 0.000272, mean_absolute_error: 0.451566, mean_q: 0.637969\n",
      " 14122/50000: episode: 226, duration: 0.121s, episode steps: 26, steps per second: 215, episode reward: 0.962, mean reward: 0.037 [-0.002, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.056 [-0.050, 0.193], loss: 0.000048, mean_absolute_error: 0.444692, mean_q: 0.630218\n",
      " 14168/50000: episode: 227, duration: 0.208s, episode steps: 46, steps per second: 221, episode reward: 0.836, mean reward: 0.018 [-0.006, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.123 [-0.210, 0.611], loss: 0.000227, mean_absolute_error: 0.442588, mean_q: 0.627269\n",
      " 14211/50000: episode: 228, duration: 0.151s, episode steps: 43, steps per second: 284, episode reward: 0.861, mean reward: 0.020 [-0.005, 1.000], mean action: 0.907 [0.000, 2.000], mean observation: 0.112 [-0.180, 0.538], loss: 0.000265, mean_absolute_error: 0.454987, mean_q: 0.644679\n",
      " 14247/50000: episode: 229, duration: 0.125s, episode steps: 36, steps per second: 288, episode reward: 0.909, mean reward: 0.025 [-0.004, 1.000], mean action: 1.194 [0.000, 2.000], mean observation: -0.088 [-0.381, 0.110], loss: 0.000072, mean_absolute_error: 0.454149, mean_q: 0.644852\n",
      " 14309/50000: episode: 230, duration: 0.204s, episode steps: 62, steps per second: 304, episode reward: 0.719, mean reward: 0.012 [-0.008, 1.000], mean action: 1.097 [0.000, 2.000], mean observation: -0.169 [-0.820, 0.180], loss: 0.000114, mean_absolute_error: 0.443702, mean_q: 0.626761\n",
      " 14353/50000: episode: 231, duration: 0.154s, episode steps: 44, steps per second: 286, episode reward: 0.849, mean reward: 0.019 [-0.006, 1.000], mean action: 1.182 [0.000, 2.000], mean observation: -0.118 [-0.574, 0.170], loss: 0.000342, mean_absolute_error: 0.441452, mean_q: 0.619926\n",
      " 14399/50000: episode: 232, duration: 0.199s, episode steps: 46, steps per second: 231, episode reward: 0.822, mean reward: 0.018 [-0.006, 1.000], mean action: 0.848 [0.000, 2.000], mean observation: 0.134 [-0.200, 0.649], loss: 0.000055, mean_absolute_error: 0.443317, mean_q: 0.623848\n",
      " 14456/50000: episode: 233, duration: 0.215s, episode steps: 57, steps per second: 265, episode reward: 0.758, mean reward: 0.013 [-0.008, 1.000], mean action: 1.123 [0.000, 2.000], mean observation: -0.154 [-0.767, 0.200], loss: 0.000049, mean_absolute_error: 0.439890, mean_q: 0.618210\n",
      " 14495/50000: episode: 234, duration: 0.154s, episode steps: 39, steps per second: 254, episode reward: 0.923, mean reward: 0.024 [-0.003, 1.000], mean action: 0.821 [0.000, 2.000], mean observation: 0.074 [-0.070, 0.290], loss: 0.000057, mean_absolute_error: 0.449218, mean_q: 0.643731\n",
      " 14539/50000: episode: 235, duration: 0.187s, episode steps: 44, steps per second: 235, episode reward: 0.843, mean reward: 0.019 [-0.006, 1.000], mean action: 0.818 [0.000, 2.000], mean observation: 0.123 [-0.170, 0.593], loss: 0.000316, mean_absolute_error: 0.443680, mean_q: 0.633731\n",
      " 14592/50000: episode: 236, duration: 0.179s, episode steps: 53, steps per second: 295, episode reward: 0.768, mean reward: 0.014 [-0.008, 1.000], mean action: 0.868 [0.000, 2.000], mean observation: 0.157 [-0.230, 0.751], loss: 0.000055, mean_absolute_error: 0.443176, mean_q: 0.627603\n",
      " 14634/50000: episode: 237, duration: 0.161s, episode steps: 42, steps per second: 261, episode reward: 0.862, mean reward: 0.021 [-0.005, 1.000], mean action: 1.190 [0.000, 2.000], mean observation: -0.112 [-0.546, 0.150], loss: 0.000039, mean_absolute_error: 0.434889, mean_q: 0.612319\n",
      " 14694/50000: episode: 238, duration: 0.182s, episode steps: 60, steps per second: 330, episode reward: 0.708, mean reward: 0.012 [-0.009, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.180 [-0.240, 0.866], loss: 0.000258, mean_absolute_error: 0.448214, mean_q: 0.646452\n",
      " 14744/50000: episode: 239, duration: 0.162s, episode steps: 50, steps per second: 309, episode reward: 0.839, mean reward: 0.017 [-0.005, 1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.120 [-0.509, 0.140], loss: 0.000056, mean_absolute_error: 0.446155, mean_q: 0.638088\n",
      " 14787/50000: episode: 240, duration: 0.133s, episode steps: 43, steps per second: 323, episode reward: 0.884, mean reward: 0.021 [-0.005, 1.000], mean action: 0.907 [0.000, 2.000], mean observation: 0.092 [-0.170, 0.475], loss: 0.000039, mean_absolute_error: 0.436688, mean_q: 0.618262\n",
      " 14815/50000: episode: 241, duration: 0.093s, episode steps: 28, steps per second: 302, episode reward: 0.967, mean reward: 0.035 [-0.001, 1.000], mean action: 0.929 [0.000, 2.000], mean observation: 0.052 [-0.030, 0.145], loss: 0.000071, mean_absolute_error: 0.439473, mean_q: 0.627546\n",
      " 14822/50000: episode: 242, duration: 0.035s, episode steps: 7, steps per second: 202, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 1.714 [0.000, 2.000], mean observation: -0.044 [-0.113, 0.050], loss: 0.000038, mean_absolute_error: 0.440270, mean_q: 0.632859\n",
      " 14878/50000: episode: 243, duration: 0.227s, episode steps: 56, steps per second: 247, episode reward: 0.746, mean reward: 0.013 [-0.008, 1.000], mean action: 1.107 [0.000, 2.000], mean observation: -0.166 [-0.774, 0.200], loss: 0.000089, mean_absolute_error: 0.449833, mean_q: 0.647675\n",
      " 14914/50000: episode: 244, duration: 0.154s, episode steps: 36, steps per second: 233, episode reward: 0.943, mean reward: 0.026 [-0.002, 1.000], mean action: 0.917 [0.000, 2.000], mean observation: 0.062 [-0.050, 0.223], loss: 0.000102, mean_absolute_error: 0.446508, mean_q: 0.647007\n",
      " 14969/50000: episode: 245, duration: 0.247s, episode steps: 55, steps per second: 223, episode reward: 0.772, mean reward: 0.014 [-0.007, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.149 [-0.746, 0.200], loss: 0.000170, mean_absolute_error: 0.436321, mean_q: 0.625293\n",
      " 15022/50000: episode: 246, duration: 0.182s, episode steps: 53, steps per second: 291, episode reward: 0.764, mean reward: 0.014 [-0.007, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.164 [-0.210, 0.729], loss: 0.000211, mean_absolute_error: 0.442406, mean_q: 0.632305\n",
      " 15031/50000: episode: 247, duration: 0.038s, episode steps: 9, steps per second: 239, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 1.889 [1.000, 2.000], mean observation: -0.041 [-0.134, 0.080], loss: 0.000062, mean_absolute_error: 0.427209, mean_q: 0.609244\n",
      " 15056/50000: episode: 248, duration: 0.087s, episode steps: 25, steps per second: 289, episode reward: 0.947, mean reward: 0.038 [-0.003, 1.000], mean action: 1.320 [0.000, 2.000], mean observation: -0.065 [-0.313, 0.130], loss: 0.000042, mean_absolute_error: 0.440489, mean_q: 0.636078\n",
      " 15118/50000: episode: 249, duration: 0.240s, episode steps: 62, steps per second: 258, episode reward: 0.681, mean reward: 0.011 [-0.008, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.197 [-0.230, 0.845], loss: 0.000093, mean_absolute_error: 0.442662, mean_q: 0.635520\n",
      " 15157/50000: episode: 250, duration: 0.122s, episode steps: 39, steps per second: 320, episode reward: 0.869, mean reward: 0.022 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.110 [-0.545, 0.170], loss: 0.000037, mean_absolute_error: 0.441256, mean_q: 0.632717\n",
      " 15172/50000: episode: 251, duration: 0.049s, episode steps: 15, steps per second: 306, episode reward: 0.978, mean reward: 0.065 [-0.002, 1.000], mean action: 1.600 [0.000, 2.000], mean observation: -0.046 [-0.194, 0.100], loss: 0.000033, mean_absolute_error: 0.438577, mean_q: 0.630851\n",
      " 15220/50000: episode: 252, duration: 0.165s, episode steps: 48, steps per second: 291, episode reward: 0.820, mean reward: 0.017 [-0.006, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.134 [-0.616, 0.190], loss: 0.000094, mean_absolute_error: 0.450789, mean_q: 0.658978\n",
      " 15246/50000: episode: 253, duration: 0.080s, episode steps: 26, steps per second: 324, episode reward: 0.949, mean reward: 0.037 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.066 [-0.266, 0.110], loss: 0.000181, mean_absolute_error: 0.446513, mean_q: 0.648081\n",
      " 15267/50000: episode: 254, duration: 0.062s, episode steps: 21, steps per second: 337, episode reward: 0.976, mean reward: 0.046 [-0.001, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.050 [-0.030, 0.136], loss: 0.000047, mean_absolute_error: 0.440206, mean_q: 0.641198\n",
      " 15314/50000: episode: 255, duration: 0.160s, episode steps: 47, steps per second: 294, episode reward: 0.813, mean reward: 0.017 [-0.007, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.137 [-0.681, 0.210], loss: 0.000034, mean_absolute_error: 0.445680, mean_q: 0.647668\n",
      " 15371/50000: episode: 256, duration: 0.168s, episode steps: 57, steps per second: 340, episode reward: 0.736, mean reward: 0.013 [-0.008, 1.000], mean action: 1.123 [0.000, 2.000], mean observation: -0.171 [-0.792, 0.210], loss: 0.000039, mean_absolute_error: 0.445845, mean_q: 0.646138\n",
      " 15419/50000: episode: 257, duration: 0.172s, episode steps: 48, steps per second: 280, episode reward: 0.806, mean reward: 0.017 [-0.007, 1.000], mean action: 0.854 [0.000, 2.000], mean observation: 0.142 [-0.200, 0.686], loss: 0.000178, mean_absolute_error: 0.444476, mean_q: 0.645306\n",
      " 15478/50000: episode: 258, duration: 0.188s, episode steps: 59, steps per second: 314, episode reward: 0.697, mean reward: 0.012 [-0.009, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.191 [-0.877, 0.210], loss: 0.000036, mean_absolute_error: 0.449945, mean_q: 0.654111\n",
      " 15548/50000: episode: 259, duration: 0.239s, episode steps: 70, steps per second: 292, episode reward: 0.683, mean reward: 0.010 [-0.009, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.160 [-0.891, 0.210], loss: 0.000074, mean_absolute_error: 0.442877, mean_q: 0.645296\n",
      " 15602/50000: episode: 260, duration: 0.154s, episode steps: 54, steps per second: 350, episode reward: 0.710, mean reward: 0.013 [-0.009, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.193 [-0.250, 0.912], loss: 0.000306, mean_absolute_error: 0.452822, mean_q: 0.659806\n",
      " 15664/50000: episode: 261, duration: 0.222s, episode steps: 62, steps per second: 279, episode reward: 0.688, mean reward: 0.011 [-0.009, 1.000], mean action: 0.903 [0.000, 2.000], mean observation: 0.187 [-0.230, 0.916], loss: 0.000166, mean_absolute_error: 0.443681, mean_q: 0.648071\n",
      " 15675/50000: episode: 262, duration: 0.035s, episode steps: 11, steps per second: 316, episode reward: 0.986, mean reward: 0.090 [-0.002, 1.000], mean action: 1.818 [0.000, 2.000], mean observation: -0.039 [-0.161, 0.100], loss: 0.000025, mean_absolute_error: 0.440242, mean_q: 0.636009\n",
      " 15721/50000: episode: 263, duration: 0.165s, episode steps: 46, steps per second: 279, episode reward: 0.817, mean reward: 0.018 [-0.007, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.136 [-0.220, 0.673], loss: 0.000133, mean_absolute_error: 0.444662, mean_q: 0.649371\n",
      " 15772/50000: episode: 264, duration: 0.168s, episode steps: 51, steps per second: 304, episode reward: 0.780, mean reward: 0.015 [-0.007, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.153 [-0.230, 0.741], loss: 0.000042, mean_absolute_error: 0.449648, mean_q: 0.661405\n",
      " 15794/50000: episode: 265, duration: 0.081s, episode steps: 22, steps per second: 270, episode reward: 0.964, mean reward: 0.044 [-0.002, 1.000], mean action: 1.409 [0.000, 2.000], mean observation: -0.047 [-0.229, 0.110], loss: 0.000040, mean_absolute_error: 0.446865, mean_q: 0.653286\n",
      " 15856/50000: episode: 266, duration: 0.205s, episode steps: 62, steps per second: 302, episode reward: 0.659, mean reward: 0.011 [-0.010, 1.000], mean action: 0.903 [0.000, 2.000], mean observation: 0.205 [-0.240, 0.971], loss: 0.000047, mean_absolute_error: 0.440229, mean_q: 0.639958\n",
      " 15876/50000: episode: 267, duration: 0.076s, episode steps: 20, steps per second: 262, episode reward: 0.968, mean reward: 0.048 [-0.002, 1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.046 [-0.225, 0.100], loss: 0.000044, mean_absolute_error: 0.448732, mean_q: 0.662389\n",
      " 15915/50000: episode: 268, duration: 0.117s, episode steps: 39, steps per second: 334, episode reward: 0.866, mean reward: 0.022 [-0.006, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.114 [-0.559, 0.170], loss: 0.000270, mean_absolute_error: 0.447742, mean_q: 0.660758\n",
      " 15956/50000: episode: 269, duration: 0.151s, episode steps: 41, steps per second: 271, episode reward: 0.862, mean reward: 0.021 [-0.006, 1.000], mean action: 0.829 [0.000, 2.000], mean observation: 0.114 [-0.180, 0.558], loss: 0.000055, mean_absolute_error: 0.444969, mean_q: 0.649956\n",
      " 15995/50000: episode: 270, duration: 0.129s, episode steps: 39, steps per second: 303, episode reward: 0.858, mean reward: 0.022 [-0.006, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.122 [-0.581, 0.210], loss: 0.000652, mean_absolute_error: 0.448085, mean_q: 0.656645\n",
      " 15996/50000: episode: 271, duration: 0.006s, episode steps: 1, steps per second: 177, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [-0.010, 0.058], loss: 0.000094, mean_absolute_error: 0.484018, mean_q: 0.722492\n",
      " 16038/50000: episode: 272, duration: 0.149s, episode steps: 42, steps per second: 282, episode reward: 0.875, mean reward: 0.021 [-0.005, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.088 [-0.534, 0.190], loss: 0.000218, mean_absolute_error: 0.443182, mean_q: 0.650223\n",
      " 16096/50000: episode: 273, duration: 0.169s, episode steps: 58, steps per second: 343, episode reward: 0.673, mean reward: 0.012 [-0.010, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.202 [-0.240, 0.992], loss: 0.000038, mean_absolute_error: 0.440900, mean_q: 0.649047\n",
      " 16097/50000: episode: 274, duration: 0.006s, episode steps: 1, steps per second: 162, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.004 [-0.002, 0.010], loss: 0.000011, mean_absolute_error: 0.433823, mean_q: 0.615075\n",
      " 16116/50000: episode: 275, duration: 0.077s, episode steps: 19, steps per second: 248, episode reward: 0.970, mean reward: 0.051 [-0.002, 1.000], mean action: 1.474 [0.000, 2.000], mean observation: -0.046 [-0.214, 0.110], loss: 0.000029, mean_absolute_error: 0.444567, mean_q: 0.652555\n",
      " 16142/50000: episode: 276, duration: 0.092s, episode steps: 26, steps per second: 281, episode reward: 0.955, mean reward: 0.037 [-0.002, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.060 [-0.070, 0.246], loss: 0.000028, mean_absolute_error: 0.440972, mean_q: 0.648699\n",
      " 16162/50000: episode: 277, duration: 0.081s, episode steps: 20, steps per second: 246, episode reward: 0.967, mean reward: 0.048 [-0.002, 1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.047 [-0.231, 0.110], loss: 0.000020, mean_absolute_error: 0.448921, mean_q: 0.658760\n",
      " 16213/50000: episode: 278, duration: 0.207s, episode steps: 51, steps per second: 246, episode reward: 0.881, mean reward: 0.017 [-0.004, 1.000], mean action: 0.863 [0.000, 2.000], mean observation: 0.085 [-0.120, 0.419], loss: 0.000207, mean_absolute_error: 0.452798, mean_q: 0.664048\n",
      " 16226/50000: episode: 279, duration: 0.058s, episode steps: 13, steps per second: 223, episode reward: 0.982, mean reward: 0.076 [-0.002, 1.000], mean action: 1.692 [0.000, 2.000], mean observation: -0.041 [-0.178, 0.100], loss: 0.000049, mean_absolute_error: 0.443634, mean_q: 0.653727\n",
      " 16294/50000: episode: 280, duration: 0.262s, episode steps: 68, steps per second: 260, episode reward: 0.622, mean reward: 0.009 [-0.010, 1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.215 [-0.954, 0.200], loss: 0.000363, mean_absolute_error: 0.451118, mean_q: 0.665237\n",
      " 16351/50000: episode: 281, duration: 0.167s, episode steps: 57, steps per second: 340, episode reward: 0.727, mean reward: 0.013 [-0.008, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.176 [-0.823, 0.220], loss: 0.000027, mean_absolute_error: 0.450626, mean_q: 0.667035\n",
      " 16412/50000: episode: 282, duration: 0.205s, episode steps: 61, steps per second: 298, episode reward: 0.683, mean reward: 0.011 [-0.010, 1.000], mean action: 0.885 [0.000, 2.000], mean observation: 0.189 [-0.240, 0.963], loss: 0.000050, mean_absolute_error: 0.441359, mean_q: 0.649707\n",
      " 16431/50000: episode: 283, duration: 0.059s, episode steps: 19, steps per second: 321, episode reward: 0.969, mean reward: 0.051 [-0.002, 1.000], mean action: 1.474 [0.000, 2.000], mean observation: -0.043 [-0.228, 0.120], loss: 0.000027, mean_absolute_error: 0.449676, mean_q: 0.664985\n",
      " 16466/50000: episode: 284, duration: 0.113s, episode steps: 35, steps per second: 310, episode reward: 0.910, mean reward: 0.026 [-0.004, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.077 [-0.427, 0.160], loss: 0.000067, mean_absolute_error: 0.441487, mean_q: 0.654877\n",
      " 16502/50000: episode: 285, duration: 0.163s, episode steps: 36, steps per second: 221, episode reward: 0.920, mean reward: 0.026 [-0.003, 1.000], mean action: 0.778 [0.000, 2.000], mean observation: 0.079 [-0.080, 0.338], loss: 0.000032, mean_absolute_error: 0.450237, mean_q: 0.669935\n",
      " 16526/50000: episode: 286, duration: 0.106s, episode steps: 24, steps per second: 227, episode reward: 0.950, mean reward: 0.040 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.065 [-0.290, 0.130], loss: 0.000032, mean_absolute_error: 0.438473, mean_q: 0.653027\n",
      " 16527/50000: episode: 287, duration: 0.009s, episode steps: 1, steps per second: 113, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.024 [-0.047, 0.000], loss: 0.000018, mean_absolute_error: 0.442688, mean_q: 0.667793\n",
      " 16539/50000: episode: 288, duration: 0.051s, episode steps: 12, steps per second: 235, episode reward: 0.986, mean reward: 0.082 [-0.002, 1.000], mean action: 1.750 [0.000, 2.000], mean observation: -0.033 [-0.156, 0.100], loss: 0.000033, mean_absolute_error: 0.441499, mean_q: 0.651543\n",
      " 16585/50000: episode: 289, duration: 0.205s, episode steps: 46, steps per second: 225, episode reward: 0.834, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.118 [-0.628, 0.190], loss: 0.000206, mean_absolute_error: 0.442661, mean_q: 0.657753\n",
      " 16641/50000: episode: 290, duration: 0.199s, episode steps: 56, steps per second: 281, episode reward: 0.754, mean reward: 0.013 [-0.007, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.165 [-0.726, 0.210], loss: 0.000045, mean_absolute_error: 0.449464, mean_q: 0.669295\n",
      " 16686/50000: episode: 291, duration: 0.154s, episode steps: 45, steps per second: 291, episode reward: 0.845, mean reward: 0.019 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.118 [-0.583, 0.180], loss: 0.000340, mean_absolute_error: 0.446088, mean_q: 0.660574\n",
      " 16701/50000: episode: 292, duration: 0.045s, episode steps: 15, steps per second: 334, episode reward: 0.980, mean reward: 0.065 [-0.002, 1.000], mean action: 0.600 [0.000, 2.000], mean observation: 0.043 [-0.080, 0.178], loss: 0.000110, mean_absolute_error: 0.465193, mean_q: 0.698629\n",
      " 16702/50000: episode: 293, duration: 0.006s, episode steps: 1, steps per second: 178, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.032 [0.010, 0.055], loss: 0.000100, mean_absolute_error: 0.433442, mean_q: 0.657627\n",
      " 16741/50000: episode: 294, duration: 0.113s, episode steps: 39, steps per second: 346, episode reward: 0.928, mean reward: 0.024 [-0.003, 1.000], mean action: 0.821 [0.000, 2.000], mean observation: 0.070 [-0.070, 0.281], loss: 0.000090, mean_absolute_error: 0.447594, mean_q: 0.666309\n",
      " 16782/50000: episode: 295, duration: 0.145s, episode steps: 41, steps per second: 283, episode reward: 0.854, mean reward: 0.021 [-0.006, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.117 [-0.588, 0.180], loss: 0.000042, mean_absolute_error: 0.448538, mean_q: 0.669044\n",
      " 16805/50000: episode: 296, duration: 0.092s, episode steps: 23, steps per second: 249, episode reward: 0.962, mean reward: 0.042 [-0.002, 1.000], mean action: 0.696 [0.000, 2.000], mean observation: 0.055 [-0.080, 0.226], loss: 0.000042, mean_absolute_error: 0.461655, mean_q: 0.688352\n",
      " 16865/50000: episode: 297, duration: 0.241s, episode steps: 60, steps per second: 249, episode reward: 0.713, mean reward: 0.012 [-0.008, 1.000], mean action: 1.083 [0.000, 2.000], mean observation: -0.179 [-0.821, 0.220], loss: 0.000194, mean_absolute_error: 0.448360, mean_q: 0.668721\n",
      " 16924/50000: episode: 298, duration: 0.253s, episode steps: 59, steps per second: 233, episode reward: 0.695, mean reward: 0.012 [-0.009, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.188 [-0.240, 0.938], loss: 0.000049, mean_absolute_error: 0.451461, mean_q: 0.673862\n",
      " 16925/50000: episode: 299, duration: 0.006s, episode steps: 1, steps per second: 167, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [-0.010, 0.057], loss: 0.000031, mean_absolute_error: 0.428303, mean_q: 0.629844\n",
      " 16946/50000: episode: 300, duration: 0.075s, episode steps: 21, steps per second: 281, episode reward: 0.961, mean reward: 0.046 [-0.003, 1.000], mean action: 1.381 [0.000, 2.000], mean observation: -0.056 [-0.259, 0.120], loss: 0.000031, mean_absolute_error: 0.459022, mean_q: 0.687713\n",
      " 17003/50000: episode: 301, duration: 0.181s, episode steps: 57, steps per second: 315, episode reward: 0.721, mean reward: 0.013 [-0.009, 1.000], mean action: 0.877 [0.000, 2.000], mean observation: 0.178 [-0.230, 0.866], loss: 0.000128, mean_absolute_error: 0.451840, mean_q: 0.673364\n",
      " 17056/50000: episode: 302, duration: 0.162s, episode steps: 53, steps per second: 327, episode reward: 0.786, mean reward: 0.015 [-0.007, 1.000], mean action: 1.075 [0.000, 2.000], mean observation: -0.144 [-0.722, 0.200], loss: 0.000150, mean_absolute_error: 0.445469, mean_q: 0.663767\n",
      " 17103/50000: episode: 303, duration: 0.178s, episode steps: 47, steps per second: 264, episode reward: 0.812, mean reward: 0.017 [-0.007, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.138 [-0.210, 0.694], loss: 0.000030, mean_absolute_error: 0.451155, mean_q: 0.669705\n",
      " 17104/50000: episode: 304, duration: 0.006s, episode steps: 1, steps per second: 169, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.019 [-0.048, 0.010], loss: 0.000042, mean_absolute_error: 0.458215, mean_q: 0.654574\n",
      " 17150/50000: episode: 305, duration: 0.136s, episode steps: 46, steps per second: 338, episode reward: 0.848, mean reward: 0.018 [-0.005, 1.000], mean action: 1.152 [0.000, 2.000], mean observation: -0.119 [-0.526, 0.150], loss: 0.000028, mean_absolute_error: 0.453065, mean_q: 0.677612\n",
      " 17161/50000: episode: 306, duration: 0.035s, episode steps: 11, steps per second: 315, episode reward: 0.986, mean reward: 0.090 [-0.002, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.040 [-0.080, 0.158], loss: 0.000029, mean_absolute_error: 0.449744, mean_q: 0.671928\n",
      " 17162/50000: episode: 307, duration: 0.006s, episode steps: 1, steps per second: 179, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.009 [-0.010, 0.027], loss: 0.000028, mean_absolute_error: 0.481410, mean_q: 0.709725\n",
      " 17215/50000: episode: 308, duration: 0.207s, episode steps: 53, steps per second: 256, episode reward: 0.742, mean reward: 0.014 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.175 [-0.200, 0.809], loss: 0.000169, mean_absolute_error: 0.451958, mean_q: 0.674724\n",
      " 17251/50000: episode: 309, duration: 0.139s, episode steps: 36, steps per second: 258, episode reward: 0.927, mean reward: 0.026 [-0.003, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.076 [-0.100, 0.289], loss: 0.000609, mean_absolute_error: 0.456214, mean_q: 0.678979\n",
      " 17252/50000: episode: 310, duration: 0.008s, episode steps: 1, steps per second: 128, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.035 [-0.070, 0.000], loss: 0.000062, mean_absolute_error: 0.441114, mean_q: 0.656156\n",
      " 17314/50000: episode: 311, duration: 0.208s, episode steps: 62, steps per second: 298, episode reward: 0.657, mean reward: 0.011 [-0.010, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.207 [-0.240, 0.967], loss: 0.000037, mean_absolute_error: 0.449545, mean_q: 0.671096\n",
      " 17372/50000: episode: 312, duration: 0.193s, episode steps: 58, steps per second: 300, episode reward: 0.699, mean reward: 0.012 [-0.009, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.191 [-0.230, 0.900], loss: 0.000122, mean_absolute_error: 0.461537, mean_q: 0.691180\n",
      " 17399/50000: episode: 313, duration: 0.100s, episode steps: 27, steps per second: 269, episode reward: 0.945, mean reward: 0.035 [-0.003, 1.000], mean action: 1.259 [0.000, 2.000], mean observation: -0.065 [-0.305, 0.110], loss: 0.000034, mean_absolute_error: 0.441949, mean_q: 0.659654\n",
      " 17476/50000: episode: 314, duration: 0.229s, episode steps: 77, steps per second: 337, episode reward: 0.540, mean reward: 0.007 [-0.010, 1.000], mean action: 1.091 [0.000, 2.000], mean observation: -0.242 [-0.980, 0.210], loss: 0.000079, mean_absolute_error: 0.452449, mean_q: 0.675648\n",
      " 17525/50000: episode: 315, duration: 0.169s, episode steps: 49, steps per second: 290, episode reward: 0.813, mean reward: 0.017 [-0.006, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.136 [-0.170, 0.635], loss: 0.000167, mean_absolute_error: 0.461569, mean_q: 0.694181\n",
      " 17567/50000: episode: 316, duration: 0.179s, episode steps: 42, steps per second: 235, episode reward: 0.857, mean reward: 0.020 [-0.005, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.121 [-0.180, 0.516], loss: 0.000140, mean_absolute_error: 0.455154, mean_q: 0.676423\n",
      " 17601/50000: episode: 317, duration: 0.116s, episode steps: 34, steps per second: 292, episode reward: 0.898, mean reward: 0.026 [-0.005, 1.000], mean action: 1.235 [0.000, 2.000], mean observation: -0.097 [-0.467, 0.170], loss: 0.000265, mean_absolute_error: 0.461332, mean_q: 0.688279\n",
      " 17624/50000: episode: 318, duration: 0.077s, episode steps: 23, steps per second: 300, episode reward: 0.959, mean reward: 0.042 [-0.003, 1.000], mean action: 0.783 [0.000, 2.000], mean observation: 0.055 [-0.130, 0.263], loss: 0.000906, mean_absolute_error: 0.458109, mean_q: 0.681635\n",
      " 17682/50000: episode: 319, duration: 0.226s, episode steps: 58, steps per second: 257, episode reward: 0.731, mean reward: 0.013 [-0.008, 1.000], mean action: 1.121 [0.000, 2.000], mean observation: -0.168 [-0.845, 0.210], loss: 0.000060, mean_absolute_error: 0.457032, mean_q: 0.683703\n",
      " 17744/50000: episode: 320, duration: 0.192s, episode steps: 62, steps per second: 322, episode reward: 0.650, mean reward: 0.010 [-0.010, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.211 [-0.220, 0.996], loss: 0.000048, mean_absolute_error: 0.453158, mean_q: 0.678309\n",
      " 17773/50000: episode: 321, duration: 0.147s, episode steps: 29, steps per second: 197, episode reward: 0.933, mean reward: 0.032 [-0.003, 1.000], mean action: 0.724 [0.000, 2.000], mean observation: 0.074 [-0.140, 0.346], loss: 0.000029, mean_absolute_error: 0.465434, mean_q: 0.695953\n",
      " 17803/50000: episode: 322, duration: 0.097s, episode steps: 30, steps per second: 310, episode reward: 0.939, mean reward: 0.031 [-0.003, 1.000], mean action: 0.767 [0.000, 2.000], mean observation: 0.068 [-0.120, 0.306], loss: 0.000136, mean_absolute_error: 0.445927, mean_q: 0.669713\n",
      " 17827/50000: episode: 323, duration: 0.073s, episode steps: 24, steps per second: 328, episode reward: 0.950, mean reward: 0.040 [-0.003, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.059 [-0.150, 0.308], loss: 0.000050, mean_absolute_error: 0.464008, mean_q: 0.695841\n",
      " 17832/50000: episode: 324, duration: 0.018s, episode steps: 5, steps per second: 278, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [-0.050, 0.113], loss: 0.000021, mean_absolute_error: 0.440741, mean_q: 0.656118\n",
      " 17883/50000: episode: 325, duration: 0.163s, episode steps: 51, steps per second: 312, episode reward: 0.789, mean reward: 0.015 [-0.007, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.147 [-0.200, 0.714], loss: 0.000094, mean_absolute_error: 0.450647, mean_q: 0.673814\n",
      " 17930/50000: episode: 326, duration: 0.144s, episode steps: 47, steps per second: 326, episode reward: 0.813, mean reward: 0.017 [-0.007, 1.000], mean action: 0.872 [0.000, 2.000], mean observation: 0.140 [-0.190, 0.660], loss: 0.000046, mean_absolute_error: 0.453250, mean_q: 0.678953\n",
      " 17977/50000: episode: 327, duration: 0.157s, episode steps: 47, steps per second: 299, episode reward: 0.845, mean reward: 0.018 [-0.006, 1.000], mean action: 1.106 [0.000, 2.000], mean observation: -0.115 [-0.573, 0.180], loss: 0.000033, mean_absolute_error: 0.452282, mean_q: 0.676383\n",
      " 17990/50000: episode: 328, duration: 0.039s, episode steps: 13, steps per second: 330, episode reward: 0.983, mean reward: 0.076 [-0.002, 1.000], mean action: 1.538 [0.000, 2.000], mean observation: -0.042 [-0.163, 0.080], loss: 0.000089, mean_absolute_error: 0.471368, mean_q: 0.707909\n",
      " 18047/50000: episode: 329, duration: 0.178s, episode steps: 57, steps per second: 320, episode reward: 0.743, mean reward: 0.013 [-0.008, 1.000], mean action: 1.105 [0.000, 2.000], mean observation: -0.163 [-0.815, 0.210], loss: 0.000034, mean_absolute_error: 0.463618, mean_q: 0.692260\n",
      " 18086/50000: episode: 330, duration: 0.150s, episode steps: 39, steps per second: 259, episode reward: 0.867, mean reward: 0.022 [-0.006, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.112 [-0.170, 0.554], loss: 0.000387, mean_absolute_error: 0.461254, mean_q: 0.692134\n",
      " 18120/50000: episode: 331, duration: 0.115s, episode steps: 34, steps per second: 296, episode reward: 0.905, mean reward: 0.027 [-0.004, 1.000], mean action: 0.794 [0.000, 2.000], mean observation: 0.090 [-0.160, 0.444], loss: 0.000040, mean_absolute_error: 0.464767, mean_q: 0.694682\n",
      " 18180/50000: episode: 332, duration: 0.200s, episode steps: 60, steps per second: 300, episode reward: 0.699, mean reward: 0.012 [-0.009, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.187 [-0.858, 0.190], loss: 0.000142, mean_absolute_error: 0.453882, mean_q: 0.679559\n",
      " 18230/50000: episode: 333, duration: 0.190s, episode steps: 50, steps per second: 264, episode reward: 0.795, mean reward: 0.016 [-0.007, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.147 [-0.691, 0.210], loss: 0.000202, mean_absolute_error: 0.455155, mean_q: 0.681938\n",
      " 18249/50000: episode: 334, duration: 0.066s, episode steps: 19, steps per second: 290, episode reward: 0.973, mean reward: 0.051 [-0.002, 1.000], mean action: 1.368 [0.000, 2.000], mean observation: -0.051 [-0.185, 0.070], loss: 0.000751, mean_absolute_error: 0.459890, mean_q: 0.688720\n",
      " 18301/50000: episode: 335, duration: 0.167s, episode steps: 52, steps per second: 311, episode reward: 0.778, mean reward: 0.015 [-0.007, 1.000], mean action: 1.077 [0.000, 2.000], mean observation: -0.151 [-0.750, 0.190], loss: 0.000195, mean_absolute_error: 0.461158, mean_q: 0.692156\n",
      " 18331/50000: episode: 336, duration: 0.105s, episode steps: 30, steps per second: 287, episode reward: 0.932, mean reward: 0.031 [-0.004, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.071 [-0.353, 0.130], loss: 0.000137, mean_absolute_error: 0.464070, mean_q: 0.694183\n",
      " 18368/50000: episode: 337, duration: 0.127s, episode steps: 37, steps per second: 292, episode reward: 0.903, mean reward: 0.024 [-0.004, 1.000], mean action: 1.162 [0.000, 2.000], mean observation: -0.087 [-0.433, 0.150], loss: 0.000039, mean_absolute_error: 0.455111, mean_q: 0.683503\n",
      " 18400/50000: episode: 338, duration: 0.113s, episode steps: 32, steps per second: 283, episode reward: 0.930, mean reward: 0.029 [-0.003, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.075 [-0.322, 0.120], loss: 0.000041, mean_absolute_error: 0.462721, mean_q: 0.694373\n",
      " 18477/50000: episode: 339, duration: 0.243s, episode steps: 77, steps per second: 317, episode reward: 0.559, mean reward: 0.007 [-0.010, 1.000], mean action: 1.065 [0.000, 2.000], mean observation: -0.229 [-0.988, 0.220], loss: 0.000211, mean_absolute_error: 0.460817, mean_q: 0.690575\n",
      " 18529/50000: episode: 340, duration: 0.168s, episode steps: 52, steps per second: 310, episode reward: 0.755, mean reward: 0.015 [-0.008, 1.000], mean action: 0.865 [0.000, 2.000], mean observation: 0.168 [-0.200, 0.801], loss: 0.000030, mean_absolute_error: 0.458335, mean_q: 0.689586\n",
      " 18536/50000: episode: 341, duration: 0.023s, episode steps: 7, steps per second: 301, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.070, 0.124], loss: 0.000038, mean_absolute_error: 0.478856, mean_q: 0.718158\n",
      " 18558/50000: episode: 342, duration: 0.083s, episode steps: 22, steps per second: 265, episode reward: 0.957, mean reward: 0.043 [-0.003, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.059 [-0.140, 0.281], loss: 0.000032, mean_absolute_error: 0.449553, mean_q: 0.673043\n",
      " 18559/50000: episode: 343, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.022 [-0.045, 0.000], loss: 0.000018, mean_absolute_error: 0.462627, mean_q: 0.694570\n",
      " 18594/50000: episode: 344, duration: 0.105s, episode steps: 35, steps per second: 334, episode reward: 0.917, mean reward: 0.026 [-0.004, 1.000], mean action: 1.171 [0.000, 2.000], mean observation: -0.080 [-0.376, 0.120], loss: 0.000211, mean_absolute_error: 0.458169, mean_q: 0.684293\n",
      " 18613/50000: episode: 345, duration: 0.057s, episode steps: 19, steps per second: 330, episode reward: 0.966, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 2.000], mean observation: 0.053 [-0.130, 0.250], loss: 0.000041, mean_absolute_error: 0.454577, mean_q: 0.683547\n",
      " 18639/50000: episode: 346, duration: 0.099s, episode steps: 26, steps per second: 262, episode reward: 0.953, mean reward: 0.037 [-0.003, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.059 [-0.271, 0.100], loss: 0.000536, mean_absolute_error: 0.461004, mean_q: 0.690284\n",
      " 18656/50000: episode: 347, duration: 0.065s, episode steps: 17, steps per second: 261, episode reward: 0.979, mean reward: 0.058 [-0.001, 1.000], mean action: 0.471 [0.000, 2.000], mean observation: 0.053 [-0.090, 0.142], loss: 0.000073, mean_absolute_error: 0.470015, mean_q: 0.705104\n",
      " 18710/50000: episode: 348, duration: 0.175s, episode steps: 54, steps per second: 308, episode reward: 0.735, mean reward: 0.014 [-0.009, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.176 [-0.220, 0.851], loss: 0.000043, mean_absolute_error: 0.469701, mean_q: 0.702844\n",
      " 18739/50000: episode: 349, duration: 0.103s, episode steps: 29, steps per second: 281, episode reward: 0.925, mean reward: 0.032 [-0.004, 1.000], mean action: 0.724 [0.000, 2.000], mean observation: 0.079 [-0.160, 0.390], loss: 0.000031, mean_absolute_error: 0.453562, mean_q: 0.679369\n",
      " 18787/50000: episode: 350, duration: 0.157s, episode steps: 48, steps per second: 306, episode reward: 0.829, mean reward: 0.017 [-0.006, 1.000], mean action: 1.104 [0.000, 2.000], mean observation: -0.128 [-0.588, 0.170], loss: 0.000027, mean_absolute_error: 0.465688, mean_q: 0.699186\n",
      " 18833/50000: episode: 351, duration: 0.154s, episode steps: 46, steps per second: 299, episode reward: 0.813, mean reward: 0.018 [-0.007, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.140 [-0.200, 0.681], loss: 0.000199, mean_absolute_error: 0.460952, mean_q: 0.689728\n",
      " 18864/50000: episode: 352, duration: 0.104s, episode steps: 31, steps per second: 298, episode reward: 0.931, mean reward: 0.030 [-0.003, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.075 [-0.332, 0.100], loss: 0.000042, mean_absolute_error: 0.469017, mean_q: 0.707175\n",
      " 18936/50000: episode: 353, duration: 0.220s, episode steps: 72, steps per second: 328, episode reward: 0.592, mean reward: 0.008 [-0.009, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.226 [-0.935, 0.200], loss: 0.000214, mean_absolute_error: 0.463037, mean_q: 0.691612\n",
      " 18990/50000: episode: 354, duration: 0.186s, episode steps: 54, steps per second: 291, episode reward: 0.754, mean reward: 0.014 [-0.008, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.165 [-0.782, 0.200], loss: 0.000173, mean_absolute_error: 0.467886, mean_q: 0.700837\n",
      " 19037/50000: episode: 355, duration: 0.160s, episode steps: 47, steps per second: 293, episode reward: 0.837, mean reward: 0.018 [-0.006, 1.000], mean action: 1.128 [0.000, 2.000], mean observation: -0.122 [-0.595, 0.180], loss: 0.000039, mean_absolute_error: 0.467865, mean_q: 0.701374\n",
      " 19082/50000: episode: 356, duration: 0.132s, episode steps: 45, steps per second: 341, episode reward: 0.840, mean reward: 0.019 [-0.006, 1.000], mean action: 0.844 [0.000, 2.000], mean observation: 0.127 [-0.190, 0.569], loss: 0.000048, mean_absolute_error: 0.463904, mean_q: 0.696698\n",
      " 19146/50000: episode: 357, duration: 0.211s, episode steps: 64, steps per second: 303, episode reward: 0.648, mean reward: 0.010 [-0.010, 1.000], mean action: 0.891 [0.000, 2.000], mean observation: 0.206 [-0.200, 0.990], loss: 0.000041, mean_absolute_error: 0.464420, mean_q: 0.696581\n",
      " 19147/50000: episode: 358, duration: 0.007s, episode steps: 1, steps per second: 151, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.027 [-0.010, 0.063], loss: 0.000009, mean_absolute_error: 0.507512, mean_q: 0.755352\n",
      " 19167/50000: episode: 359, duration: 0.063s, episode steps: 20, steps per second: 317, episode reward: 0.968, mean reward: 0.048 [-0.002, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.050 [-0.219, 0.080], loss: 0.000023, mean_absolute_error: 0.466080, mean_q: 0.700917\n",
      " 19219/50000: episode: 360, duration: 0.160s, episode steps: 52, steps per second: 326, episode reward: 0.796, mean reward: 0.015 [-0.007, 1.000], mean action: 1.096 [0.000, 2.000], mean observation: -0.143 [-0.663, 0.180], loss: 0.000028, mean_absolute_error: 0.464714, mean_q: 0.696836\n",
      " 19245/50000: episode: 361, duration: 0.097s, episode steps: 26, steps per second: 268, episode reward: 0.956, mean reward: 0.037 [-0.002, 1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.060 [-0.230, 0.090], loss: 0.000032, mean_absolute_error: 0.469101, mean_q: 0.702020\n",
      " 19246/50000: episode: 362, duration: 0.006s, episode steps: 1, steps per second: 168, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [-0.010, 0.038], loss: 0.000015, mean_absolute_error: 0.494753, mean_q: 0.747851\n",
      " 19273/50000: episode: 363, duration: 0.083s, episode steps: 27, steps per second: 325, episode reward: 0.953, mean reward: 0.035 [-0.003, 1.000], mean action: 1.222 [0.000, 2.000], mean observation: -0.060 [-0.257, 0.100], loss: 0.000292, mean_absolute_error: 0.452743, mean_q: 0.673924\n",
      " 19327/50000: episode: 364, duration: 0.180s, episode steps: 54, steps per second: 299, episode reward: 0.766, mean reward: 0.014 [-0.008, 1.000], mean action: 1.093 [0.000, 2.000], mean observation: -0.157 [-0.754, 0.180], loss: 0.000161, mean_absolute_error: 0.468311, mean_q: 0.704560\n",
      " 19365/50000: episode: 365, duration: 0.113s, episode steps: 38, steps per second: 336, episode reward: 0.884, mean reward: 0.023 [-0.005, 1.000], mean action: 0.789 [0.000, 2.000], mean observation: 0.102 [-0.180, 0.497], loss: 0.000040, mean_absolute_error: 0.466806, mean_q: 0.700106\n",
      " 19426/50000: episode: 366, duration: 0.208s, episode steps: 61, steps per second: 293, episode reward: 0.696, mean reward: 0.011 [-0.009, 1.000], mean action: 1.098 [0.000, 2.000], mean observation: -0.184 [-0.901, 0.210], loss: 0.000272, mean_absolute_error: 0.466769, mean_q: 0.698599\n",
      " 19464/50000: episode: 367, duration: 0.118s, episode steps: 38, steps per second: 322, episode reward: 0.884, mean reward: 0.023 [-0.005, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.102 [-0.170, 0.485], loss: 0.000055, mean_absolute_error: 0.472796, mean_q: 0.711078\n",
      " 19529/50000: episode: 368, duration: 0.212s, episode steps: 65, steps per second: 307, episode reward: 0.649, mean reward: 0.010 [-0.010, 1.000], mean action: 1.092 [0.000, 2.000], mean observation: -0.203 [-0.980, 0.220], loss: 0.000035, mean_absolute_error: 0.467774, mean_q: 0.699363\n",
      " 19582/50000: episode: 369, duration: 0.170s, episode steps: 53, steps per second: 312, episode reward: 0.775, mean reward: 0.015 [-0.008, 1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.151 [-0.751, 0.200], loss: 0.000094, mean_absolute_error: 0.475217, mean_q: 0.711631\n",
      " 19593/50000: episode: 370, duration: 0.038s, episode steps: 11, steps per second: 290, episode reward: 0.988, mean reward: 0.090 [-0.001, 1.000], mean action: 1.455 [0.000, 2.000], mean observation: -0.042 [-0.138, 0.050], loss: 0.001595, mean_absolute_error: 0.464101, mean_q: 0.692933\n",
      " 19656/50000: episode: 371, duration: 0.210s, episode steps: 63, steps per second: 300, episode reward: 0.699, mean reward: 0.011 [-0.009, 1.000], mean action: 1.095 [0.000, 2.000], mean observation: -0.177 [-0.884, 0.200], loss: 0.000065, mean_absolute_error: 0.471306, mean_q: 0.708071\n",
      " 19676/50000: episode: 372, duration: 0.077s, episode steps: 20, steps per second: 261, episode reward: 0.965, mean reward: 0.048 [-0.002, 1.000], mean action: 0.650 [0.000, 2.000], mean observation: 0.051 [-0.110, 0.243], loss: 0.000036, mean_absolute_error: 0.469220, mean_q: 0.706465\n",
      " 19718/50000: episode: 373, duration: 0.121s, episode steps: 42, steps per second: 347, episode reward: 0.864, mean reward: 0.021 [-0.005, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.109 [-0.180, 0.546], loss: 0.000029, mean_absolute_error: 0.472913, mean_q: 0.708026\n",
      " 19719/50000: episode: 374, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.040 [-0.010, 0.091], loss: 0.000149, mean_absolute_error: 0.389316, mean_q: 0.577639\n",
      " 19750/50000: episode: 375, duration: 0.117s, episode steps: 31, steps per second: 266, episode reward: 0.914, mean reward: 0.029 [-0.004, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.089 [-0.411, 0.170], loss: 0.000041, mean_absolute_error: 0.458875, mean_q: 0.684855\n",
      " 19796/50000: episode: 376, duration: 0.165s, episode steps: 46, steps per second: 279, episode reward: 0.846, mean reward: 0.018 [-0.006, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.119 [-0.140, 0.550], loss: 0.000247, mean_absolute_error: 0.475888, mean_q: 0.710855\n",
      " 19857/50000: episode: 377, duration: 0.196s, episode steps: 61, steps per second: 312, episode reward: 0.677, mean reward: 0.011 [-0.009, 1.000], mean action: 0.918 [0.000, 2.000], mean observation: 0.197 [-0.200, 0.931], loss: 0.000257, mean_absolute_error: 0.471066, mean_q: 0.705759\n",
      " 19887/50000: episode: 378, duration: 0.116s, episode steps: 30, steps per second: 258, episode reward: 0.926, mean reward: 0.031 [-0.004, 1.000], mean action: 0.733 [0.000, 2.000], mean observation: 0.081 [-0.150, 0.361], loss: 0.000034, mean_absolute_error: 0.469621, mean_q: 0.701230\n",
      " 19937/50000: episode: 379, duration: 0.160s, episode steps: 50, steps per second: 313, episode reward: 0.779, mean reward: 0.016 [-0.008, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.156 [-0.200, 0.759], loss: 0.000102, mean_absolute_error: 0.463892, mean_q: 0.696736\n",
      " 19978/50000: episode: 380, duration: 0.151s, episode steps: 41, steps per second: 271, episode reward: 0.878, mean reward: 0.021 [-0.004, 1.000], mean action: 0.829 [0.000, 2.000], mean observation: 0.107 [-0.160, 0.446], loss: 0.000114, mean_absolute_error: 0.474851, mean_q: 0.709695\n",
      " 20039/50000: episode: 381, duration: 0.196s, episode steps: 61, steps per second: 312, episode reward: 0.699, mean reward: 0.011 [-0.009, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.177 [-0.920, 0.210], loss: 0.000348, mean_absolute_error: 0.467912, mean_q: 0.702108\n",
      " 20076/50000: episode: 382, duration: 0.143s, episode steps: 37, steps per second: 259, episode reward: 0.876, mean reward: 0.024 [-0.005, 1.000], mean action: 0.811 [0.000, 2.000], mean observation: 0.109 [-0.200, 0.539], loss: 0.000053, mean_absolute_error: 0.473947, mean_q: 0.711723\n",
      " 20121/50000: episode: 383, duration: 0.145s, episode steps: 45, steps per second: 310, episode reward: 0.839, mean reward: 0.019 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.125 [-0.180, 0.590], loss: 0.000055, mean_absolute_error: 0.474341, mean_q: 0.712479\n",
      " 20150/50000: episode: 384, duration: 0.118s, episode steps: 29, steps per second: 245, episode reward: 0.932, mean reward: 0.032 [-0.004, 1.000], mean action: 0.724 [0.000, 2.000], mean observation: 0.074 [-0.140, 0.355], loss: 0.000034, mean_absolute_error: 0.468401, mean_q: 0.703973\n",
      " 20212/50000: episode: 385, duration: 0.199s, episode steps: 62, steps per second: 312, episode reward: 0.657, mean reward: 0.011 [-0.010, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.206 [-0.967, 0.230], loss: 0.000045, mean_absolute_error: 0.468446, mean_q: 0.701582\n",
      " 20252/50000: episode: 386, duration: 0.145s, episode steps: 40, steps per second: 276, episode reward: 0.867, mean reward: 0.022 [-0.005, 1.000], mean action: 1.175 [0.000, 2.000], mean observation: -0.111 [-0.547, 0.170], loss: 0.000050, mean_absolute_error: 0.467030, mean_q: 0.699826\n",
      " 20316/50000: episode: 387, duration: 0.244s, episode steps: 64, steps per second: 262, episode reward: 0.659, mean reward: 0.010 [-0.009, 1.000], mean action: 1.078 [0.000, 2.000], mean observation: -0.201 [-0.944, 0.220], loss: 0.000031, mean_absolute_error: 0.469228, mean_q: 0.701715\n",
      " 20360/50000: episode: 388, duration: 0.183s, episode steps: 44, steps per second: 241, episode reward: 0.828, mean reward: 0.019 [-0.007, 1.000], mean action: 1.159 [0.000, 2.000], mean observation: -0.133 [-0.655, 0.210], loss: 0.000026, mean_absolute_error: 0.465426, mean_q: 0.699229\n",
      " 20361/50000: episode: 389, duration: 0.008s, episode steps: 1, steps per second: 127, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [-0.010, 0.066], loss: 0.000010, mean_absolute_error: 0.409042, mean_q: 0.599668\n",
      " 20362/50000: episode: 390, duration: 0.008s, episode steps: 1, steps per second: 125, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.016 [-0.041, 0.010], loss: 0.000012, mean_absolute_error: 0.421178, mean_q: 0.641473\n",
      " 20363/50000: episode: 391, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.027 [-0.063, 0.010], loss: 0.000011, mean_absolute_error: 0.444615, mean_q: 0.673895\n",
      " 20394/50000: episode: 392, duration: 0.138s, episode steps: 31, steps per second: 224, episode reward: 0.923, mean reward: 0.030 [-0.004, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.071 [-0.401, 0.160], loss: 0.000293, mean_absolute_error: 0.471756, mean_q: 0.705062\n",
      " 20424/50000: episode: 393, duration: 0.131s, episode steps: 30, steps per second: 230, episode reward: 0.940, mean reward: 0.031 [-0.003, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.066 [-0.277, 0.120], loss: 0.000036, mean_absolute_error: 0.475359, mean_q: 0.714101\n",
      " 20470/50000: episode: 394, duration: 0.187s, episode steps: 46, steps per second: 246, episode reward: 0.833, mean reward: 0.018 [-0.006, 1.000], mean action: 1.152 [0.000, 2.000], mean observation: -0.128 [-0.600, 0.180], loss: 0.000030, mean_absolute_error: 0.469236, mean_q: 0.704322\n",
      " 20531/50000: episode: 395, duration: 0.214s, episode steps: 61, steps per second: 285, episode reward: 0.676, mean reward: 0.011 [-0.010, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.196 [-0.220, 0.964], loss: 0.000040, mean_absolute_error: 0.469988, mean_q: 0.703008\n",
      " 20554/50000: episode: 396, duration: 0.084s, episode steps: 23, steps per second: 273, episode reward: 0.959, mean reward: 0.042 [-0.003, 1.000], mean action: 0.783 [0.000, 2.000], mean observation: 0.056 [-0.110, 0.258], loss: 0.000025, mean_absolute_error: 0.472979, mean_q: 0.711686\n",
      " 20555/50000: episode: 397, duration: 0.007s, episode steps: 1, steps per second: 154, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.041 [-0.092, 0.010], loss: 0.012000, mean_absolute_error: 0.509326, mean_q: 0.758071\n",
      " 20573/50000: episode: 398, duration: 0.061s, episode steps: 18, steps per second: 294, episode reward: 0.972, mean reward: 0.054 [-0.002, 1.000], mean action: 1.444 [1.000, 2.000], mean observation: -0.047 [-0.209, 0.080], loss: 0.000117, mean_absolute_error: 0.468776, mean_q: 0.702339\n",
      " 20585/50000: episode: 399, duration: 0.041s, episode steps: 12, steps per second: 292, episode reward: 0.985, mean reward: 0.082 [-0.002, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.039 [-0.070, 0.155], loss: 0.000070, mean_absolute_error: 0.468647, mean_q: 0.704201\n",
      " 20593/50000: episode: 400, duration: 0.034s, episode steps: 8, steps per second: 233, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: -0.046 [-0.109, 0.040], loss: 0.000454, mean_absolute_error: 0.488036, mean_q: 0.733710\n",
      " 20610/50000: episode: 401, duration: 0.066s, episode steps: 17, steps per second: 258, episode reward: 0.975, mean reward: 0.057 [-0.002, 1.000], mean action: 1.412 [1.000, 2.000], mean observation: -0.048 [-0.197, 0.070], loss: 0.000056, mean_absolute_error: 0.467658, mean_q: 0.698704\n",
      " 20638/50000: episode: 402, duration: 0.089s, episode steps: 28, steps per second: 314, episode reward: 0.944, mean reward: 0.034 [-0.003, 1.000], mean action: 0.821 [0.000, 2.000], mean observation: 0.064 [-0.120, 0.303], loss: 0.000058, mean_absolute_error: 0.472702, mean_q: 0.708427\n",
      " 20659/50000: episode: 403, duration: 0.081s, episode steps: 21, steps per second: 260, episode reward: 0.970, mean reward: 0.046 [-0.002, 1.000], mean action: 1.238 [0.000, 2.000], mean observation: -0.054 [-0.185, 0.050], loss: 0.000060, mean_absolute_error: 0.474997, mean_q: 0.713564\n",
      " 20704/50000: episode: 404, duration: 0.153s, episode steps: 45, steps per second: 293, episode reward: 0.855, mean reward: 0.019 [-0.005, 1.000], mean action: 0.911 [0.000, 2.000], mean observation: 0.114 [-0.180, 0.537], loss: 0.000391, mean_absolute_error: 0.473966, mean_q: 0.710343\n",
      " 20742/50000: episode: 405, duration: 0.128s, episode steps: 38, steps per second: 297, episode reward: 0.900, mean reward: 0.024 [-0.005, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.076 [-0.458, 0.180], loss: 0.000047, mean_absolute_error: 0.471234, mean_q: 0.707180\n",
      " 21042/50000: episode: 406, duration: 1.002s, episode steps: 300, steps per second: 300, episode reward: -2.999, mean reward: -0.010 [-0.010, -0.010], mean action: 1.130 [0.000, 2.000], mean observation: 0.673 [0.010, 1.000], loss: 0.000097, mean_absolute_error: 0.469047, mean_q: 0.702617\n",
      " 21102/50000: episode: 407, duration: 0.174s, episode steps: 60, steps per second: 345, episode reward: 0.705, mean reward: 0.012 [-0.009, 1.000], mean action: 0.900 [0.000, 2.000], mean observation: 0.180 [-0.210, 0.900], loss: 0.000381, mean_absolute_error: 0.466068, mean_q: 0.696976\n",
      " 21119/50000: episode: 408, duration: 0.063s, episode steps: 17, steps per second: 270, episode reward: 0.977, mean reward: 0.057 [-0.002, 1.000], mean action: 0.706 [0.000, 2.000], mean observation: 0.045 [-0.070, 0.178], loss: 0.000052, mean_absolute_error: 0.465823, mean_q: 0.697513\n",
      " 21156/50000: episode: 409, duration: 0.129s, episode steps: 37, steps per second: 287, episode reward: 0.899, mean reward: 0.024 [-0.004, 1.000], mean action: 0.811 [0.000, 2.000], mean observation: 0.090 [-0.150, 0.445], loss: 0.000050, mean_absolute_error: 0.467706, mean_q: 0.701918\n",
      " 21210/50000: episode: 410, duration: 0.189s, episode steps: 54, steps per second: 286, episode reward: 0.754, mean reward: 0.014 [-0.008, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.166 [-0.230, 0.773], loss: 0.000204, mean_absolute_error: 0.466992, mean_q: 0.700040\n",
      " 21225/50000: episode: 411, duration: 0.046s, episode steps: 15, steps per second: 324, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 0.533 [0.000, 2.000], mean observation: 0.044 [-0.090, 0.188], loss: 0.000032, mean_absolute_error: 0.474573, mean_q: 0.715369\n",
      " 21226/50000: episode: 412, duration: 0.006s, episode steps: 1, steps per second: 179, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.010 [-0.010, 0.030], loss: 0.000019, mean_absolute_error: 0.496306, mean_q: 0.744721\n",
      " 21265/50000: episode: 413, duration: 0.122s, episode steps: 39, steps per second: 320, episode reward: 0.859, mean reward: 0.022 [-0.006, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.119 [-0.581, 0.190], loss: 0.000161, mean_absolute_error: 0.471502, mean_q: 0.706576\n",
      " 21317/50000: episode: 414, duration: 0.177s, episode steps: 52, steps per second: 294, episode reward: 0.741, mean reward: 0.014 [-0.008, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.177 [-0.847, 0.210], loss: 0.000054, mean_absolute_error: 0.466216, mean_q: 0.700424\n",
      " 21318/50000: episode: 415, duration: 0.006s, episode steps: 1, steps per second: 172, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.022 [-0.034, -0.010], loss: 0.000051, mean_absolute_error: 0.467714, mean_q: 0.707674\n",
      " 21349/50000: episode: 416, duration: 0.091s, episode steps: 31, steps per second: 340, episode reward: 0.942, mean reward: 0.030 [-0.003, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.067 [-0.100, 0.268], loss: 0.000390, mean_absolute_error: 0.465048, mean_q: 0.697533\n",
      " 21392/50000: episode: 417, duration: 0.139s, episode steps: 43, steps per second: 309, episode reward: 0.866, mean reward: 0.020 [-0.005, 1.000], mean action: 1.093 [0.000, 2.000], mean observation: -0.108 [-0.519, 0.180], loss: 0.000059, mean_absolute_error: 0.467471, mean_q: 0.701159\n",
      " 21393/50000: episode: 418, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [-0.010, 0.046], loss: 0.012134, mean_absolute_error: 0.497784, mean_q: 0.739540\n",
      " 21434/50000: episode: 419, duration: 0.159s, episode steps: 41, steps per second: 258, episode reward: 0.895, mean reward: 0.022 [-0.004, 1.000], mean action: 0.854 [0.000, 2.000], mean observation: 0.089 [-0.160, 0.432], loss: 0.000062, mean_absolute_error: 0.466169, mean_q: 0.697253\n",
      " 21488/50000: episode: 420, duration: 0.221s, episode steps: 54, steps per second: 244, episode reward: 0.728, mean reward: 0.013 [-0.009, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.183 [-0.857, 0.190], loss: 0.000112, mean_absolute_error: 0.468998, mean_q: 0.701901\n",
      " 21510/50000: episode: 421, duration: 0.071s, episode steps: 22, steps per second: 310, episode reward: 0.956, mean reward: 0.043 [-0.003, 1.000], mean action: 1.409 [0.000, 2.000], mean observation: -0.061 [-0.285, 0.120], loss: 0.000029, mean_absolute_error: 0.462181, mean_q: 0.694620\n",
      " 21575/50000: episode: 422, duration: 0.232s, episode steps: 65, steps per second: 280, episode reward: 0.686, mean reward: 0.011 [-0.009, 1.000], mean action: 0.908 [0.000, 2.000], mean observation: 0.181 [-0.190, 0.891], loss: 0.000032, mean_absolute_error: 0.463152, mean_q: 0.693767\n",
      " 21623/50000: episode: 423, duration: 0.157s, episode steps: 48, steps per second: 307, episode reward: 0.795, mean reward: 0.017 [-0.007, 1.000], mean action: 1.146 [0.000, 2.000], mean observation: -0.153 [-0.690, 0.190], loss: 0.000118, mean_absolute_error: 0.464324, mean_q: 0.693538\n",
      " 21666/50000: episode: 424, duration: 0.168s, episode steps: 43, steps per second: 256, episode reward: 0.879, mean reward: 0.020 [-0.005, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.098 [-0.150, 0.472], loss: 0.000034, mean_absolute_error: 0.471755, mean_q: 0.709936\n",
      " 21681/50000: episode: 425, duration: 0.066s, episode steps: 15, steps per second: 226, episode reward: 0.981, mean reward: 0.065 [-0.002, 1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.045 [-0.166, 0.060], loss: 0.000029, mean_absolute_error: 0.467820, mean_q: 0.700393\n",
      " 21716/50000: episode: 426, duration: 0.167s, episode steps: 35, steps per second: 210, episode reward: 0.898, mean reward: 0.026 [-0.004, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.098 [-0.443, 0.140], loss: 0.000027, mean_absolute_error: 0.470740, mean_q: 0.706308\n",
      " 21778/50000: episode: 427, duration: 0.256s, episode steps: 62, steps per second: 242, episode reward: 0.657, mean reward: 0.011 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.208 [-0.928, 0.210], loss: 0.000088, mean_absolute_error: 0.465065, mean_q: 0.697399\n",
      " 21824/50000: episode: 428, duration: 0.190s, episode steps: 46, steps per second: 242, episode reward: 0.874, mean reward: 0.019 [-0.005, 1.000], mean action: 0.891 [0.000, 2.000], mean observation: 0.100 [-0.140, 0.455], loss: 0.000040, mean_absolute_error: 0.477285, mean_q: 0.716264\n",
      " 21840/50000: episode: 429, duration: 0.064s, episode steps: 16, steps per second: 252, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: -0.045 [-0.195, 0.080], loss: 0.000034, mean_absolute_error: 0.473325, mean_q: 0.709817\n",
      " 21884/50000: episode: 430, duration: 0.142s, episode steps: 44, steps per second: 309, episode reward: 0.813, mean reward: 0.018 [-0.007, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.146 [-0.691, 0.200], loss: 0.000030, mean_absolute_error: 0.464507, mean_q: 0.696396\n",
      " 21956/50000: episode: 431, duration: 0.238s, episode steps: 72, steps per second: 302, episode reward: 0.680, mean reward: 0.009 [-0.009, 1.000], mean action: 0.944 [0.000, 2.000], mean observation: 0.166 [-0.220, 0.907], loss: 0.000049, mean_absolute_error: 0.464302, mean_q: 0.695077\n",
      " 21977/50000: episode: 432, duration: 0.069s, episode steps: 21, steps per second: 302, episode reward: 0.967, mean reward: 0.046 [-0.002, 1.000], mean action: 0.762 [0.000, 2.000], mean observation: 0.051 [-0.100, 0.219], loss: 0.000034, mean_absolute_error: 0.472484, mean_q: 0.710698\n",
      " 22034/50000: episode: 433, duration: 0.207s, episode steps: 57, steps per second: 275, episode reward: 0.727, mean reward: 0.013 [-0.008, 1.000], mean action: 0.877 [0.000, 2.000], mean observation: 0.176 [-0.200, 0.837], loss: 0.000037, mean_absolute_error: 0.465876, mean_q: 0.697479\n",
      " 22056/50000: episode: 434, duration: 0.097s, episode steps: 22, steps per second: 226, episode reward: 0.957, mean reward: 0.043 [-0.003, 1.000], mean action: 1.409 [0.000, 2.000], mean observation: -0.058 [-0.280, 0.130], loss: 0.000044, mean_absolute_error: 0.477376, mean_q: 0.715143\n",
      " 22115/50000: episode: 435, duration: 0.225s, episode steps: 59, steps per second: 262, episode reward: 0.749, mean reward: 0.013 [-0.008, 1.000], mean action: 0.932 [0.000, 2.000], mean observation: 0.158 [-0.190, 0.751], loss: 0.000035, mean_absolute_error: 0.470774, mean_q: 0.705889\n",
      " 22147/50000: episode: 436, duration: 0.135s, episode steps: 32, steps per second: 236, episode reward: 0.928, mean reward: 0.029 [-0.004, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.067 [-0.150, 0.351], loss: 0.000158, mean_absolute_error: 0.470505, mean_q: 0.702585\n",
      " 22161/50000: episode: 437, duration: 0.053s, episode steps: 14, steps per second: 264, episode reward: 0.981, mean reward: 0.070 [-0.002, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.044 [-0.080, 0.170], loss: 0.000060, mean_absolute_error: 0.482038, mean_q: 0.723696\n",
      " 22186/50000: episode: 438, duration: 0.094s, episode steps: 25, steps per second: 265, episode reward: 0.964, mean reward: 0.039 [-0.002, 1.000], mean action: 0.880 [0.000, 2.000], mean observation: 0.052 [-0.060, 0.206], loss: 0.000052, mean_absolute_error: 0.486220, mean_q: 0.731984\n",
      " 22235/50000: episode: 439, duration: 0.206s, episode steps: 49, steps per second: 237, episode reward: 0.770, mean reward: 0.016 [-0.008, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.161 [-0.797, 0.220], loss: 0.000111, mean_absolute_error: 0.467383, mean_q: 0.699901\n",
      " 22288/50000: episode: 440, duration: 0.172s, episode steps: 53, steps per second: 307, episode reward: 0.725, mean reward: 0.014 [-0.009, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.180 [-0.901, 0.230], loss: 0.000100, mean_absolute_error: 0.471143, mean_q: 0.705627\n",
      " 22327/50000: episode: 441, duration: 0.156s, episode steps: 39, steps per second: 250, episode reward: 0.884, mean reward: 0.023 [-0.005, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.099 [-0.170, 0.498], loss: 0.000074, mean_absolute_error: 0.471140, mean_q: 0.704196\n",
      " 22372/50000: episode: 442, duration: 0.194s, episode steps: 45, steps per second: 231, episode reward: 0.810, mean reward: 0.018 [-0.007, 1.000], mean action: 1.178 [0.000, 2.000], mean observation: -0.145 [-0.692, 0.220], loss: 0.000040, mean_absolute_error: 0.470726, mean_q: 0.704649\n",
      " 22406/50000: episode: 443, duration: 0.140s, episode steps: 34, steps per second: 242, episode reward: 0.909, mean reward: 0.027 [-0.004, 1.000], mean action: 1.206 [0.000, 2.000], mean observation: -0.088 [-0.421, 0.140], loss: 0.000037, mean_absolute_error: 0.466710, mean_q: 0.697095\n",
      " 22456/50000: episode: 444, duration: 0.175s, episode steps: 50, steps per second: 286, episode reward: 0.802, mean reward: 0.016 [-0.007, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.137 [-0.680, 0.210], loss: 0.000064, mean_absolute_error: 0.464424, mean_q: 0.697074\n",
      " 22483/50000: episode: 445, duration: 0.119s, episode steps: 27, steps per second: 227, episode reward: 0.953, mean reward: 0.035 [-0.003, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.059 [-0.100, 0.260], loss: 0.000026, mean_absolute_error: 0.470677, mean_q: 0.705041\n",
      " 22515/50000: episode: 446, duration: 0.121s, episode steps: 32, steps per second: 264, episode reward: 0.926, mean reward: 0.029 [-0.004, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.075 [-0.367, 0.140], loss: 0.000025, mean_absolute_error: 0.461282, mean_q: 0.690524\n",
      " 22541/50000: episode: 447, duration: 0.078s, episode steps: 26, steps per second: 332, episode reward: 0.945, mean reward: 0.036 [-0.003, 1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.065 [-0.315, 0.140], loss: 0.000151, mean_absolute_error: 0.473576, mean_q: 0.710200\n",
      " 22586/50000: episode: 448, duration: 0.194s, episode steps: 45, steps per second: 233, episode reward: 0.801, mean reward: 0.018 [-0.007, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.152 [-0.728, 0.200], loss: 0.000061, mean_absolute_error: 0.470008, mean_q: 0.703669\n",
      " 22587/50000: episode: 449, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.019 [-0.039, 0.000], loss: 0.000068, mean_absolute_error: 0.467404, mean_q: 0.704131\n",
      " 22613/50000: episode: 450, duration: 0.098s, episode steps: 26, steps per second: 265, episode reward: 0.946, mean reward: 0.036 [-0.003, 1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.063 [-0.309, 0.140], loss: 0.000029, mean_absolute_error: 0.460726, mean_q: 0.690724\n",
      " 22653/50000: episode: 451, duration: 0.136s, episode steps: 40, steps per second: 294, episode reward: 0.888, mean reward: 0.022 [-0.005, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.096 [-0.454, 0.150], loss: 0.000041, mean_absolute_error: 0.460669, mean_q: 0.687558\n",
      " 22707/50000: episode: 452, duration: 0.203s, episode steps: 54, steps per second: 265, episode reward: 0.738, mean reward: 0.014 [-0.008, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.174 [-0.844, 0.240], loss: 0.000050, mean_absolute_error: 0.460273, mean_q: 0.685554\n",
      " 22743/50000: episode: 453, duration: 0.155s, episode steps: 36, steps per second: 232, episode reward: 0.920, mean reward: 0.026 [-0.003, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.075 [-0.100, 0.349], loss: 0.000413, mean_absolute_error: 0.476012, mean_q: 0.712105\n",
      " 22804/50000: episode: 454, duration: 0.240s, episode steps: 61, steps per second: 254, episode reward: 0.712, mean reward: 0.012 [-0.009, 1.000], mean action: 0.885 [0.000, 2.000], mean observation: 0.175 [-0.200, 0.861], loss: 0.000064, mean_absolute_error: 0.472960, mean_q: 0.707001\n",
      " 22843/50000: episode: 455, duration: 0.155s, episode steps: 39, steps per second: 251, episode reward: 0.886, mean reward: 0.023 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.094 [-0.483, 0.170], loss: 0.000039, mean_absolute_error: 0.471497, mean_q: 0.704368\n",
      " 22888/50000: episode: 456, duration: 0.159s, episode steps: 45, steps per second: 283, episode reward: 0.878, mean reward: 0.020 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.092 [-0.130, 0.451], loss: 0.000275, mean_absolute_error: 0.469964, mean_q: 0.701613\n",
      " 22944/50000: episode: 457, duration: 0.236s, episode steps: 56, steps per second: 237, episode reward: 0.766, mean reward: 0.014 [-0.007, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.150 [-0.170, 0.740], loss: 0.000185, mean_absolute_error: 0.470317, mean_q: 0.700870\n",
      " 22999/50000: episode: 458, duration: 0.224s, episode steps: 55, steps per second: 246, episode reward: 0.720, mean reward: 0.013 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.184 [-0.874, 0.210], loss: 0.000046, mean_absolute_error: 0.465116, mean_q: 0.694291\n",
      " 23000/50000: episode: 459, duration: 0.006s, episode steps: 1, steps per second: 155, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.013 [-0.016, -0.010], loss: 0.000063, mean_absolute_error: 0.466820, mean_q: 0.700519\n",
      " 23039/50000: episode: 460, duration: 0.141s, episode steps: 39, steps per second: 277, episode reward: 0.906, mean reward: 0.023 [-0.004, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.084 [-0.120, 0.389], loss: 0.000058, mean_absolute_error: 0.461570, mean_q: 0.688058\n",
      " 23040/50000: episode: 461, duration: 0.006s, episode steps: 1, steps per second: 179, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.034 [-0.068, 0.000], loss: 0.000010, mean_absolute_error: 0.492862, mean_q: 0.740426\n",
      " 23073/50000: episode: 462, duration: 0.112s, episode steps: 33, steps per second: 294, episode reward: 0.942, mean reward: 0.029 [-0.003, 1.000], mean action: 0.848 [0.000, 2.000], mean observation: 0.063 [-0.080, 0.268], loss: 0.000041, mean_absolute_error: 0.458675, mean_q: 0.684592\n",
      " 23122/50000: episode: 463, duration: 0.176s, episode steps: 49, steps per second: 279, episode reward: 0.767, mean reward: 0.016 [-0.008, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.165 [-0.802, 0.210], loss: 0.000037, mean_absolute_error: 0.468412, mean_q: 0.699373\n",
      " 23178/50000: episode: 464, duration: 0.203s, episode steps: 56, steps per second: 275, episode reward: 0.698, mean reward: 0.012 [-0.010, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.188 [-0.974, 0.300], loss: 0.000043, mean_absolute_error: 0.460239, mean_q: 0.686498\n",
      " 23204/50000: episode: 465, duration: 0.079s, episode steps: 26, steps per second: 331, episode reward: 0.942, mean reward: 0.036 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.067 [-0.332, 0.130], loss: 0.000045, mean_absolute_error: 0.472949, mean_q: 0.703744\n",
      " 23265/50000: episode: 466, duration: 0.220s, episode steps: 61, steps per second: 277, episode reward: 0.691, mean reward: 0.011 [-0.010, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.167 [-0.967, 0.290], loss: 0.000282, mean_absolute_error: 0.469880, mean_q: 0.699963\n",
      " 23298/50000: episode: 467, duration: 0.133s, episode steps: 33, steps per second: 248, episode reward: 0.930, mean reward: 0.028 [-0.003, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.067 [-0.110, 0.338], loss: 0.000659, mean_absolute_error: 0.470807, mean_q: 0.700713\n",
      " 23367/50000: episode: 468, duration: 0.262s, episode steps: 69, steps per second: 264, episode reward: 0.667, mean reward: 0.010 [-0.009, 1.000], mean action: 0.928 [0.000, 2.000], mean observation: 0.182 [-0.200, 0.920], loss: 0.000071, mean_absolute_error: 0.462473, mean_q: 0.689964\n",
      " 23426/50000: episode: 469, duration: 0.235s, episode steps: 59, steps per second: 252, episode reward: 0.669, mean reward: 0.011 [-0.010, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.207 [-0.965, 0.230], loss: 0.000038, mean_absolute_error: 0.467796, mean_q: 0.698610\n",
      " 23489/50000: episode: 470, duration: 0.240s, episode steps: 63, steps per second: 262, episode reward: 0.778, mean reward: 0.012 [-0.007, 1.000], mean action: 0.921 [0.000, 2.000], mean observation: 0.130 [-0.180, 0.678], loss: 0.000049, mean_absolute_error: 0.467249, mean_q: 0.696609\n",
      " 23554/50000: episode: 471, duration: 0.264s, episode steps: 65, steps per second: 246, episode reward: 0.656, mean reward: 0.010 [-0.009, 1.000], mean action: 1.138 [0.000, 2.000], mean observation: -0.198 [-0.928, 0.190], loss: 0.000277, mean_absolute_error: 0.462778, mean_q: 0.688163\n",
      " 23577/50000: episode: 472, duration: 0.101s, episode steps: 23, steps per second: 227, episode reward: 0.961, mean reward: 0.042 [-0.002, 1.000], mean action: 1.261 [0.000, 2.000], mean observation: -0.056 [-0.237, 0.100], loss: 0.000418, mean_absolute_error: 0.472783, mean_q: 0.705195\n",
      " 23606/50000: episode: 473, duration: 0.118s, episode steps: 29, steps per second: 246, episode reward: 0.935, mean reward: 0.032 [-0.003, 1.000], mean action: 1.276 [0.000, 2.000], mean observation: -0.072 [-0.340, 0.130], loss: 0.000337, mean_absolute_error: 0.466365, mean_q: 0.696354\n",
      " 23624/50000: episode: 474, duration: 0.074s, episode steps: 18, steps per second: 243, episode reward: 0.978, mean reward: 0.054 [-0.001, 1.000], mean action: 0.778 [0.000, 2.000], mean observation: 0.049 [-0.050, 0.144], loss: 0.000052, mean_absolute_error: 0.465372, mean_q: 0.690926\n",
      " 23668/50000: episode: 475, duration: 0.193s, episode steps: 44, steps per second: 228, episode reward: 0.872, mean reward: 0.020 [-0.005, 1.000], mean action: 0.886 [0.000, 2.000], mean observation: 0.100 [-0.180, 0.511], loss: 0.000036, mean_absolute_error: 0.464164, mean_q: 0.693304\n",
      " 23693/50000: episode: 476, duration: 0.107s, episode steps: 25, steps per second: 234, episode reward: 0.957, mean reward: 0.038 [-0.002, 1.000], mean action: 0.760 [0.000, 2.000], mean observation: 0.058 [-0.090, 0.243], loss: 0.000039, mean_absolute_error: 0.467015, mean_q: 0.692086\n",
      " 23694/50000: episode: 477, duration: 0.008s, episode steps: 1, steps per second: 121, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.041 [-0.092, 0.010], loss: 0.000026, mean_absolute_error: 0.461178, mean_q: 0.690541\n",
      " 23760/50000: episode: 478, duration: 0.256s, episode steps: 66, steps per second: 258, episode reward: 0.691, mean reward: 0.010 [-0.009, 1.000], mean action: 0.939 [0.000, 2.000], mean observation: 0.176 [-0.190, 0.866], loss: 0.000045, mean_absolute_error: 0.464110, mean_q: 0.689063\n",
      " 23816/50000: episode: 479, duration: 0.289s, episode steps: 56, steps per second: 194, episode reward: 0.731, mean reward: 0.013 [-0.008, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.169 [-0.850, 0.210], loss: 0.000039, mean_absolute_error: 0.463401, mean_q: 0.689875\n",
      " 23857/50000: episode: 480, duration: 0.238s, episode steps: 41, steps per second: 172, episode reward: 0.867, mean reward: 0.021 [-0.005, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.105 [-0.160, 0.540], loss: 0.000051, mean_absolute_error: 0.461634, mean_q: 0.683567\n",
      " 23912/50000: episode: 481, duration: 0.271s, episode steps: 55, steps per second: 203, episode reward: 0.735, mean reward: 0.013 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.175 [-0.833, 0.220], loss: 0.000034, mean_absolute_error: 0.458728, mean_q: 0.681476\n",
      " 23957/50000: episode: 482, duration: 0.215s, episode steps: 45, steps per second: 210, episode reward: 0.817, mean reward: 0.018 [-0.007, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.141 [-0.666, 0.210], loss: 0.000048, mean_absolute_error: 0.466938, mean_q: 0.695181\n",
      " 23958/50000: episode: 483, duration: 0.007s, episode steps: 1, steps per second: 144, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.026 [0.000, 0.052], loss: 0.000025, mean_absolute_error: 0.496221, mean_q: 0.751172\n",
      " 24010/50000: episode: 484, duration: 0.233s, episode steps: 52, steps per second: 223, episode reward: 0.748, mean reward: 0.014 [-0.008, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.172 [-0.833, 0.230], loss: 0.000047, mean_absolute_error: 0.468427, mean_q: 0.697589\n",
      " 24029/50000: episode: 485, duration: 0.072s, episode steps: 19, steps per second: 263, episode reward: 0.968, mean reward: 0.051 [-0.002, 1.000], mean action: 1.474 [0.000, 2.000], mean observation: -0.053 [-0.223, 0.100], loss: 0.000036, mean_absolute_error: 0.457210, mean_q: 0.678148\n",
      " 24054/50000: episode: 486, duration: 0.114s, episode steps: 25, steps per second: 219, episode reward: 0.946, mean reward: 0.038 [-0.003, 1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.067 [-0.304, 0.130], loss: 0.000049, mean_absolute_error: 0.472220, mean_q: 0.699257\n",
      " 24085/50000: episode: 487, duration: 0.127s, episode steps: 31, steps per second: 245, episode reward: 0.921, mean reward: 0.030 [-0.004, 1.000], mean action: 1.258 [0.000, 2.000], mean observation: -0.082 [-0.391, 0.140], loss: 0.000045, mean_absolute_error: 0.469799, mean_q: 0.700858\n",
      " 24121/50000: episode: 488, duration: 0.150s, episode steps: 36, steps per second: 239, episode reward: 0.938, mean reward: 0.026 [-0.002, 1.000], mean action: 0.861 [0.000, 2.000], mean observation: 0.069 [-0.060, 0.231], loss: 0.000053, mean_absolute_error: 0.467423, mean_q: 0.693460\n",
      " 24141/50000: episode: 489, duration: 0.077s, episode steps: 20, steps per second: 261, episode reward: 0.972, mean reward: 0.049 [-0.002, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.050 [-0.060, 0.181], loss: 0.000043, mean_absolute_error: 0.469121, mean_q: 0.697129\n",
      " 24192/50000: episode: 490, duration: 0.224s, episode steps: 51, steps per second: 227, episode reward: 0.772, mean reward: 0.015 [-0.008, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.158 [-0.761, 0.220], loss: 0.000065, mean_absolute_error: 0.470276, mean_q: 0.698923\n",
      " 24249/50000: episode: 491, duration: 0.220s, episode steps: 57, steps per second: 260, episode reward: 0.763, mean reward: 0.013 [-0.008, 1.000], mean action: 0.895 [0.000, 2.000], mean observation: 0.150 [-0.200, 0.771], loss: 0.000038, mean_absolute_error: 0.468446, mean_q: 0.696913\n",
      " 24303/50000: episode: 492, duration: 0.218s, episode steps: 54, steps per second: 247, episode reward: 0.873, mean reward: 0.016 [-0.004, 1.000], mean action: 0.926 [0.000, 2.000], mean observation: 0.087 [-0.150, 0.437], loss: 0.000110, mean_absolute_error: 0.465633, mean_q: 0.691628\n",
      " 24351/50000: episode: 493, duration: 0.190s, episode steps: 48, steps per second: 252, episode reward: 0.845, mean reward: 0.018 [-0.006, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.115 [-0.160, 0.557], loss: 0.000043, mean_absolute_error: 0.462270, mean_q: 0.682661\n",
      " 24402/50000: episode: 494, duration: 0.212s, episode steps: 51, steps per second: 240, episode reward: 0.769, mean reward: 0.015 [-0.008, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.160 [-0.771, 0.220], loss: 0.000041, mean_absolute_error: 0.468759, mean_q: 0.692842\n",
      " 24403/50000: episode: 495, duration: 0.007s, episode steps: 1, steps per second: 138, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.033 [-0.075, 0.010], loss: 0.000069, mean_absolute_error: 0.452727, mean_q: 0.666050\n",
      " 24463/50000: episode: 496, duration: 0.249s, episode steps: 60, steps per second: 241, episode reward: 0.655, mean reward: 0.011 [-0.010, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.211 [-0.220, 0.993], loss: 0.000034, mean_absolute_error: 0.463303, mean_q: 0.687852\n",
      " 24464/50000: episode: 497, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.001 [-0.010, 0.008], loss: 0.000014, mean_absolute_error: 0.473824, mean_q: 0.688951\n",
      " 24529/50000: episode: 498, duration: 0.259s, episode steps: 65, steps per second: 251, episode reward: 0.776, mean reward: 0.012 [-0.006, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.132 [-0.150, 0.612], loss: 0.000402, mean_absolute_error: 0.468772, mean_q: 0.693548\n",
      " 24550/50000: episode: 499, duration: 0.090s, episode steps: 21, steps per second: 235, episode reward: 0.965, mean reward: 0.046 [-0.002, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.055 [-0.110, 0.220], loss: 0.000083, mean_absolute_error: 0.463115, mean_q: 0.684212\n",
      " 24605/50000: episode: 500, duration: 0.214s, episode steps: 55, steps per second: 257, episode reward: 0.732, mean reward: 0.013 [-0.008, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.179 [-0.810, 0.170], loss: 0.000071, mean_absolute_error: 0.462304, mean_q: 0.683716\n",
      " 24644/50000: episode: 501, duration: 0.156s, episode steps: 39, steps per second: 249, episode reward: 0.869, mean reward: 0.022 [-0.005, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.110 [-0.549, 0.180], loss: 0.000048, mean_absolute_error: 0.456957, mean_q: 0.673625\n",
      " 24689/50000: episode: 502, duration: 0.192s, episode steps: 45, steps per second: 235, episode reward: 0.899, mean reward: 0.020 [-0.004, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.084 [-0.090, 0.359], loss: 0.000336, mean_absolute_error: 0.460101, mean_q: 0.680064\n",
      " 24749/50000: episode: 503, duration: 0.246s, episode steps: 60, steps per second: 244, episode reward: 0.715, mean reward: 0.012 [-0.009, 1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.173 [-0.879, 0.240], loss: 0.000067, mean_absolute_error: 0.462222, mean_q: 0.684239\n",
      " 24750/50000: episode: 504, duration: 0.007s, episode steps: 1, steps per second: 147, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.019 [-0.049, 0.010], loss: 0.000026, mean_absolute_error: 0.424407, mean_q: 0.616170\n",
      " 24777/50000: episode: 505, duration: 0.113s, episode steps: 27, steps per second: 239, episode reward: 0.947, mean reward: 0.035 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.062 [-0.279, 0.100], loss: 0.000098, mean_absolute_error: 0.467596, mean_q: 0.691063\n",
      " 24819/50000: episode: 506, duration: 0.164s, episode steps: 42, steps per second: 256, episode reward: 0.845, mean reward: 0.020 [-0.006, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.116 [-0.220, 0.626], loss: 0.000145, mean_absolute_error: 0.457376, mean_q: 0.672415\n",
      " 24880/50000: episode: 507, duration: 0.245s, episode steps: 61, steps per second: 249, episode reward: 0.738, mean reward: 0.012 [-0.008, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.159 [-0.190, 0.787], loss: 0.000051, mean_absolute_error: 0.462362, mean_q: 0.685213\n",
      " 24918/50000: episode: 508, duration: 0.163s, episode steps: 38, steps per second: 233, episode reward: 0.903, mean reward: 0.024 [-0.004, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.087 [-0.140, 0.412], loss: 0.000043, mean_absolute_error: 0.466322, mean_q: 0.690198\n",
      " 24919/50000: episode: 509, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.029 [-0.010, 0.067], loss: 0.000014, mean_absolute_error: 0.459987, mean_q: 0.696163\n",
      " 24963/50000: episode: 510, duration: 0.169s, episode steps: 44, steps per second: 261, episode reward: 0.843, mean reward: 0.019 [-0.006, 1.000], mean action: 1.159 [0.000, 2.000], mean observation: -0.123 [-0.598, 0.200], loss: 0.000378, mean_absolute_error: 0.457728, mean_q: 0.675994\n",
      " 25017/50000: episode: 511, duration: 0.230s, episode steps: 54, steps per second: 234, episode reward: 0.781, mean reward: 0.014 [-0.007, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.145 [-0.180, 0.725], loss: 0.000119, mean_absolute_error: 0.464569, mean_q: 0.685524\n",
      " 25075/50000: episode: 512, duration: 0.228s, episode steps: 58, steps per second: 254, episode reward: 0.802, mean reward: 0.014 [-0.006, 1.000], mean action: 0.879 [0.000, 2.000], mean observation: 0.125 [-0.160, 0.628], loss: 0.000041, mean_absolute_error: 0.460564, mean_q: 0.681906\n",
      " 25119/50000: episode: 513, duration: 0.151s, episode steps: 44, steps per second: 291, episode reward: 0.835, mean reward: 0.019 [-0.006, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.129 [-0.620, 0.200], loss: 0.000033, mean_absolute_error: 0.461000, mean_q: 0.683129\n",
      " 25178/50000: episode: 514, duration: 0.216s, episode steps: 59, steps per second: 274, episode reward: 0.699, mean reward: 0.012 [-0.008, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.193 [-0.837, 0.230], loss: 0.000029, mean_absolute_error: 0.461199, mean_q: 0.681734\n",
      " 25235/50000: episode: 515, duration: 0.172s, episode steps: 57, steps per second: 332, episode reward: 0.740, mean reward: 0.013 [-0.008, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.167 [-0.180, 0.808], loss: 0.000042, mean_absolute_error: 0.466037, mean_q: 0.689783\n",
      " 25301/50000: episode: 516, duration: 0.224s, episode steps: 66, steps per second: 295, episode reward: 0.671, mean reward: 0.010 [-0.009, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.179 [-0.220, 0.942], loss: 0.000053, mean_absolute_error: 0.453605, mean_q: 0.664385\n",
      " 25350/50000: episode: 517, duration: 0.162s, episode steps: 49, steps per second: 303, episode reward: 0.865, mean reward: 0.018 [-0.005, 1.000], mean action: 0.878 [0.000, 2.000], mean observation: 0.100 [-0.130, 0.478], loss: 0.000037, mean_absolute_error: 0.460370, mean_q: 0.678888\n",
      " 25407/50000: episode: 518, duration: 0.200s, episode steps: 57, steps per second: 284, episode reward: 0.740, mean reward: 0.013 [-0.008, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.159 [-0.200, 0.826], loss: 0.000037, mean_absolute_error: 0.455134, mean_q: 0.670653\n",
      " 25460/50000: episode: 519, duration: 0.161s, episode steps: 53, steps per second: 330, episode reward: 0.760, mean reward: 0.014 [-0.008, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.162 [-0.778, 0.220], loss: 0.000061, mean_absolute_error: 0.460633, mean_q: 0.679527\n",
      " 25495/50000: episode: 520, duration: 0.132s, episode steps: 35, steps per second: 265, episode reward: 0.912, mean reward: 0.026 [-0.004, 1.000], mean action: 0.771 [0.000, 2.000], mean observation: 0.083 [-0.150, 0.396], loss: 0.000028, mean_absolute_error: 0.467127, mean_q: 0.690022\n",
      " 25541/50000: episode: 521, duration: 0.165s, episode steps: 46, steps per second: 279, episode reward: 0.805, mean reward: 0.018 [-0.007, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.146 [-0.200, 0.697], loss: 0.000034, mean_absolute_error: 0.463043, mean_q: 0.685356\n",
      " 25579/50000: episode: 522, duration: 0.137s, episode steps: 38, steps per second: 278, episode reward: 0.884, mean reward: 0.023 [-0.005, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.101 [-0.492, 0.160], loss: 0.000133, mean_absolute_error: 0.463118, mean_q: 0.684903\n",
      " 25632/50000: episode: 523, duration: 0.171s, episode steps: 53, steps per second: 310, episode reward: 0.804, mean reward: 0.015 [-0.007, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.132 [-0.200, 0.665], loss: 0.000274, mean_absolute_error: 0.462512, mean_q: 0.682498\n",
      " 25668/50000: episode: 524, duration: 0.124s, episode steps: 36, steps per second: 291, episode reward: 0.911, mean reward: 0.025 [-0.004, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.082 [-0.140, 0.390], loss: 0.000032, mean_absolute_error: 0.466381, mean_q: 0.690361\n",
      " 25693/50000: episode: 525, duration: 0.082s, episode steps: 25, steps per second: 306, episode reward: 0.952, mean reward: 0.038 [-0.003, 1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.054 [-0.289, 0.130], loss: 0.000441, mean_absolute_error: 0.466416, mean_q: 0.688130\n",
      " 25752/50000: episode: 526, duration: 0.192s, episode steps: 59, steps per second: 308, episode reward: 0.701, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.185 [-0.904, 0.210], loss: 0.000038, mean_absolute_error: 0.456932, mean_q: 0.672647\n",
      " 25797/50000: episode: 527, duration: 0.155s, episode steps: 45, steps per second: 290, episode reward: 0.844, mean reward: 0.019 [-0.006, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.119 [-0.596, 0.170], loss: 0.000026, mean_absolute_error: 0.462045, mean_q: 0.681134\n",
      " 25860/50000: episode: 528, duration: 0.215s, episode steps: 63, steps per second: 294, episode reward: 0.666, mean reward: 0.011 [-0.010, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.194 [-0.200, 0.957], loss: 0.000039, mean_absolute_error: 0.454682, mean_q: 0.672007\n",
      " 25871/50000: episode: 529, duration: 0.034s, episode steps: 11, steps per second: 326, episode reward: 0.987, mean reward: 0.090 [-0.002, 1.000], mean action: 1.727 [1.000, 2.000], mean observation: -0.037 [-0.153, 0.080], loss: 0.000025, mean_absolute_error: 0.467390, mean_q: 0.692128\n",
      " 25907/50000: episode: 530, duration: 0.114s, episode steps: 36, steps per second: 315, episode reward: 0.910, mean reward: 0.025 [-0.004, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.081 [-0.160, 0.406], loss: 0.000031, mean_absolute_error: 0.464271, mean_q: 0.685059\n",
      " 25930/50000: episode: 531, duration: 0.076s, episode steps: 23, steps per second: 302, episode reward: 0.964, mean reward: 0.042 [-0.002, 1.000], mean action: 1.304 [0.000, 2.000], mean observation: -0.055 [-0.206, 0.070], loss: 0.000026, mean_absolute_error: 0.461972, mean_q: 0.683363\n",
      " 25966/50000: episode: 532, duration: 0.126s, episode steps: 36, steps per second: 286, episode reward: 0.894, mean reward: 0.025 [-0.005, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.091 [-0.180, 0.480], loss: 0.000021, mean_absolute_error: 0.459505, mean_q: 0.680772\n",
      " 25991/50000: episode: 533, duration: 0.077s, episode steps: 25, steps per second: 324, episode reward: 0.954, mean reward: 0.038 [-0.003, 1.000], mean action: 1.240 [0.000, 2.000], mean observation: -0.061 [-0.264, 0.100], loss: 0.000026, mean_absolute_error: 0.467861, mean_q: 0.693402\n",
      " 25992/50000: episode: 534, duration: 0.008s, episode steps: 1, steps per second: 129, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.045 [-0.100, 0.010], loss: 0.000007, mean_absolute_error: 0.452461, mean_q: 0.674463\n",
      " 26029/50000: episode: 535, duration: 0.128s, episode steps: 37, steps per second: 289, episode reward: 0.887, mean reward: 0.024 [-0.005, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.099 [-0.160, 0.477], loss: 0.000063, mean_absolute_error: 0.453643, mean_q: 0.667636\n",
      " 26090/50000: episode: 536, duration: 0.251s, episode steps: 61, steps per second: 243, episode reward: 0.701, mean reward: 0.011 [-0.009, 1.000], mean action: 1.131 [0.000, 2.000], mean observation: -0.182 [-0.870, 0.200], loss: 0.000023, mean_absolute_error: 0.458467, mean_q: 0.680279\n",
      " 26141/50000: episode: 537, duration: 0.213s, episode steps: 51, steps per second: 240, episode reward: 0.790, mean reward: 0.015 [-0.007, 1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.151 [-0.665, 0.190], loss: 0.000034, mean_absolute_error: 0.457147, mean_q: 0.674961\n",
      " 26203/50000: episode: 538, duration: 0.292s, episode steps: 62, steps per second: 213, episode reward: 0.659, mean reward: 0.011 [-0.010, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.204 [-0.992, 0.220], loss: 0.000044, mean_absolute_error: 0.463488, mean_q: 0.688615\n",
      " 26244/50000: episode: 539, duration: 0.130s, episode steps: 41, steps per second: 315, episode reward: 0.888, mean reward: 0.022 [-0.004, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.099 [-0.140, 0.410], loss: 0.000163, mean_absolute_error: 0.458871, mean_q: 0.675605\n",
      " 26273/50000: episode: 540, duration: 0.093s, episode steps: 29, steps per second: 310, episode reward: 0.938, mean reward: 0.032 [-0.003, 1.000], mean action: 0.690 [0.000, 2.000], mean observation: 0.061 [-0.150, 0.327], loss: 0.000031, mean_absolute_error: 0.461747, mean_q: 0.683189\n",
      " 26302/50000: episode: 541, duration: 0.137s, episode steps: 29, steps per second: 211, episode reward: 0.934, mean reward: 0.032 [-0.003, 1.000], mean action: 0.690 [0.000, 2.000], mean observation: 0.072 [-0.120, 0.339], loss: 0.000021, mean_absolute_error: 0.463095, mean_q: 0.684377\n",
      " 26315/50000: episode: 542, duration: 0.041s, episode steps: 13, steps per second: 315, episode reward: 0.984, mean reward: 0.076 [-0.002, 1.000], mean action: 0.462 [0.000, 2.000], mean observation: 0.042 [-0.070, 0.152], loss: 0.000028, mean_absolute_error: 0.465172, mean_q: 0.691547\n",
      " 26343/50000: episode: 543, duration: 0.105s, episode steps: 28, steps per second: 267, episode reward: 0.936, mean reward: 0.033 [-0.004, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.065 [-0.160, 0.350], loss: 0.000031, mean_absolute_error: 0.459833, mean_q: 0.681250\n",
      " 26365/50000: episode: 544, duration: 0.091s, episode steps: 22, steps per second: 243, episode reward: 0.965, mean reward: 0.044 [-0.002, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.045 [-0.120, 0.224], loss: 0.000022, mean_absolute_error: 0.456067, mean_q: 0.675403\n",
      " 26366/50000: episode: 545, duration: 0.008s, episode steps: 1, steps per second: 131, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.017 [-0.045, 0.010], loss: 0.000006, mean_absolute_error: 0.491395, mean_q: 0.727300\n",
      " 26421/50000: episode: 546, duration: 0.229s, episode steps: 55, steps per second: 240, episode reward: 0.764, mean reward: 0.014 [-0.007, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.156 [-0.747, 0.200], loss: 0.000039, mean_absolute_error: 0.466172, mean_q: 0.686975\n",
      " 26477/50000: episode: 547, duration: 0.230s, episode steps: 56, steps per second: 244, episode reward: 0.702, mean reward: 0.013 [-0.009, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.189 [-0.240, 0.936], loss: 0.000029, mean_absolute_error: 0.460167, mean_q: 0.680774\n",
      " 26508/50000: episode: 548, duration: 0.144s, episode steps: 31, steps per second: 215, episode reward: 0.923, mean reward: 0.030 [-0.004, 1.000], mean action: 0.710 [0.000, 2.000], mean observation: 0.075 [-0.150, 0.386], loss: 0.000022, mean_absolute_error: 0.454493, mean_q: 0.669774\n",
      " 26568/50000: episode: 549, duration: 0.243s, episode steps: 60, steps per second: 247, episode reward: 0.682, mean reward: 0.011 [-0.010, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.189 [-0.240, 0.954], loss: 0.000023, mean_absolute_error: 0.467259, mean_q: 0.689572\n",
      " 26569/50000: episode: 550, duration: 0.008s, episode steps: 1, steps per second: 125, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.005 [-0.010, -0.000], loss: 0.000012, mean_absolute_error: 0.463561, mean_q: 0.691260\n",
      " 26579/50000: episode: 551, duration: 0.031s, episode steps: 10, steps per second: 321, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 1.600 [0.000, 2.000], mean observation: -0.037 [-0.141, 0.070], loss: 0.000683, mean_absolute_error: 0.455673, mean_q: 0.670983\n",
      " 26620/50000: episode: 552, duration: 0.126s, episode steps: 41, steps per second: 324, episode reward: 0.847, mean reward: 0.021 [-0.006, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.121 [-0.210, 0.618], loss: 0.000033, mean_absolute_error: 0.462686, mean_q: 0.682766\n",
      " 26671/50000: episode: 553, duration: 0.191s, episode steps: 51, steps per second: 267, episode reward: 0.775, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.153 [-0.220, 0.756], loss: 0.000022, mean_absolute_error: 0.453304, mean_q: 0.669120\n",
      " 26731/50000: episode: 554, duration: 0.175s, episode steps: 60, steps per second: 342, episode reward: 0.688, mean reward: 0.011 [-0.009, 1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.189 [-0.946, 0.220], loss: 0.000024, mean_absolute_error: 0.464267, mean_q: 0.687984\n",
      " 26764/50000: episode: 555, duration: 0.117s, episode steps: 33, steps per second: 282, episode reward: 0.903, mean reward: 0.027 [-0.005, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.094 [-0.180, 0.458], loss: 0.000042, mean_absolute_error: 0.459918, mean_q: 0.679317\n",
      " 26804/50000: episode: 556, duration: 0.129s, episode steps: 40, steps per second: 310, episode reward: 0.850, mean reward: 0.021 [-0.006, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.120 [-0.210, 0.618], loss: 0.000055, mean_absolute_error: 0.447266, mean_q: 0.662068\n",
      " 26827/50000: episode: 557, duration: 0.074s, episode steps: 23, steps per second: 311, episode reward: 0.961, mean reward: 0.042 [-0.002, 1.000], mean action: 1.304 [0.000, 2.000], mean observation: -0.057 [-0.236, 0.080], loss: 0.000028, mean_absolute_error: 0.452078, mean_q: 0.664071\n",
      " 26864/50000: episode: 558, duration: 0.150s, episode steps: 37, steps per second: 246, episode reward: 0.867, mean reward: 0.023 [-0.006, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.118 [-0.210, 0.562], loss: 0.000023, mean_absolute_error: 0.462908, mean_q: 0.688866\n",
      " 26927/50000: episode: 559, duration: 0.220s, episode steps: 63, steps per second: 286, episode reward: 0.662, mean reward: 0.011 [-0.010, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.199 [-0.976, 0.240], loss: 0.000046, mean_absolute_error: 0.458863, mean_q: 0.677486\n",
      " 26965/50000: episode: 560, duration: 0.125s, episode steps: 38, steps per second: 304, episode reward: 0.886, mean reward: 0.023 [-0.005, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.100 [-0.484, 0.150], loss: 0.000031, mean_absolute_error: 0.464767, mean_q: 0.687661\n",
      " 27012/50000: episode: 561, duration: 0.158s, episode steps: 47, steps per second: 298, episode reward: 0.799, mean reward: 0.017 [-0.007, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.145 [-0.220, 0.722], loss: 0.000022, mean_absolute_error: 0.463260, mean_q: 0.686466\n",
      " 27013/50000: episode: 562, duration: 0.007s, episode steps: 1, steps per second: 143, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.010, 0.082], loss: 0.000004, mean_absolute_error: 0.446210, mean_q: 0.667388\n",
      " 27054/50000: episode: 563, duration: 0.129s, episode steps: 41, steps per second: 318, episode reward: 0.862, mean reward: 0.021 [-0.006, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.107 [-0.180, 0.558], loss: 0.000279, mean_absolute_error: 0.458978, mean_q: 0.680371\n",
      " 27114/50000: episode: 564, duration: 0.207s, episode steps: 60, steps per second: 290, episode reward: 0.696, mean reward: 0.012 [-0.009, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.186 [-0.905, 0.220], loss: 0.000122, mean_absolute_error: 0.473028, mean_q: 0.701624\n",
      " 27115/50000: episode: 565, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [-0.010, 0.034], loss: 0.000074, mean_absolute_error: 0.416239, mean_q: 0.578430\n",
      " 27138/50000: episode: 566, duration: 0.075s, episode steps: 23, steps per second: 307, episode reward: 0.971, mean reward: 0.042 [-0.002, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.025 [-0.120, 0.195], loss: 0.000083, mean_absolute_error: 0.473474, mean_q: 0.704985\n",
      " 27166/50000: episode: 567, duration: 0.088s, episode steps: 28, steps per second: 317, episode reward: 0.940, mean reward: 0.034 [-0.003, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.072 [-0.303, 0.110], loss: 0.000034, mean_absolute_error: 0.462156, mean_q: 0.685623\n",
      " 27209/50000: episode: 568, duration: 0.173s, episode steps: 43, steps per second: 248, episode reward: 0.828, mean reward: 0.019 [-0.006, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.137 [-0.643, 0.180], loss: 0.000343, mean_absolute_error: 0.457129, mean_q: 0.674490\n",
      " 27255/50000: episode: 569, duration: 0.145s, episode steps: 46, steps per second: 318, episode reward: 0.833, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.128 [-0.600, 0.180], loss: 0.000046, mean_absolute_error: 0.466349, mean_q: 0.691399\n",
      " 27260/50000: episode: 570, duration: 0.018s, episode steps: 5, steps per second: 282, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [-0.050, 0.109], loss: 0.000029, mean_absolute_error: 0.449202, mean_q: 0.658582\n",
      " 27283/50000: episode: 571, duration: 0.085s, episode steps: 23, steps per second: 269, episode reward: 0.962, mean reward: 0.042 [-0.002, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.044 [-0.120, 0.240], loss: 0.000033, mean_absolute_error: 0.475714, mean_q: 0.706537\n",
      " 27335/50000: episode: 572, duration: 0.176s, episode steps: 52, steps per second: 296, episode reward: 0.776, mean reward: 0.015 [-0.008, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.145 [-0.230, 0.759], loss: 0.000028, mean_absolute_error: 0.469055, mean_q: 0.695969\n",
      " 27338/50000: episode: 573, duration: 0.012s, episode steps: 3, steps per second: 245, episode reward: 0.998, mean reward: 0.333 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.040 [-0.030, 0.103], loss: 0.000033, mean_absolute_error: 0.480339, mean_q: 0.724500\n",
      " 27374/50000: episode: 574, duration: 0.136s, episode steps: 36, steps per second: 265, episode reward: 0.931, mean reward: 0.026 [-0.004, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.040 [-0.180, 0.355], loss: 0.000204, mean_absolute_error: 0.463920, mean_q: 0.691404\n",
      " 27412/50000: episode: 575, duration: 0.129s, episode steps: 38, steps per second: 294, episode reward: 0.865, mean reward: 0.023 [-0.006, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.114 [-0.190, 0.573], loss: 0.000036, mean_absolute_error: 0.456902, mean_q: 0.675733\n",
      " 27465/50000: episode: 576, duration: 0.182s, episode steps: 53, steps per second: 291, episode reward: 0.715, mean reward: 0.013 [-0.009, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.192 [-0.240, 0.916], loss: 0.000024, mean_absolute_error: 0.461577, mean_q: 0.684634\n",
      " 27514/50000: episode: 577, duration: 0.149s, episode steps: 49, steps per second: 329, episode reward: 0.780, mean reward: 0.016 [-0.007, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.157 [-0.220, 0.741], loss: 0.000025, mean_absolute_error: 0.469535, mean_q: 0.699555\n",
      " 27544/50000: episode: 578, duration: 0.111s, episode steps: 30, steps per second: 270, episode reward: 0.936, mean reward: 0.031 [-0.003, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.070 [-0.140, 0.303], loss: 0.000029, mean_absolute_error: 0.468899, mean_q: 0.697032\n",
      " 27584/50000: episode: 579, duration: 0.134s, episode steps: 40, steps per second: 298, episode reward: 0.866, mean reward: 0.022 [-0.006, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.102 [-0.200, 0.568], loss: 0.000022, mean_absolute_error: 0.460508, mean_q: 0.682122\n",
      " 27639/50000: episode: 580, duration: 0.183s, episode steps: 55, steps per second: 300, episode reward: 0.772, mean reward: 0.014 [-0.008, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.148 [-0.751, 0.210], loss: 0.000024, mean_absolute_error: 0.465942, mean_q: 0.692700\n",
      " 27681/50000: episode: 581, duration: 0.166s, episode steps: 42, steps per second: 254, episode reward: 0.847, mean reward: 0.020 [-0.006, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.122 [-0.609, 0.190], loss: 0.000032, mean_absolute_error: 0.471476, mean_q: 0.701386\n",
      " 27707/50000: episode: 582, duration: 0.114s, episode steps: 26, steps per second: 229, episode reward: 0.950, mean reward: 0.037 [-0.003, 1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.062 [-0.278, 0.110], loss: 0.000031, mean_absolute_error: 0.466619, mean_q: 0.697251\n",
      " 27738/50000: episode: 583, duration: 0.119s, episode steps: 31, steps per second: 262, episode reward: 0.911, mean reward: 0.029 [-0.004, 1.000], mean action: 0.710 [0.000, 2.000], mean observation: 0.087 [-0.190, 0.444], loss: 0.000022, mean_absolute_error: 0.465426, mean_q: 0.689957\n",
      " 27768/50000: episode: 584, duration: 0.093s, episode steps: 30, steps per second: 324, episode reward: 0.923, mean reward: 0.031 [-0.004, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.085 [-0.170, 0.363], loss: 0.000025, mean_absolute_error: 0.476864, mean_q: 0.710258\n",
      " 27825/50000: episode: 585, duration: 0.214s, episode steps: 57, steps per second: 266, episode reward: 0.778, mean reward: 0.014 [-0.007, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.134 [-0.727, 0.200], loss: 0.000023, mean_absolute_error: 0.468022, mean_q: 0.692714\n",
      " 27881/50000: episode: 586, duration: 0.191s, episode steps: 56, steps per second: 293, episode reward: 0.740, mean reward: 0.013 [-0.008, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.168 [-0.825, 0.210], loss: 0.000028, mean_absolute_error: 0.463848, mean_q: 0.690888\n",
      " 27930/50000: episode: 587, duration: 0.200s, episode steps: 49, steps per second: 244, episode reward: 0.767, mean reward: 0.016 [-0.008, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.166 [-0.220, 0.803], loss: 0.000022, mean_absolute_error: 0.473913, mean_q: 0.706170\n",
      " 27956/50000: episode: 588, duration: 0.096s, episode steps: 26, steps per second: 270, episode reward: 0.935, mean reward: 0.036 [-0.004, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.075 [-0.170, 0.367], loss: 0.000023, mean_absolute_error: 0.472475, mean_q: 0.705532\n",
      " 27999/50000: episode: 589, duration: 0.149s, episode steps: 43, steps per second: 288, episode reward: 0.827, mean reward: 0.019 [-0.006, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.141 [-0.210, 0.630], loss: 0.000129, mean_absolute_error: 0.460995, mean_q: 0.682121\n",
      " 28029/50000: episode: 590, duration: 0.109s, episode steps: 30, steps per second: 275, episode reward: 0.929, mean reward: 0.031 [-0.004, 1.000], mean action: 1.267 [0.000, 2.000], mean observation: -0.076 [-0.361, 0.130], loss: 0.000029, mean_absolute_error: 0.464362, mean_q: 0.690840\n",
      " 28084/50000: episode: 591, duration: 0.177s, episode steps: 55, steps per second: 311, episode reward: 0.713, mean reward: 0.013 [-0.009, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.188 [-0.230, 0.900], loss: 0.000138, mean_absolute_error: 0.463856, mean_q: 0.691657\n",
      " 28147/50000: episode: 592, duration: 0.197s, episode steps: 63, steps per second: 320, episode reward: 0.678, mean reward: 0.011 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.185 [-0.230, 0.916], loss: 0.000285, mean_absolute_error: 0.470578, mean_q: 0.702432\n",
      " 28198/50000: episode: 593, duration: 0.180s, episode steps: 51, steps per second: 284, episode reward: 0.767, mean reward: 0.015 [-0.008, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.163 [-0.760, 0.210], loss: 0.000040, mean_absolute_error: 0.474800, mean_q: 0.707997\n",
      " 28259/50000: episode: 594, duration: 0.187s, episode steps: 61, steps per second: 326, episode reward: 0.664, mean reward: 0.011 [-0.010, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.205 [-0.971, 0.220], loss: 0.000027, mean_absolute_error: 0.471276, mean_q: 0.701451\n",
      " 28306/50000: episode: 595, duration: 0.177s, episode steps: 47, steps per second: 266, episode reward: 0.806, mean reward: 0.017 [-0.007, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.142 [-0.210, 0.687], loss: 0.000027, mean_absolute_error: 0.473969, mean_q: 0.707875\n",
      " 28307/50000: episode: 596, duration: 0.006s, episode steps: 1, steps per second: 179, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.016 [-0.010, 0.042], loss: 0.000028, mean_absolute_error: 0.560288, mean_q: 0.834732\n",
      " 28365/50000: episode: 597, duration: 0.176s, episode steps: 58, steps per second: 330, episode reward: 0.681, mean reward: 0.012 [-0.010, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.200 [-0.230, 0.962], loss: 0.000032, mean_absolute_error: 0.469612, mean_q: 0.699690\n",
      " 28384/50000: episode: 598, duration: 0.105s, episode steps: 19, steps per second: 181, episode reward: 0.966, mean reward: 0.051 [-0.002, 1.000], mean action: 1.474 [0.000, 2.000], mean observation: -0.053 [-0.245, 0.110], loss: 0.000027, mean_absolute_error: 0.468770, mean_q: 0.695863\n",
      " 28426/50000: episode: 599, duration: 0.144s, episode steps: 42, steps per second: 292, episode reward: 0.874, mean reward: 0.021 [-0.005, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.104 [-0.481, 0.150], loss: 0.000021, mean_absolute_error: 0.469082, mean_q: 0.698024\n",
      " 28486/50000: episode: 600, duration: 0.207s, episode steps: 60, steps per second: 289, episode reward: 0.666, mean reward: 0.011 [-0.010, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.206 [-0.971, 0.220], loss: 0.000026, mean_absolute_error: 0.469445, mean_q: 0.699401\n",
      " 28543/50000: episode: 601, duration: 0.177s, episode steps: 57, steps per second: 321, episode reward: 0.702, mean reward: 0.012 [-0.009, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.187 [-0.220, 0.925], loss: 0.000034, mean_absolute_error: 0.477834, mean_q: 0.713714\n",
      " 28589/50000: episode: 602, duration: 0.178s, episode steps: 46, steps per second: 258, episode reward: 0.825, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.132 [-0.625, 0.200], loss: 0.000022, mean_absolute_error: 0.478196, mean_q: 0.715695\n",
      " 28600/50000: episode: 603, duration: 0.035s, episode steps: 11, steps per second: 318, episode reward: 0.987, mean reward: 0.090 [-0.002, 1.000], mean action: 1.727 [1.000, 2.000], mean observation: -0.038 [-0.155, 0.080], loss: 0.000020, mean_absolute_error: 0.470093, mean_q: 0.704285\n",
      " 28601/50000: episode: 604, duration: 0.006s, episode steps: 1, steps per second: 177, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.040 [-0.090, 0.010], loss: 0.000013, mean_absolute_error: 0.466632, mean_q: 0.694020\n",
      " 28602/50000: episode: 605, duration: 0.006s, episode steps: 1, steps per second: 164, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [-0.010, 0.037], loss: 0.000078, mean_absolute_error: 0.445619, mean_q: 0.655376\n",
      " 28620/50000: episode: 606, duration: 0.054s, episode steps: 18, steps per second: 332, episode reward: 0.969, mean reward: 0.054 [-0.002, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.048 [-0.130, 0.236], loss: 0.000021, mean_absolute_error: 0.461563, mean_q: 0.685332\n",
      " 28673/50000: episode: 607, duration: 0.187s, episode steps: 53, steps per second: 284, episode reward: 0.739, mean reward: 0.014 [-0.008, 1.000], mean action: 0.849 [0.000, 2.000], mean observation: 0.176 [-0.220, 0.850], loss: 0.000027, mean_absolute_error: 0.478239, mean_q: 0.714565\n",
      " 28674/50000: episode: 608, duration: 0.007s, episode steps: 1, steps per second: 139, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.043 [-0.010, 0.096], loss: 0.000018, mean_absolute_error: 0.467096, mean_q: 0.687592\n",
      " 28723/50000: episode: 609, duration: 0.153s, episode steps: 49, steps per second: 320, episode reward: 0.780, mean reward: 0.016 [-0.008, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.158 [-0.200, 0.761], loss: 0.000026, mean_absolute_error: 0.476310, mean_q: 0.710069\n",
      " 28759/50000: episode: 610, duration: 0.124s, episode steps: 36, steps per second: 291, episode reward: 0.903, mean reward: 0.025 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.084 [-0.441, 0.150], loss: 0.000029, mean_absolute_error: 0.475369, mean_q: 0.712379\n",
      " 28805/50000: episode: 611, duration: 0.146s, episode steps: 46, steps per second: 315, episode reward: 0.811, mean reward: 0.018 [-0.007, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.144 [-0.669, 0.200], loss: 0.000021, mean_absolute_error: 0.476319, mean_q: 0.713227\n",
      " 28844/50000: episode: 612, duration: 0.141s, episode steps: 39, steps per second: 276, episode reward: 0.863, mean reward: 0.022 [-0.006, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.117 [-0.180, 0.562], loss: 0.000018, mean_absolute_error: 0.474951, mean_q: 0.710730\n",
      " 28845/50000: episode: 613, duration: 0.007s, episode steps: 1, steps per second: 142, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.020 [-0.049, 0.010], loss: 0.000019, mean_absolute_error: 0.440234, mean_q: 0.653295\n",
      " 28846/50000: episode: 614, duration: 0.008s, episode steps: 1, steps per second: 121, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.027 [-0.065, 0.010], loss: 0.000018, mean_absolute_error: 0.461463, mean_q: 0.701330\n",
      " 28890/50000: episode: 615, duration: 0.143s, episode steps: 44, steps per second: 308, episode reward: 0.826, mean reward: 0.019 [-0.006, 1.000], mean action: 0.818 [0.000, 2.000], mean observation: 0.138 [-0.190, 0.635], loss: 0.000024, mean_absolute_error: 0.474163, mean_q: 0.708654\n",
      " 28891/50000: episode: 616, duration: 0.007s, episode steps: 1, steps per second: 142, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.028 [-0.066, 0.010], loss: 0.000014, mean_absolute_error: 0.506645, mean_q: 0.765768\n",
      " 28945/50000: episode: 617, duration: 0.189s, episode steps: 54, steps per second: 286, episode reward: 0.732, mean reward: 0.014 [-0.008, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.180 [-0.839, 0.200], loss: 0.000029, mean_absolute_error: 0.480049, mean_q: 0.717124\n",
      " 28946/50000: episode: 618, duration: 0.006s, episode steps: 1, steps per second: 163, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.021 [-0.052, 0.010], loss: 0.000022, mean_absolute_error: 0.439036, mean_q: 0.656384\n",
      " 28993/50000: episode: 619, duration: 0.145s, episode steps: 47, steps per second: 324, episode reward: 0.816, mean reward: 0.017 [-0.006, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.137 [-0.650, 0.170], loss: 0.000028, mean_absolute_error: 0.480109, mean_q: 0.718448\n",
      " 29018/50000: episode: 620, duration: 0.084s, episode steps: 25, steps per second: 296, episode reward: 0.946, mean reward: 0.038 [-0.003, 1.000], mean action: 0.680 [0.000, 2.000], mean observation: 0.068 [-0.140, 0.300], loss: 0.000027, mean_absolute_error: 0.485238, mean_q: 0.726160\n",
      " 29066/50000: episode: 621, duration: 0.157s, episode steps: 48, steps per second: 305, episode reward: 0.851, mean reward: 0.018 [-0.006, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.095 [-0.570, 0.170], loss: 0.000112, mean_absolute_error: 0.478959, mean_q: 0.712684\n",
      " 29101/50000: episode: 622, duration: 0.129s, episode steps: 35, steps per second: 272, episode reward: 0.895, mean reward: 0.026 [-0.005, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.095 [-0.478, 0.160], loss: 0.000034, mean_absolute_error: 0.480110, mean_q: 0.717561\n",
      " 29132/50000: episode: 623, duration: 0.119s, episode steps: 31, steps per second: 261, episode reward: 0.913, mean reward: 0.029 [-0.004, 1.000], mean action: 0.710 [0.000, 2.000], mean observation: 0.088 [-0.160, 0.432], loss: 0.000041, mean_absolute_error: 0.486981, mean_q: 0.725178\n",
      " 29192/50000: episode: 624, duration: 0.190s, episode steps: 60, steps per second: 315, episode reward: 0.681, mean reward: 0.011 [-0.009, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.196 [-0.200, 0.941], loss: 0.000026, mean_absolute_error: 0.482560, mean_q: 0.723285\n",
      " 29235/50000: episode: 625, duration: 0.136s, episode steps: 43, steps per second: 317, episode reward: 0.843, mean reward: 0.020 [-0.006, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.123 [-0.190, 0.611], loss: 0.000030, mean_absolute_error: 0.480740, mean_q: 0.720001\n",
      " 29236/50000: episode: 626, duration: 0.006s, episode steps: 1, steps per second: 165, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.022 [-0.054, 0.010], loss: 0.000026, mean_absolute_error: 0.441675, mean_q: 0.667393\n",
      " 29240/50000: episode: 627, duration: 0.015s, episode steps: 4, steps per second: 274, episode reward: 0.997, mean reward: 0.249 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.040 [-0.040, 0.109], loss: 0.000023, mean_absolute_error: 0.492458, mean_q: 0.739954\n",
      " 29269/50000: episode: 628, duration: 0.108s, episode steps: 29, steps per second: 269, episode reward: 0.927, mean reward: 0.032 [-0.004, 1.000], mean action: 0.724 [0.000, 2.000], mean observation: 0.079 [-0.160, 0.384], loss: 0.000025, mean_absolute_error: 0.478258, mean_q: 0.712524\n",
      " 29273/50000: episode: 629, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: 0.997, mean reward: 0.249 [-0.001, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.046 [-0.020, 0.103], loss: 0.000062, mean_absolute_error: 0.476800, mean_q: 0.716621\n",
      " 29311/50000: episode: 630, duration: 0.124s, episode steps: 38, steps per second: 308, episode reward: 0.881, mean reward: 0.023 [-0.005, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.103 [-0.170, 0.511], loss: 0.000023, mean_absolute_error: 0.478158, mean_q: 0.712682\n",
      " 29357/50000: episode: 631, duration: 0.155s, episode steps: 46, steps per second: 297, episode reward: 0.831, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.129 [-0.597, 0.160], loss: 0.000024, mean_absolute_error: 0.477205, mean_q: 0.713825\n",
      " 29393/50000: episode: 632, duration: 0.117s, episode steps: 36, steps per second: 307, episode reward: 0.890, mean reward: 0.025 [-0.005, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.100 [-0.473, 0.170], loss: 0.000019, mean_absolute_error: 0.475448, mean_q: 0.712226\n",
      " 29430/50000: episode: 633, duration: 0.129s, episode steps: 37, steps per second: 287, episode reward: 0.897, mean reward: 0.024 [-0.004, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.090 [-0.435, 0.140], loss: 0.000022, mean_absolute_error: 0.482416, mean_q: 0.725272\n",
      " 29487/50000: episode: 634, duration: 0.212s, episode steps: 57, steps per second: 268, episode reward: 0.703, mean reward: 0.012 [-0.009, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.191 [-0.200, 0.893], loss: 0.000036, mean_absolute_error: 0.481159, mean_q: 0.717870\n",
      " 29546/50000: episode: 635, duration: 0.231s, episode steps: 59, steps per second: 255, episode reward: 0.708, mean reward: 0.012 [-0.009, 1.000], mean action: 0.915 [0.000, 2.000], mean observation: 0.181 [-0.200, 0.890], loss: 0.000027, mean_absolute_error: 0.483817, mean_q: 0.725873\n",
      " 29547/50000: episode: 636, duration: 0.008s, episode steps: 1, steps per second: 124, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [-0.010, 0.099], loss: 0.000013, mean_absolute_error: 0.494250, mean_q: 0.749264\n",
      " 29587/50000: episode: 637, duration: 0.136s, episode steps: 40, steps per second: 295, episode reward: 0.870, mean reward: 0.022 [-0.005, 1.000], mean action: 0.825 [0.000, 2.000], mean observation: 0.108 [-0.170, 0.540], loss: 0.000029, mean_absolute_error: 0.491209, mean_q: 0.733535\n",
      " 29634/50000: episode: 638, duration: 0.152s, episode steps: 47, steps per second: 308, episode reward: 0.792, mean reward: 0.017 [-0.007, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.151 [-0.747, 0.210], loss: 0.000023, mean_absolute_error: 0.482333, mean_q: 0.724025\n",
      " 29689/50000: episode: 639, duration: 0.186s, episode steps: 55, steps per second: 296, episode reward: 0.724, mean reward: 0.013 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.183 [-0.850, 0.220], loss: 0.000023, mean_absolute_error: 0.485795, mean_q: 0.727111\n",
      " 29728/50000: episode: 640, duration: 0.158s, episode steps: 39, steps per second: 246, episode reward: 0.881, mean reward: 0.023 [-0.005, 1.000], mean action: 0.821 [0.000, 2.000], mean observation: 0.101 [-0.170, 0.505], loss: 0.000026, mean_absolute_error: 0.486481, mean_q: 0.728451\n",
      " 29780/50000: episode: 641, duration: 0.163s, episode steps: 52, steps per second: 320, episode reward: 0.760, mean reward: 0.015 [-0.008, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.161 [-0.803, 0.210], loss: 0.000023, mean_absolute_error: 0.487974, mean_q: 0.731756\n",
      " 29808/50000: episode: 642, duration: 0.101s, episode steps: 28, steps per second: 276, episode reward: 0.938, mean reward: 0.034 [-0.003, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.067 [-0.336, 0.120], loss: 0.000023, mean_absolute_error: 0.483959, mean_q: 0.725219\n",
      " 29842/50000: episode: 643, duration: 0.145s, episode steps: 34, steps per second: 234, episode reward: 0.959, mean reward: 0.028 [-0.002, 1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.006 [-0.233, 0.140], loss: 0.000398, mean_absolute_error: 0.483717, mean_q: 0.718715\n",
      " 29884/50000: episode: 644, duration: 0.163s, episode steps: 42, steps per second: 258, episode reward: 0.854, mean reward: 0.020 [-0.006, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.117 [-0.555, 0.160], loss: 0.000185, mean_absolute_error: 0.491588, mean_q: 0.736654\n",
      " 29939/50000: episode: 645, duration: 0.177s, episode steps: 55, steps per second: 310, episode reward: 0.754, mean reward: 0.014 [-0.008, 1.000], mean action: 0.909 [0.000, 2.000], mean observation: 0.162 [-0.190, 0.779], loss: 0.000028, mean_absolute_error: 0.477296, mean_q: 0.714780\n",
      " 29974/50000: episode: 646, duration: 0.148s, episode steps: 35, steps per second: 236, episode reward: 0.912, mean reward: 0.026 [-0.004, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.084 [-0.140, 0.395], loss: 0.000032, mean_absolute_error: 0.486254, mean_q: 0.730179\n",
      " 30036/50000: episode: 647, duration: 0.227s, episode steps: 62, steps per second: 274, episode reward: 0.668, mean reward: 0.011 [-0.010, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.196 [-0.967, 0.210], loss: 0.000039, mean_absolute_error: 0.489202, mean_q: 0.731779\n",
      " 30071/50000: episode: 648, duration: 0.130s, episode steps: 35, steps per second: 270, episode reward: 0.909, mean reward: 0.026 [-0.004, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.085 [-0.150, 0.422], loss: 0.000028, mean_absolute_error: 0.486938, mean_q: 0.729267\n",
      " 30104/50000: episode: 649, duration: 0.116s, episode steps: 33, steps per second: 285, episode reward: 0.913, mean reward: 0.028 [-0.004, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.082 [-0.411, 0.140], loss: 0.000024, mean_absolute_error: 0.487237, mean_q: 0.730073\n",
      " 30105/50000: episode: 650, duration: 0.008s, episode steps: 1, steps per second: 118, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.016 [0.000, 0.032], loss: 0.000022, mean_absolute_error: 0.470361, mean_q: 0.712114\n",
      " 30153/50000: episode: 651, duration: 0.163s, episode steps: 48, steps per second: 294, episode reward: 0.835, mean reward: 0.017 [-0.006, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.110 [-0.620, 0.190], loss: 0.000039, mean_absolute_error: 0.484340, mean_q: 0.726668\n",
      " 30197/50000: episode: 652, duration: 0.151s, episode steps: 44, steps per second: 291, episode reward: 0.855, mean reward: 0.019 [-0.006, 1.000], mean action: 0.886 [0.000, 2.000], mean observation: 0.113 [-0.180, 0.558], loss: 0.000051, mean_absolute_error: 0.483953, mean_q: 0.723736\n",
      " 30212/50000: episode: 653, duration: 0.052s, episode steps: 15, steps per second: 291, episode reward: 0.981, mean reward: 0.065 [-0.002, 1.000], mean action: 1.600 [0.000, 2.000], mean observation: -0.034 [-0.169, 0.110], loss: 0.000063, mean_absolute_error: 0.485456, mean_q: 0.724643\n",
      " 30270/50000: episode: 654, duration: 0.219s, episode steps: 58, steps per second: 265, episode reward: 0.734, mean reward: 0.013 [-0.008, 1.000], mean action: 1.121 [0.000, 2.000], mean observation: -0.170 [-0.789, 0.190], loss: 0.000212, mean_absolute_error: 0.490472, mean_q: 0.735037\n",
      " 30271/50000: episode: 655, duration: 0.007s, episode steps: 1, steps per second: 135, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.022 [-0.054, 0.010], loss: 0.000076, mean_absolute_error: 0.465613, mean_q: 0.702901\n",
      " 30333/50000: episode: 656, duration: 0.218s, episode steps: 62, steps per second: 284, episode reward: 0.697, mean reward: 0.011 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.176 [-0.874, 0.190], loss: 0.000039, mean_absolute_error: 0.488058, mean_q: 0.730190\n",
      " 30362/50000: episode: 657, duration: 0.122s, episode steps: 29, steps per second: 237, episode reward: 0.962, mean reward: 0.033 [-0.002, 1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.021 [-0.245, 0.120], loss: 0.000386, mean_absolute_error: 0.488857, mean_q: 0.728465\n",
      " 30426/50000: episode: 658, duration: 0.237s, episode steps: 64, steps per second: 270, episode reward: 0.671, mean reward: 0.010 [-0.010, 1.000], mean action: 0.906 [0.000, 2.000], mean observation: 0.191 [-0.200, 0.950], loss: 0.000054, mean_absolute_error: 0.481547, mean_q: 0.721466\n",
      " 30487/50000: episode: 659, duration: 0.192s, episode steps: 61, steps per second: 318, episode reward: 0.687, mean reward: 0.011 [-0.009, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.187 [-0.919, 0.210], loss: 0.000037, mean_absolute_error: 0.487881, mean_q: 0.729947\n",
      " 30511/50000: episode: 660, duration: 0.091s, episode steps: 24, steps per second: 264, episode reward: 0.953, mean reward: 0.040 [-0.003, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.054 [-0.295, 0.140], loss: 0.000046, mean_absolute_error: 0.480533, mean_q: 0.719747\n",
      " 30564/50000: episode: 661, duration: 0.237s, episode steps: 53, steps per second: 223, episode reward: 0.794, mean reward: 0.015 [-0.007, 1.000], mean action: 0.868 [0.000, 2.000], mean observation: 0.139 [-0.180, 0.694], loss: 0.000032, mean_absolute_error: 0.489042, mean_q: 0.733865\n",
      " 30630/50000: episode: 662, duration: 0.249s, episode steps: 66, steps per second: 265, episode reward: 0.677, mean reward: 0.010 [-0.009, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.184 [-0.884, 0.200], loss: 0.000120, mean_absolute_error: 0.491256, mean_q: 0.735978\n",
      " 30676/50000: episode: 663, duration: 0.157s, episode steps: 46, steps per second: 293, episode reward: 0.832, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.122 [-0.635, 0.190], loss: 0.000039, mean_absolute_error: 0.484846, mean_q: 0.728038\n",
      " 30682/50000: episode: 664, duration: 0.022s, episode steps: 6, steps per second: 276, episode reward: 0.995, mean reward: 0.166 [-0.001, 1.000], mean action: 0.333 [0.000, 2.000], mean observation: 0.044 [-0.040, 0.110], loss: 0.000056, mean_absolute_error: 0.489795, mean_q: 0.736365\n",
      " 30734/50000: episode: 665, duration: 0.185s, episode steps: 52, steps per second: 282, episode reward: 0.822, mean reward: 0.016 [-0.006, 1.000], mean action: 0.885 [0.000, 2.000], mean observation: 0.121 [-0.190, 0.631], loss: 0.000223, mean_absolute_error: 0.484459, mean_q: 0.725482\n",
      " 30794/50000: episode: 666, duration: 0.185s, episode steps: 60, steps per second: 325, episode reward: 0.716, mean reward: 0.012 [-0.009, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.170 [-0.873, 0.210], loss: 0.000118, mean_absolute_error: 0.492217, mean_q: 0.735552\n",
      " 30826/50000: episode: 667, duration: 0.127s, episode steps: 32, steps per second: 252, episode reward: 0.927, mean reward: 0.029 [-0.004, 1.000], mean action: 0.781 [0.000, 2.000], mean observation: 0.075 [-0.130, 0.356], loss: 0.000038, mean_absolute_error: 0.487594, mean_q: 0.730104\n",
      " 30863/50000: episode: 668, duration: 0.125s, episode steps: 37, steps per second: 296, episode reward: 0.904, mean reward: 0.024 [-0.004, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.080 [-0.425, 0.140], loss: 0.000289, mean_absolute_error: 0.481449, mean_q: 0.721111\n",
      " 30864/50000: episode: 669, duration: 0.006s, episode steps: 1, steps per second: 165, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.002 [-0.006, 0.010], loss: 0.000048, mean_absolute_error: 0.508234, mean_q: 0.770288\n",
      " 30923/50000: episode: 670, duration: 0.214s, episode steps: 59, steps per second: 275, episode reward: 0.700, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.184 [-0.902, 0.210], loss: 0.000037, mean_absolute_error: 0.487330, mean_q: 0.729575\n",
      " 30947/50000: episode: 671, duration: 0.084s, episode steps: 24, steps per second: 286, episode reward: 0.967, mean reward: 0.040 [-0.002, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.032 [-0.213, 0.120], loss: 0.000637, mean_absolute_error: 0.489626, mean_q: 0.726314\n",
      " 30998/50000: episode: 672, duration: 0.178s, episode steps: 51, steps per second: 287, episode reward: 0.779, mean reward: 0.015 [-0.008, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.150 [-0.754, 0.200], loss: 0.000146, mean_absolute_error: 0.486895, mean_q: 0.727450\n",
      " 30999/50000: episode: 673, duration: 0.008s, episode steps: 1, steps per second: 123, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.024 [0.000, 0.047], loss: 0.000277, mean_absolute_error: 0.480058, mean_q: 0.702614\n",
      " 31015/50000: episode: 674, duration: 0.062s, episode steps: 16, steps per second: 259, episode reward: 0.982, mean reward: 0.061 [-0.002, 1.000], mean action: 1.562 [0.000, 2.000], mean observation: -0.022 [-0.161, 0.100], loss: 0.000041, mean_absolute_error: 0.493279, mean_q: 0.744090\n",
      " 31076/50000: episode: 675, duration: 0.223s, episode steps: 61, steps per second: 273, episode reward: 0.668, mean reward: 0.011 [-0.010, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.201 [-0.964, 0.230], loss: 0.000236, mean_absolute_error: 0.485510, mean_q: 0.725928\n",
      " 31138/50000: episode: 676, duration: 0.231s, episode steps: 62, steps per second: 268, episode reward: 0.658, mean reward: 0.011 [-0.010, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.205 [-0.974, 0.230], loss: 0.000028, mean_absolute_error: 0.488187, mean_q: 0.731808\n",
      " 31201/50000: episode: 677, duration: 0.263s, episode steps: 63, steps per second: 240, episode reward: 0.752, mean reward: 0.012 [-0.007, 1.000], mean action: 0.968 [0.000, 2.000], mean observation: 0.148 [-0.170, 0.716], loss: 0.000031, mean_absolute_error: 0.488802, mean_q: 0.732695\n",
      " 31235/50000: episode: 678, duration: 0.135s, episode steps: 34, steps per second: 252, episode reward: 0.911, mean reward: 0.027 [-0.004, 1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.082 [-0.413, 0.160], loss: 0.000026, mean_absolute_error: 0.476674, mean_q: 0.713210\n",
      " 31282/50000: episode: 679, duration: 0.165s, episode steps: 47, steps per second: 285, episode reward: 0.860, mean reward: 0.018 [-0.005, 1.000], mean action: 0.872 [0.000, 2.000], mean observation: 0.104 [-0.170, 0.533], loss: 0.000052, mean_absolute_error: 0.489120, mean_q: 0.732116\n",
      " 31317/50000: episode: 680, duration: 0.124s, episode steps: 35, steps per second: 283, episode reward: 0.900, mean reward: 0.026 [-0.004, 1.000], mean action: 0.771 [0.000, 2.000], mean observation: 0.093 [-0.140, 0.450], loss: 0.000033, mean_absolute_error: 0.487514, mean_q: 0.729588\n",
      " 31376/50000: episode: 681, duration: 0.238s, episode steps: 59, steps per second: 248, episode reward: 0.692, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.193 [-0.899, 0.200], loss: 0.000039, mean_absolute_error: 0.483601, mean_q: 0.724085\n",
      " 31421/50000: episode: 682, duration: 0.184s, episode steps: 45, steps per second: 245, episode reward: 0.862, mean reward: 0.019 [-0.005, 1.000], mean action: 0.844 [0.000, 2.000], mean observation: 0.106 [-0.150, 0.524], loss: 0.000027, mean_absolute_error: 0.489831, mean_q: 0.733860\n",
      " 31469/50000: episode: 683, duration: 0.199s, episode steps: 48, steps per second: 241, episode reward: 0.854, mean reward: 0.018 [-0.005, 1.000], mean action: 0.896 [0.000, 2.000], mean observation: 0.108 [-0.170, 0.534], loss: 0.000039, mean_absolute_error: 0.488575, mean_q: 0.730226\n",
      " 31523/50000: episode: 684, duration: 0.157s, episode steps: 54, steps per second: 343, episode reward: 0.747, mean reward: 0.014 [-0.008, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.168 [-0.804, 0.200], loss: 0.000032, mean_absolute_error: 0.484741, mean_q: 0.725779\n",
      " 31585/50000: episode: 685, duration: 0.212s, episode steps: 62, steps per second: 293, episode reward: 0.741, mean reward: 0.012 [-0.008, 1.000], mean action: 0.968 [0.000, 2.000], mean observation: 0.154 [-0.180, 0.792], loss: 0.000029, mean_absolute_error: 0.488369, mean_q: 0.729844\n",
      " 31644/50000: episode: 686, duration: 0.187s, episode steps: 59, steps per second: 315, episode reward: 0.695, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.189 [-0.919, 0.210], loss: 0.000036, mean_absolute_error: 0.484966, mean_q: 0.726103\n",
      " 31706/50000: episode: 687, duration: 0.219s, episode steps: 62, steps per second: 283, episode reward: 0.680, mean reward: 0.011 [-0.009, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.191 [-0.935, 0.220], loss: 0.000035, mean_absolute_error: 0.478860, mean_q: 0.715925\n",
      " 31761/50000: episode: 688, duration: 0.181s, episode steps: 55, steps per second: 304, episode reward: 0.806, mean reward: 0.015 [-0.007, 1.000], mean action: 0.964 [0.000, 2.000], mean observation: 0.126 [-0.190, 0.662], loss: 0.000038, mean_absolute_error: 0.499346, mean_q: 0.748838\n",
      " 31788/50000: episode: 689, duration: 0.105s, episode steps: 27, steps per second: 258, episode reward: 0.938, mean reward: 0.035 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.073 [-0.338, 0.140], loss: 0.000028, mean_absolute_error: 0.492047, mean_q: 0.738863\n",
      " 31817/50000: episode: 690, duration: 0.102s, episode steps: 29, steps per second: 285, episode reward: 0.937, mean reward: 0.032 [-0.003, 1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.065 [-0.331, 0.120], loss: 0.000030, mean_absolute_error: 0.488202, mean_q: 0.729585\n",
      " 31851/50000: episode: 691, duration: 0.113s, episode steps: 34, steps per second: 302, episode reward: 0.915, mean reward: 0.027 [-0.004, 1.000], mean action: 0.794 [0.000, 2.000], mean observation: 0.086 [-0.120, 0.375], loss: 0.000333, mean_absolute_error: 0.488703, mean_q: 0.729636\n",
      " 31910/50000: episode: 692, duration: 0.218s, episode steps: 59, steps per second: 270, episode reward: 0.698, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.182 [-0.919, 0.210], loss: 0.000235, mean_absolute_error: 0.495853, mean_q: 0.741715\n",
      " 31954/50000: episode: 693, duration: 0.152s, episode steps: 44, steps per second: 289, episode reward: 0.898, mean reward: 0.020 [-0.004, 1.000], mean action: 0.955 [0.000, 2.000], mean observation: 0.082 [-0.130, 0.414], loss: 0.000046, mean_absolute_error: 0.487925, mean_q: 0.730402\n",
      " 31963/50000: episode: 694, duration: 0.028s, episode steps: 9, steps per second: 316, episode reward: 0.991, mean reward: 0.110 [-0.001, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.040 [-0.040, 0.123], loss: 0.000035, mean_absolute_error: 0.493450, mean_q: 0.739954\n",
      " 31983/50000: episode: 695, duration: 0.076s, episode steps: 20, steps per second: 264, episode reward: 0.966, mean reward: 0.048 [-0.002, 1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.056 [-0.224, 0.100], loss: 0.000036, mean_absolute_error: 0.493528, mean_q: 0.737654\n",
      " 32032/50000: episode: 696, duration: 0.206s, episode steps: 49, steps per second: 238, episode reward: 0.801, mean reward: 0.016 [-0.007, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.145 [-0.669, 0.200], loss: 0.000046, mean_absolute_error: 0.499535, mean_q: 0.750788\n",
      " 32083/50000: episode: 697, duration: 0.188s, episode steps: 51, steps per second: 271, episode reward: 0.779, mean reward: 0.015 [-0.007, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.154 [-0.742, 0.200], loss: 0.000035, mean_absolute_error: 0.493161, mean_q: 0.736799\n",
      " 32084/50000: episode: 698, duration: 0.006s, episode steps: 1, steps per second: 165, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.019 [-0.049, 0.010], loss: 0.000020, mean_absolute_error: 0.459963, mean_q: 0.696618\n",
      " 32087/50000: episode: 699, duration: 0.016s, episode steps: 3, steps per second: 188, episode reward: 0.998, mean reward: 0.333 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.041 [-0.104, 0.030], loss: 0.000036, mean_absolute_error: 0.481277, mean_q: 0.718770\n",
      " 32109/50000: episode: 700, duration: 0.082s, episode steps: 22, steps per second: 270, episode reward: 0.961, mean reward: 0.044 [-0.003, 1.000], mean action: 1.318 [0.000, 2.000], mean observation: -0.055 [-0.255, 0.110], loss: 0.000027, mean_absolute_error: 0.487350, mean_q: 0.730611\n",
      " 32148/50000: episode: 701, duration: 0.146s, episode steps: 39, steps per second: 268, episode reward: 0.885, mean reward: 0.023 [-0.005, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.101 [-0.458, 0.150], loss: 0.000032, mean_absolute_error: 0.484892, mean_q: 0.727019\n",
      " 32163/50000: episode: 702, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 0.978, mean reward: 0.065 [-0.002, 1.000], mean action: 1.533 [0.000, 2.000], mean observation: -0.042 [-0.194, 0.110], loss: 0.000036, mean_absolute_error: 0.486332, mean_q: 0.728037\n",
      " 32209/50000: episode: 703, duration: 0.166s, episode steps: 46, steps per second: 276, episode reward: 0.841, mean reward: 0.018 [-0.006, 1.000], mean action: 0.891 [0.000, 2.000], mean observation: 0.121 [-0.170, 0.576], loss: 0.000032, mean_absolute_error: 0.488689, mean_q: 0.733528\n",
      " 32242/50000: episode: 704, duration: 0.119s, episode steps: 33, steps per second: 276, episode reward: 0.918, mean reward: 0.028 [-0.004, 1.000], mean action: 1.242 [0.000, 2.000], mean observation: -0.083 [-0.375, 0.140], loss: 0.000038, mean_absolute_error: 0.490063, mean_q: 0.734755\n",
      " 32289/50000: episode: 705, duration: 0.157s, episode steps: 47, steps per second: 299, episode reward: 0.824, mean reward: 0.018 [-0.006, 1.000], mean action: 1.149 [0.000, 2.000], mean observation: -0.132 [-0.622, 0.190], loss: 0.000037, mean_absolute_error: 0.492209, mean_q: 0.735244\n",
      " 32341/50000: episode: 706, duration: 0.167s, episode steps: 52, steps per second: 312, episode reward: 0.793, mean reward: 0.015 [-0.007, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.144 [-0.672, 0.200], loss: 0.000058, mean_absolute_error: 0.485747, mean_q: 0.728639\n",
      " 32390/50000: episode: 707, duration: 0.176s, episode steps: 49, steps per second: 278, episode reward: 0.862, mean reward: 0.018 [-0.005, 1.000], mean action: 0.918 [0.000, 2.000], mean observation: 0.100 [-0.160, 0.498], loss: 0.000236, mean_absolute_error: 0.488371, mean_q: 0.730499\n",
      " 32427/50000: episode: 708, duration: 0.118s, episode steps: 37, steps per second: 314, episode reward: 0.915, mean reward: 0.025 [-0.004, 1.000], mean action: 0.838 [0.000, 2.000], mean observation: 0.078 [-0.110, 0.372], loss: 0.000042, mean_absolute_error: 0.496885, mean_q: 0.744025\n",
      " 32450/50000: episode: 709, duration: 0.093s, episode steps: 23, steps per second: 248, episode reward: 0.957, mean reward: 0.042 [-0.003, 1.000], mean action: 1.391 [0.000, 2.000], mean observation: -0.054 [-0.272, 0.120], loss: 0.000036, mean_absolute_error: 0.496575, mean_q: 0.743671\n",
      " 32495/50000: episode: 710, duration: 0.175s, episode steps: 45, steps per second: 257, episode reward: 0.859, mean reward: 0.019 [-0.005, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.109 [-0.150, 0.535], loss: 0.000097, mean_absolute_error: 0.482191, mean_q: 0.722936\n",
      " 32496/50000: episode: 711, duration: 0.007s, episode steps: 1, steps per second: 154, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.011 [0.000, 0.022], loss: 0.000012, mean_absolute_error: 0.484562, mean_q: 0.736206\n",
      " 32497/50000: episode: 712, duration: 0.005s, episode steps: 1, steps per second: 183, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.040 [-0.091, 0.010], loss: 0.000006, mean_absolute_error: 0.483097, mean_q: 0.735139\n",
      " 32554/50000: episode: 713, duration: 0.212s, episode steps: 57, steps per second: 269, episode reward: 0.735, mean reward: 0.013 [-0.008, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.169 [-0.200, 0.831], loss: 0.000032, mean_absolute_error: 0.489257, mean_q: 0.733145\n",
      " 32555/50000: episode: 714, duration: 0.006s, episode steps: 1, steps per second: 163, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.020 [0.000, 0.040], loss: 0.000040, mean_absolute_error: 0.490673, mean_q: 0.743676\n",
      " 32612/50000: episode: 715, duration: 0.178s, episode steps: 57, steps per second: 321, episode reward: 0.773, mean reward: 0.014 [-0.007, 1.000], mean action: 0.930 [0.000, 2.000], mean observation: 0.144 [-0.180, 0.734], loss: 0.000129, mean_absolute_error: 0.483767, mean_q: 0.721722\n",
      " 32647/50000: episode: 716, duration: 0.130s, episode steps: 35, steps per second: 269, episode reward: 0.918, mean reward: 0.026 [-0.004, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.078 [-0.376, 0.140], loss: 0.000040, mean_absolute_error: 0.487528, mean_q: 0.727150\n",
      " 32654/50000: episode: 717, duration: 0.029s, episode steps: 7, steps per second: 243, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 1.857 [1.000, 2.000], mean observation: -0.038 [-0.122, 0.060], loss: 0.000043, mean_absolute_error: 0.492345, mean_q: 0.739296\n",
      " 32662/50000: episode: 718, duration: 0.034s, episode steps: 8, steps per second: 236, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.034 [-0.127, 0.080], loss: 0.000032, mean_absolute_error: 0.479476, mean_q: 0.722471\n",
      " 32704/50000: episode: 719, duration: 0.123s, episode steps: 42, steps per second: 341, episode reward: 0.855, mean reward: 0.020 [-0.006, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.116 [-0.579, 0.180], loss: 0.000061, mean_absolute_error: 0.492447, mean_q: 0.735596\n",
      " 32725/50000: episode: 720, duration: 0.099s, episode steps: 21, steps per second: 213, episode reward: 0.963, mean reward: 0.046 [-0.002, 1.000], mean action: 1.381 [0.000, 2.000], mean observation: -0.055 [-0.248, 0.100], loss: 0.000034, mean_absolute_error: 0.491640, mean_q: 0.736383\n",
      " 32778/50000: episode: 721, duration: 0.246s, episode steps: 53, steps per second: 215, episode reward: 0.751, mean reward: 0.014 [-0.008, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.166 [-0.815, 0.220], loss: 0.000295, mean_absolute_error: 0.492605, mean_q: 0.737211\n",
      " 32828/50000: episode: 722, duration: 0.195s, episode steps: 50, steps per second: 256, episode reward: 0.818, mean reward: 0.016 [-0.006, 1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.132 [-0.602, 0.160], loss: 0.000035, mean_absolute_error: 0.490410, mean_q: 0.734964\n",
      " 32868/50000: episode: 723, duration: 0.153s, episode steps: 40, steps per second: 262, episode reward: 0.898, mean reward: 0.022 [-0.004, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.087 [-0.423, 0.130], loss: 0.000067, mean_absolute_error: 0.485822, mean_q: 0.725503\n",
      " 32869/50000: episode: 724, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.011 [-0.032, 0.010], loss: 0.000017, mean_absolute_error: 0.507913, mean_q: 0.770290\n",
      " 32911/50000: episode: 725, duration: 0.126s, episode steps: 42, steps per second: 334, episode reward: 0.863, mean reward: 0.021 [-0.005, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.111 [-0.537, 0.160], loss: 0.000034, mean_absolute_error: 0.491410, mean_q: 0.735892\n",
      " 32972/50000: episode: 726, duration: 0.228s, episode steps: 61, steps per second: 268, episode reward: 0.721, mean reward: 0.012 [-0.009, 1.000], mean action: 0.951 [0.000, 2.000], mean observation: 0.168 [-0.200, 0.857], loss: 0.000110, mean_absolute_error: 0.486955, mean_q: 0.727529\n",
      " 33027/50000: episode: 727, duration: 0.173s, episode steps: 55, steps per second: 319, episode reward: 0.725, mean reward: 0.013 [-0.009, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.180 [-0.871, 0.230], loss: 0.000054, mean_absolute_error: 0.488031, mean_q: 0.729092\n",
      " 33048/50000: episode: 728, duration: 0.080s, episode steps: 21, steps per second: 263, episode reward: 0.972, mean reward: 0.046 [-0.002, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.051 [-0.050, 0.178], loss: 0.000030, mean_absolute_error: 0.483927, mean_q: 0.718094\n",
      " 33103/50000: episode: 729, duration: 0.192s, episode steps: 55, steps per second: 287, episode reward: 0.788, mean reward: 0.014 [-0.007, 1.000], mean action: 0.909 [0.000, 2.000], mean observation: 0.142 [-0.160, 0.664], loss: 0.000168, mean_absolute_error: 0.486899, mean_q: 0.725598\n",
      " 33145/50000: episode: 730, duration: 0.157s, episode steps: 42, steps per second: 268, episode reward: 0.865, mean reward: 0.021 [-0.005, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.112 [-0.150, 0.509], loss: 0.000135, mean_absolute_error: 0.484314, mean_q: 0.722773\n",
      " 33162/50000: episode: 731, duration: 0.058s, episode steps: 17, steps per second: 295, episode reward: 0.973, mean reward: 0.057 [-0.002, 1.000], mean action: 1.471 [0.000, 2.000], mean observation: -0.049 [-0.215, 0.090], loss: 0.000028, mean_absolute_error: 0.485588, mean_q: 0.724839\n",
      " 33204/50000: episode: 732, duration: 0.130s, episode steps: 42, steps per second: 322, episode reward: 0.857, mean reward: 0.020 [-0.005, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.118 [-0.150, 0.540], loss: 0.000040, mean_absolute_error: 0.486402, mean_q: 0.726990\n",
      " 33267/50000: episode: 733, duration: 0.246s, episode steps: 63, steps per second: 256, episode reward: 0.688, mean reward: 0.011 [-0.009, 1.000], mean action: 0.905 [0.000, 2.000], mean observation: 0.184 [-0.210, 0.900], loss: 0.000034, mean_absolute_error: 0.480941, mean_q: 0.719163\n",
      " 33324/50000: episode: 734, duration: 0.173s, episode steps: 57, steps per second: 329, episode reward: 0.755, mean reward: 0.013 [-0.008, 1.000], mean action: 0.947 [0.000, 2.000], mean observation: 0.156 [-0.200, 0.776], loss: 0.000100, mean_absolute_error: 0.488147, mean_q: 0.731429\n",
      " 33359/50000: episode: 735, duration: 0.123s, episode steps: 35, steps per second: 285, episode reward: 0.913, mean reward: 0.026 [-0.004, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.086 [-0.120, 0.370], loss: 0.000150, mean_absolute_error: 0.487922, mean_q: 0.728736\n",
      " 33382/50000: episode: 736, duration: 0.095s, episode steps: 23, steps per second: 243, episode reward: 0.958, mean reward: 0.042 [-0.003, 1.000], mean action: 0.652 [0.000, 2.000], mean observation: 0.057 [-0.100, 0.256], loss: 0.000057, mean_absolute_error: 0.492672, mean_q: 0.736955\n",
      " 33404/50000: episode: 737, duration: 0.066s, episode steps: 22, steps per second: 333, episode reward: 0.959, mean reward: 0.044 [-0.003, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.059 [-0.100, 0.255], loss: 0.000027, mean_absolute_error: 0.475191, mean_q: 0.710781\n",
      " 33460/50000: episode: 738, duration: 0.194s, episode steps: 56, steps per second: 289, episode reward: 0.735, mean reward: 0.013 [-0.008, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.172 [-0.826, 0.200], loss: 0.000040, mean_absolute_error: 0.485188, mean_q: 0.725311\n",
      " 33506/50000: episode: 739, duration: 0.141s, episode steps: 46, steps per second: 326, episode reward: 0.856, mean reward: 0.019 [-0.006, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.102 [-0.160, 0.557], loss: 0.000042, mean_absolute_error: 0.484230, mean_q: 0.723286\n",
      " 33531/50000: episode: 740, duration: 0.081s, episode steps: 25, steps per second: 308, episode reward: 0.951, mean reward: 0.038 [-0.003, 1.000], mean action: 0.680 [0.000, 2.000], mean observation: 0.063 [-0.100, 0.284], loss: 0.000034, mean_absolute_error: 0.477467, mean_q: 0.711411\n",
      " 33593/50000: episode: 741, duration: 0.237s, episode steps: 62, steps per second: 262, episode reward: 0.671, mean reward: 0.011 [-0.010, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.195 [-0.230, 0.968], loss: 0.000061, mean_absolute_error: 0.487987, mean_q: 0.731260\n",
      " 33627/50000: episode: 742, duration: 0.127s, episode steps: 34, steps per second: 269, episode reward: 0.908, mean reward: 0.027 [-0.004, 1.000], mean action: 1.147 [0.000, 2.000], mean observation: -0.087 [-0.430, 0.140], loss: 0.000470, mean_absolute_error: 0.486828, mean_q: 0.725925\n",
      " 33631/50000: episode: 743, duration: 0.014s, episode steps: 4, steps per second: 285, episode reward: 0.997, mean reward: 0.249 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [-0.040, 0.108], loss: 0.000058, mean_absolute_error: 0.483958, mean_q: 0.726692\n",
      " 33641/50000: episode: 744, duration: 0.033s, episode steps: 10, steps per second: 306, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 1.600 [0.000, 2.000], mean observation: -0.038 [-0.144, 0.070], loss: 0.000044, mean_absolute_error: 0.487319, mean_q: 0.727552\n",
      " 33703/50000: episode: 745, duration: 0.242s, episode steps: 62, steps per second: 257, episode reward: 0.671, mean reward: 0.011 [-0.010, 1.000], mean action: 1.081 [0.000, 2.000], mean observation: -0.197 [-0.961, 0.220], loss: 0.000041, mean_absolute_error: 0.485203, mean_q: 0.726819\n",
      " 33741/50000: episode: 746, duration: 0.112s, episode steps: 38, steps per second: 339, episode reward: 0.886, mean reward: 0.023 [-0.005, 1.000], mean action: 1.211 [0.000, 2.000], mean observation: -0.100 [-0.490, 0.160], loss: 0.000044, mean_absolute_error: 0.484217, mean_q: 0.723935\n",
      " 33774/50000: episode: 747, duration: 0.104s, episode steps: 33, steps per second: 318, episode reward: 0.917, mean reward: 0.028 [-0.004, 1.000], mean action: 0.788 [0.000, 2.000], mean observation: 0.085 [-0.130, 0.366], loss: 0.000033, mean_absolute_error: 0.483368, mean_q: 0.724339\n",
      " 33809/50000: episode: 748, duration: 0.131s, episode steps: 35, steps per second: 267, episode reward: 0.912, mean reward: 0.026 [-0.004, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.083 [-0.120, 0.398], loss: 0.000044, mean_absolute_error: 0.480431, mean_q: 0.716702\n",
      " 33837/50000: episode: 749, duration: 0.101s, episode steps: 28, steps per second: 278, episode reward: 0.937, mean reward: 0.033 [-0.003, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.071 [-0.332, 0.120], loss: 0.000144, mean_absolute_error: 0.490085, mean_q: 0.733493\n",
      " 33891/50000: episode: 750, duration: 0.184s, episode steps: 54, steps per second: 294, episode reward: 0.765, mean reward: 0.014 [-0.008, 1.000], mean action: 0.926 [0.000, 2.000], mean observation: 0.158 [-0.190, 0.759], loss: 0.000105, mean_absolute_error: 0.474904, mean_q: 0.710412\n",
      " 33925/50000: episode: 751, duration: 0.129s, episode steps: 34, steps per second: 263, episode reward: 0.906, mean reward: 0.027 [-0.004, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.089 [-0.150, 0.434], loss: 0.000048, mean_absolute_error: 0.493547, mean_q: 0.738389\n",
      " 33984/50000: episode: 752, duration: 0.196s, episode steps: 59, steps per second: 302, episode reward: 0.718, mean reward: 0.012 [-0.008, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.176 [-0.210, 0.846], loss: 0.000039, mean_absolute_error: 0.480272, mean_q: 0.719973\n",
      " 34001/50000: episode: 753, duration: 0.067s, episode steps: 17, steps per second: 255, episode reward: 0.979, mean reward: 0.058 [-0.002, 1.000], mean action: 0.765 [0.000, 2.000], mean observation: 0.044 [-0.060, 0.162], loss: 0.000031, mean_absolute_error: 0.478622, mean_q: 0.712360\n",
      " 34062/50000: episode: 754, duration: 0.199s, episode steps: 61, steps per second: 307, episode reward: 0.707, mean reward: 0.012 [-0.009, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.177 [-0.871, 0.220], loss: 0.000225, mean_absolute_error: 0.486281, mean_q: 0.728446\n",
      " 34116/50000: episode: 755, duration: 0.217s, episode steps: 54, steps per second: 249, episode reward: 0.762, mean reward: 0.014 [-0.008, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.158 [-0.220, 0.788], loss: 0.000040, mean_absolute_error: 0.483960, mean_q: 0.724464\n",
      " 34162/50000: episode: 756, duration: 0.150s, episode steps: 46, steps per second: 307, episode reward: 0.855, mean reward: 0.019 [-0.005, 1.000], mean action: 1.087 [0.000, 2.000], mean observation: -0.112 [-0.526, 0.150], loss: 0.000037, mean_absolute_error: 0.490664, mean_q: 0.733374\n",
      " 34228/50000: episode: 757, duration: 0.251s, episode steps: 66, steps per second: 263, episode reward: 0.712, mean reward: 0.011 [-0.009, 1.000], mean action: 1.045 [0.000, 2.000], mean observation: -0.161 [-0.863, 0.210], loss: 0.000167, mean_absolute_error: 0.480046, mean_q: 0.717533\n",
      " 34259/50000: episode: 758, duration: 0.100s, episode steps: 31, steps per second: 311, episode reward: 0.924, mean reward: 0.030 [-0.004, 1.000], mean action: 0.806 [0.000, 2.000], mean observation: 0.078 [-0.140, 0.382], loss: 0.000044, mean_absolute_error: 0.487276, mean_q: 0.728145\n",
      " 34319/50000: episode: 759, duration: 0.196s, episode steps: 60, steps per second: 306, episode reward: 0.685, mean reward: 0.011 [-0.009, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.195 [-0.220, 0.912], loss: 0.000040, mean_absolute_error: 0.488069, mean_q: 0.729717\n",
      " 34356/50000: episode: 760, duration: 0.134s, episode steps: 37, steps per second: 275, episode reward: 0.909, mean reward: 0.025 [-0.004, 1.000], mean action: 1.108 [0.000, 2.000], mean observation: -0.084 [-0.397, 0.120], loss: 0.000033, mean_absolute_error: 0.486173, mean_q: 0.728433\n",
      " 34392/50000: episode: 761, duration: 0.138s, episode steps: 36, steps per second: 260, episode reward: 0.914, mean reward: 0.025 [-0.004, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.081 [-0.120, 0.381], loss: 0.000040, mean_absolute_error: 0.482898, mean_q: 0.722092\n",
      " 34415/50000: episode: 762, duration: 0.153s, episode steps: 23, steps per second: 150, episode reward: 0.960, mean reward: 0.042 [-0.003, 1.000], mean action: 1.261 [0.000, 2.000], mean observation: -0.056 [-0.250, 0.100], loss: 0.000034, mean_absolute_error: 0.491583, mean_q: 0.736130\n",
      " 34416/50000: episode: 763, duration: 0.023s, episode steps: 1, steps per second: 44, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.035 [-0.010, 0.079], loss: 0.000010, mean_absolute_error: 0.488147, mean_q: 0.732707\n",
      " 34433/50000: episode: 764, duration: 0.099s, episode steps: 17, steps per second: 172, episode reward: 0.980, mean reward: 0.058 [-0.002, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.047 [-0.050, 0.153], loss: 0.000051, mean_absolute_error: 0.484568, mean_q: 0.725083\n",
      " 34452/50000: episode: 765, duration: 0.089s, episode steps: 19, steps per second: 212, episode reward: 0.973, mean reward: 0.051 [-0.002, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.048 [-0.192, 0.070], loss: 0.000274, mean_absolute_error: 0.487592, mean_q: 0.728708\n",
      " 34495/50000: episode: 766, duration: 0.165s, episode steps: 43, steps per second: 260, episode reward: 0.874, mean reward: 0.020 [-0.005, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.103 [-0.481, 0.140], loss: 0.000073, mean_absolute_error: 0.491713, mean_q: 0.736115\n",
      " 34551/50000: episode: 767, duration: 0.186s, episode steps: 56, steps per second: 301, episode reward: 0.835, mean reward: 0.015 [-0.006, 1.000], mean action: 0.946 [0.000, 2.000], mean observation: 0.106 [-0.180, 0.575], loss: 0.000304, mean_absolute_error: 0.488275, mean_q: 0.729956\n",
      " 34561/50000: episode: 768, duration: 0.042s, episode steps: 10, steps per second: 238, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 1.600 [0.000, 2.000], mean observation: -0.040 [-0.141, 0.060], loss: 0.000038, mean_absolute_error: 0.474708, mean_q: 0.715233\n",
      " 34621/50000: episode: 769, duration: 0.236s, episode steps: 60, steps per second: 254, episode reward: 0.743, mean reward: 0.012 [-0.007, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.163 [-0.714, 0.190], loss: 0.000037, mean_absolute_error: 0.479448, mean_q: 0.718639\n",
      " 34666/50000: episode: 770, duration: 0.169s, episode steps: 45, steps per second: 266, episode reward: 0.847, mean reward: 0.019 [-0.006, 1.000], mean action: 1.178 [0.000, 2.000], mean observation: -0.119 [-0.562, 0.130], loss: 0.000032, mean_absolute_error: 0.481016, mean_q: 0.721680\n",
      " 34708/50000: episode: 771, duration: 0.145s, episode steps: 42, steps per second: 290, episode reward: 0.876, mean reward: 0.021 [-0.005, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.098 [-0.492, 0.130], loss: 0.000039, mean_absolute_error: 0.482856, mean_q: 0.722676\n",
      " 34747/50000: episode: 772, duration: 0.142s, episode steps: 39, steps per second: 274, episode reward: 0.899, mean reward: 0.023 [-0.004, 1.000], mean action: 0.821 [0.000, 2.000], mean observation: 0.089 [-0.130, 0.416], loss: 0.000047, mean_absolute_error: 0.486386, mean_q: 0.729731\n",
      " 34777/50000: episode: 773, duration: 0.104s, episode steps: 30, steps per second: 289, episode reward: 0.935, mean reward: 0.031 [-0.003, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.071 [-0.327, 0.110], loss: 0.000030, mean_absolute_error: 0.479864, mean_q: 0.715231\n",
      " 34827/50000: episode: 774, duration: 0.174s, episode steps: 50, steps per second: 288, episode reward: 0.828, mean reward: 0.017 [-0.006, 1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.123 [-0.592, 0.150], loss: 0.000040, mean_absolute_error: 0.480501, mean_q: 0.719216\n",
      " 34872/50000: episode: 775, duration: 0.163s, episode steps: 45, steps per second: 276, episode reward: 0.886, mean reward: 0.020 [-0.004, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.089 [-0.445, 0.140], loss: 0.000086, mean_absolute_error: 0.479098, mean_q: 0.716880\n",
      " 34873/50000: episode: 776, duration: 0.007s, episode steps: 1, steps per second: 141, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.000 [-0.010, 0.010], loss: 0.000034, mean_absolute_error: 0.489710, mean_q: 0.737505\n",
      " 34912/50000: episode: 777, duration: 0.142s, episode steps: 39, steps per second: 275, episode reward: 0.885, mean reward: 0.023 [-0.005, 1.000], mean action: 0.821 [0.000, 2.000], mean observation: 0.101 [-0.150, 0.461], loss: 0.000068, mean_absolute_error: 0.488083, mean_q: 0.728456\n",
      " 34981/50000: episode: 778, duration: 0.293s, episode steps: 69, steps per second: 236, episode reward: 0.625, mean reward: 0.009 [-0.010, 1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.207 [-0.992, 0.200], loss: 0.000059, mean_absolute_error: 0.479527, mean_q: 0.717708\n",
      " 34994/50000: episode: 779, duration: 0.058s, episode steps: 13, steps per second: 226, episode reward: 0.984, mean reward: 0.076 [-0.002, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.041 [-0.080, 0.157], loss: 0.000030, mean_absolute_error: 0.482813, mean_q: 0.725185\n",
      " 34995/50000: episode: 780, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.016 [0.010, 0.022], loss: 0.000016, mean_absolute_error: 0.500585, mean_q: 0.758017\n",
      " 35063/50000: episode: 781, duration: 0.319s, episode steps: 68, steps per second: 213, episode reward: 0.642, mean reward: 0.009 [-0.010, 1.000], mean action: 1.103 [0.000, 2.000], mean observation: -0.198 [-0.991, 0.240], loss: 0.000249, mean_absolute_error: 0.482857, mean_q: 0.721505\n",
      " 35064/50000: episode: 782, duration: 0.008s, episode steps: 1, steps per second: 125, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.044 [-0.099, 0.010], loss: 0.000033, mean_absolute_error: 0.475460, mean_q: 0.702395\n",
      " 35113/50000: episode: 783, duration: 0.150s, episode steps: 49, steps per second: 326, episode reward: 0.825, mean reward: 0.017 [-0.006, 1.000], mean action: 0.898 [0.000, 2.000], mean observation: 0.126 [-0.190, 0.619], loss: 0.000250, mean_absolute_error: 0.481742, mean_q: 0.721100\n",
      " 35149/50000: episode: 784, duration: 0.136s, episode steps: 36, steps per second: 266, episode reward: 0.932, mean reward: 0.026 [-0.003, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.048 [-0.334, 0.120], loss: 0.000306, mean_absolute_error: 0.481808, mean_q: 0.721299\n",
      " 35198/50000: episode: 785, duration: 0.166s, episode steps: 49, steps per second: 296, episode reward: 0.822, mean reward: 0.017 [-0.006, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.126 [-0.180, 0.609], loss: 0.000242, mean_absolute_error: 0.482355, mean_q: 0.722161\n",
      " 35228/50000: episode: 786, duration: 0.130s, episode steps: 30, steps per second: 230, episode reward: 0.935, mean reward: 0.031 [-0.003, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.065 [-0.120, 0.331], loss: 0.000026, mean_absolute_error: 0.480447, mean_q: 0.720912\n",
      " 35273/50000: episode: 787, duration: 0.221s, episode steps: 45, steps per second: 204, episode reward: 0.860, mean reward: 0.019 [-0.005, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.108 [-0.536, 0.150], loss: 0.000026, mean_absolute_error: 0.472657, mean_q: 0.708902\n",
      " 35297/50000: episode: 788, duration: 0.116s, episode steps: 24, steps per second: 207, episode reward: 0.950, mean reward: 0.040 [-0.003, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.061 [-0.120, 0.304], loss: 0.000035, mean_absolute_error: 0.484890, mean_q: 0.727087\n",
      " 35334/50000: episode: 789, duration: 0.137s, episode steps: 37, steps per second: 270, episode reward: 0.916, mean reward: 0.025 [-0.004, 1.000], mean action: 0.892 [0.000, 2.000], mean observation: 0.077 [-0.120, 0.379], loss: 0.000032, mean_absolute_error: 0.483012, mean_q: 0.725766\n",
      " 35384/50000: episode: 790, duration: 0.183s, episode steps: 50, steps per second: 273, episode reward: 0.783, mean reward: 0.016 [-0.007, 1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.154 [-0.740, 0.220], loss: 0.000033, mean_absolute_error: 0.476240, mean_q: 0.712608\n",
      " 35414/50000: episode: 791, duration: 0.118s, episode steps: 30, steps per second: 255, episode reward: 0.936, mean reward: 0.031 [-0.003, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.071 [-0.110, 0.321], loss: 0.000026, mean_absolute_error: 0.488183, mean_q: 0.733601\n",
      " 35415/50000: episode: 792, duration: 0.008s, episode steps: 1, steps per second: 129, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.017 [-0.010, 0.044], loss: 0.000020, mean_absolute_error: 0.487530, mean_q: 0.738630\n",
      " 35477/50000: episode: 793, duration: 0.201s, episode steps: 62, steps per second: 309, episode reward: 0.688, mean reward: 0.011 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.186 [-0.912, 0.200], loss: 0.000028, mean_absolute_error: 0.474499, mean_q: 0.710171\n",
      " 35512/50000: episode: 794, duration: 0.180s, episode steps: 35, steps per second: 194, episode reward: 0.904, mean reward: 0.026 [-0.004, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.093 [-0.140, 0.408], loss: 0.000033, mean_absolute_error: 0.473661, mean_q: 0.707732\n",
      " 35571/50000: episode: 795, duration: 0.342s, episode steps: 59, steps per second: 173, episode reward: 0.717, mean reward: 0.012 [-0.008, 1.000], mean action: 0.932 [0.000, 2.000], mean observation: 0.179 [-0.230, 0.826], loss: 0.000033, mean_absolute_error: 0.480682, mean_q: 0.719880\n",
      " 35616/50000: episode: 796, duration: 0.193s, episode steps: 45, steps per second: 233, episode reward: 0.838, mean reward: 0.019 [-0.006, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.124 [-0.180, 0.616], loss: 0.000033, mean_absolute_error: 0.480174, mean_q: 0.719839\n",
      " 35637/50000: episode: 797, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 0.962, mean reward: 0.046 [-0.003, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.048 [-0.130, 0.260], loss: 0.000078, mean_absolute_error: 0.487749, mean_q: 0.732360\n",
      " 35650/50000: episode: 798, duration: 0.078s, episode steps: 13, steps per second: 167, episode reward: 0.984, mean reward: 0.076 [-0.001, 1.000], mean action: 0.385 [0.000, 2.000], mean observation: 0.043 [-0.090, 0.149], loss: 0.000043, mean_absolute_error: 0.477278, mean_q: 0.713422\n",
      " 35681/50000: episode: 799, duration: 0.116s, episode steps: 31, steps per second: 267, episode reward: 0.926, mean reward: 0.030 [-0.004, 1.000], mean action: 0.774 [0.000, 2.000], mean observation: 0.077 [-0.130, 0.366], loss: 0.000032, mean_absolute_error: 0.473859, mean_q: 0.707338\n",
      " 35734/50000: episode: 800, duration: 0.264s, episode steps: 53, steps per second: 201, episode reward: 0.778, mean reward: 0.015 [-0.007, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.151 [-0.726, 0.200], loss: 0.000138, mean_absolute_error: 0.480688, mean_q: 0.719700\n",
      " 35756/50000: episode: 801, duration: 0.074s, episode steps: 22, steps per second: 296, episode reward: 0.959, mean reward: 0.044 [-0.003, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.052 [-0.120, 0.271], loss: 0.000034, mean_absolute_error: 0.485301, mean_q: 0.725634\n",
      " 35813/50000: episode: 802, duration: 0.181s, episode steps: 57, steps per second: 315, episode reward: 0.755, mean reward: 0.013 [-0.008, 1.000], mean action: 1.088 [0.000, 2.000], mean observation: -0.157 [-0.762, 0.200], loss: 0.000087, mean_absolute_error: 0.475409, mean_q: 0.710440\n",
      " 35874/50000: episode: 803, duration: 0.246s, episode steps: 61, steps per second: 248, episode reward: 0.707, mean reward: 0.012 [-0.008, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.178 [-0.838, 0.200], loss: 0.000114, mean_absolute_error: 0.474403, mean_q: 0.709456\n",
      " 35915/50000: episode: 804, duration: 0.158s, episode steps: 41, steps per second: 260, episode reward: 0.881, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.100 [-0.462, 0.140], loss: 0.000053, mean_absolute_error: 0.484251, mean_q: 0.728594\n",
      " 35972/50000: episode: 805, duration: 0.239s, episode steps: 57, steps per second: 239, episode reward: 0.712, mean reward: 0.012 [-0.009, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.183 [-0.895, 0.240], loss: 0.000031, mean_absolute_error: 0.478810, mean_q: 0.717894\n",
      " 36030/50000: episode: 806, duration: 0.194s, episode steps: 58, steps per second: 299, episode reward: 0.708, mean reward: 0.012 [-0.009, 1.000], mean action: 0.914 [0.000, 2.000], mean observation: 0.185 [-0.220, 0.875], loss: 0.000030, mean_absolute_error: 0.483839, mean_q: 0.726319\n",
      " 36031/50000: episode: 807, duration: 0.007s, episode steps: 1, steps per second: 142, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [-0.010, 0.075], loss: 0.000022, mean_absolute_error: 0.499196, mean_q: 0.744414\n",
      " 36089/50000: episode: 808, duration: 0.219s, episode steps: 58, steps per second: 265, episode reward: 0.715, mean reward: 0.012 [-0.009, 1.000], mean action: 1.138 [0.000, 2.000], mean observation: -0.177 [-0.894, 0.240], loss: 0.000030, mean_absolute_error: 0.476277, mean_q: 0.714712\n",
      " 36115/50000: episode: 809, duration: 0.105s, episode steps: 26, steps per second: 249, episode reward: 0.951, mean reward: 0.037 [-0.003, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.059 [-0.120, 0.267], loss: 0.000027, mean_absolute_error: 0.475095, mean_q: 0.710904\n",
      " 36116/50000: episode: 810, duration: 0.007s, episode steps: 1, steps per second: 147, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [-0.010, 0.051], loss: 0.000037, mean_absolute_error: 0.440764, mean_q: 0.665201\n",
      " 36128/50000: episode: 811, duration: 0.052s, episode steps: 12, steps per second: 230, episode reward: 0.986, mean reward: 0.082 [-0.001, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.040 [-0.060, 0.150], loss: 0.000280, mean_absolute_error: 0.468747, mean_q: 0.705897\n",
      " 36171/50000: episode: 812, duration: 0.163s, episode steps: 43, steps per second: 263, episode reward: 0.860, mean reward: 0.020 [-0.006, 1.000], mean action: 0.930 [0.000, 2.000], mean observation: 0.110 [-0.180, 0.560], loss: 0.000076, mean_absolute_error: 0.485756, mean_q: 0.730048\n",
      " 36212/50000: episode: 813, duration: 0.148s, episode steps: 41, steps per second: 278, episode reward: 0.878, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.101 [-0.477, 0.130], loss: 0.000055, mean_absolute_error: 0.481552, mean_q: 0.723278\n",
      " 36247/50000: episode: 814, duration: 0.160s, episode steps: 35, steps per second: 219, episode reward: 0.910, mean reward: 0.026 [-0.004, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.087 [-0.393, 0.110], loss: 0.000034, mean_absolute_error: 0.478233, mean_q: 0.716735\n",
      " 36248/50000: episode: 815, duration: 0.024s, episode steps: 1, steps per second: 42, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.024 [-0.058, 0.010], loss: 0.000051, mean_absolute_error: 0.482397, mean_q: 0.719122\n",
      " 36268/50000: episode: 816, duration: 0.258s, episode steps: 20, steps per second: 78, episode reward: 0.964, mean reward: 0.048 [-0.003, 1.000], mean action: 0.650 [0.000, 2.000], mean observation: 0.052 [-0.120, 0.250], loss: 0.000030, mean_absolute_error: 0.475213, mean_q: 0.711388\n",
      " 36313/50000: episode: 817, duration: 0.475s, episode steps: 45, steps per second: 95, episode reward: 0.866, mean reward: 0.019 [-0.005, 1.000], mean action: 1.178 [0.000, 2.000], mean observation: -0.106 [-0.489, 0.120], loss: 0.000029, mean_absolute_error: 0.473027, mean_q: 0.706783\n",
      " 36363/50000: episode: 818, duration: 0.354s, episode steps: 50, steps per second: 141, episode reward: 0.753, mean reward: 0.015 [-0.008, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.175 [-0.230, 0.823], loss: 0.000035, mean_absolute_error: 0.481714, mean_q: 0.722808\n",
      " 36423/50000: episode: 819, duration: 0.589s, episode steps: 60, steps per second: 102, episode reward: 0.715, mean reward: 0.012 [-0.009, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.169 [-0.200, 0.870], loss: 0.000023, mean_absolute_error: 0.470640, mean_q: 0.705824\n",
      " 36485/50000: episode: 820, duration: 0.564s, episode steps: 62, steps per second: 110, episode reward: 0.669, mean reward: 0.011 [-0.009, 1.000], mean action: 0.871 [0.000, 2.000], mean observation: 0.201 [-0.210, 0.921], loss: 0.000186, mean_absolute_error: 0.470112, mean_q: 0.704250\n",
      " 36533/50000: episode: 821, duration: 0.372s, episode steps: 48, steps per second: 129, episode reward: 0.823, mean reward: 0.017 [-0.006, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.128 [-0.642, 0.190], loss: 0.000051, mean_absolute_error: 0.482469, mean_q: 0.724361\n",
      " 36576/50000: episode: 822, duration: 0.287s, episode steps: 43, steps per second: 150, episode reward: 0.878, mean reward: 0.020 [-0.005, 1.000], mean action: 1.163 [0.000, 2.000], mean observation: -0.099 [-0.484, 0.150], loss: 0.000460, mean_absolute_error: 0.475803, mean_q: 0.709676\n",
      " 36632/50000: episode: 823, duration: 0.352s, episode steps: 56, steps per second: 159, episode reward: 0.768, mean reward: 0.014 [-0.007, 1.000], mean action: 1.089 [0.000, 2.000], mean observation: -0.153 [-0.712, 0.180], loss: 0.000060, mean_absolute_error: 0.477048, mean_q: 0.714990\n",
      " 36671/50000: episode: 824, duration: 0.181s, episode steps: 39, steps per second: 216, episode reward: 0.876, mean reward: 0.022 [-0.005, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.103 [-0.150, 0.518], loss: 0.000030, mean_absolute_error: 0.476606, mean_q: 0.715530\n",
      " 36693/50000: episode: 825, duration: 0.105s, episode steps: 22, steps per second: 210, episode reward: 0.960, mean reward: 0.044 [-0.003, 1.000], mean action: 0.773 [0.000, 2.000], mean observation: 0.056 [-0.120, 0.256], loss: 0.000034, mean_absolute_error: 0.466814, mean_q: 0.696560\n",
      " 36749/50000: episode: 826, duration: 0.259s, episode steps: 56, steps per second: 216, episode reward: 0.749, mean reward: 0.013 [-0.008, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.156 [-0.220, 0.815], loss: 0.000025, mean_absolute_error: 0.479041, mean_q: 0.718154\n",
      " 36785/50000: episode: 827, duration: 0.148s, episode steps: 36, steps per second: 243, episode reward: 0.891, mean reward: 0.025 [-0.005, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.099 [-0.160, 0.467], loss: 0.000030, mean_absolute_error: 0.473517, mean_q: 0.710707\n",
      " 36833/50000: episode: 828, duration: 0.172s, episode steps: 48, steps per second: 279, episode reward: 0.826, mean reward: 0.017 [-0.006, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.127 [-0.628, 0.170], loss: 0.000024, mean_absolute_error: 0.477898, mean_q: 0.717540\n",
      " 36858/50000: episode: 829, duration: 0.093s, episode steps: 25, steps per second: 269, episode reward: 0.946, mean reward: 0.038 [-0.003, 1.000], mean action: 0.640 [0.000, 2.000], mean observation: 0.065 [-0.130, 0.311], loss: 0.000040, mean_absolute_error: 0.476549, mean_q: 0.716283\n",
      " 36907/50000: episode: 830, duration: 0.176s, episode steps: 49, steps per second: 278, episode reward: 0.851, mean reward: 0.017 [-0.005, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.106 [-0.517, 0.140], loss: 0.000051, mean_absolute_error: 0.478811, mean_q: 0.717321\n",
      " 36961/50000: episode: 831, duration: 0.198s, episode steps: 54, steps per second: 273, episode reward: 0.747, mean reward: 0.014 [-0.008, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.167 [-0.210, 0.820], loss: 0.000032, mean_absolute_error: 0.476413, mean_q: 0.715174\n",
      " 37027/50000: episode: 832, duration: 0.257s, episode steps: 66, steps per second: 257, episode reward: 0.633, mean reward: 0.010 [-0.010, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.206 [-0.250, 0.990], loss: 0.000047, mean_absolute_error: 0.476407, mean_q: 0.713220\n",
      " 37074/50000: episode: 833, duration: 0.159s, episode steps: 47, steps per second: 296, episode reward: 0.836, mean reward: 0.018 [-0.005, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.128 [-0.549, 0.130], loss: 0.000020, mean_absolute_error: 0.473495, mean_q: 0.711802\n",
      " 37132/50000: episode: 834, duration: 0.211s, episode steps: 58, steps per second: 275, episode reward: 0.690, mean reward: 0.012 [-0.009, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.196 [-0.230, 0.928], loss: 0.000023, mean_absolute_error: 0.472083, mean_q: 0.707879\n",
      " 37191/50000: episode: 835, duration: 0.264s, episode steps: 59, steps per second: 223, episode reward: 0.689, mean reward: 0.012 [-0.009, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.196 [-0.904, 0.230], loss: 0.000021, mean_absolute_error: 0.473899, mean_q: 0.709502\n",
      " 37248/50000: episode: 836, duration: 0.247s, episode steps: 57, steps per second: 231, episode reward: 0.694, mean reward: 0.012 [-0.009, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.194 [-0.934, 0.230], loss: 0.000047, mean_absolute_error: 0.471191, mean_q: 0.704270\n",
      " 37302/50000: episode: 837, duration: 0.274s, episode steps: 54, steps per second: 197, episode reward: 0.742, mean reward: 0.014 [-0.008, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.174 [-0.220, 0.813], loss: 0.000025, mean_absolute_error: 0.475800, mean_q: 0.711851\n",
      " 37322/50000: episode: 838, duration: 0.096s, episode steps: 20, steps per second: 208, episode reward: 0.966, mean reward: 0.048 [-0.002, 1.000], mean action: 0.600 [0.000, 2.000], mean observation: 0.051 [-0.120, 0.242], loss: 0.000028, mean_absolute_error: 0.477965, mean_q: 0.715645\n",
      " 37363/50000: episode: 839, duration: 0.181s, episode steps: 41, steps per second: 227, episode reward: 0.877, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.107 [-0.459, 0.130], loss: 0.000022, mean_absolute_error: 0.469945, mean_q: 0.704329\n",
      " 37377/50000: episode: 840, duration: 0.076s, episode steps: 14, steps per second: 185, episode reward: 0.981, mean reward: 0.070 [-0.002, 1.000], mean action: 1.643 [0.000, 2.000], mean observation: -0.042 [-0.178, 0.100], loss: 0.000023, mean_absolute_error: 0.485392, mean_q: 0.724447\n",
      " 37401/50000: episode: 841, duration: 0.113s, episode steps: 24, steps per second: 212, episode reward: 0.954, mean reward: 0.040 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.058 [-0.277, 0.110], loss: 0.000022, mean_absolute_error: 0.474336, mean_q: 0.710801\n",
      " 37462/50000: episode: 842, duration: 0.242s, episode steps: 61, steps per second: 252, episode reward: 0.665, mean reward: 0.011 [-0.010, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.204 [-0.220, 0.971], loss: 0.000033, mean_absolute_error: 0.482394, mean_q: 0.722600\n",
      " 37504/50000: episode: 843, duration: 0.134s, episode steps: 42, steps per second: 314, episode reward: 0.886, mean reward: 0.021 [-0.005, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.072 [-0.180, 0.505], loss: 0.000307, mean_absolute_error: 0.472244, mean_q: 0.707131\n",
      " 37568/50000: episode: 844, duration: 0.219s, episode steps: 64, steps per second: 292, episode reward: 0.717, mean reward: 0.011 [-0.008, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.161 [-0.764, 0.210], loss: 0.000040, mean_absolute_error: 0.476816, mean_q: 0.715482\n",
      " 37569/50000: episode: 845, duration: 0.008s, episode steps: 1, steps per second: 124, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.032 [-0.073, 0.010], loss: 0.000027, mean_absolute_error: 0.501591, mean_q: 0.752138\n",
      " 37624/50000: episode: 846, duration: 0.176s, episode steps: 55, steps per second: 313, episode reward: 0.708, mean reward: 0.013 [-0.009, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.191 [-0.901, 0.240], loss: 0.000024, mean_absolute_error: 0.475292, mean_q: 0.711824\n",
      " 37625/50000: episode: 847, duration: 0.006s, episode steps: 1, steps per second: 165, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.035 [-0.010, 0.080], loss: 0.000029, mean_absolute_error: 0.451781, mean_q: 0.675063\n",
      " 37683/50000: episode: 848, duration: 0.225s, episode steps: 58, steps per second: 258, episode reward: 0.678, mean reward: 0.012 [-0.009, 1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.206 [-0.937, 0.240], loss: 0.000025, mean_absolute_error: 0.479056, mean_q: 0.718332\n",
      " 37736/50000: episode: 849, duration: 0.170s, episode steps: 53, steps per second: 311, episode reward: 0.755, mean reward: 0.014 [-0.007, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.171 [-0.200, 0.742], loss: 0.000080, mean_absolute_error: 0.476312, mean_q: 0.713926\n",
      " 37763/50000: episode: 850, duration: 0.096s, episode steps: 27, steps per second: 281, episode reward: 0.942, mean reward: 0.035 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.065 [-0.321, 0.130], loss: 0.000021, mean_absolute_error: 0.476008, mean_q: 0.714533\n",
      " 37799/50000: episode: 851, duration: 0.152s, episode steps: 36, steps per second: 237, episode reward: 0.899, mean reward: 0.025 [-0.005, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.088 [-0.150, 0.453], loss: 0.000022, mean_absolute_error: 0.473221, mean_q: 0.712219\n",
      " 37830/50000: episode: 852, duration: 0.117s, episode steps: 31, steps per second: 266, episode reward: 0.930, mean reward: 0.030 [-0.004, 1.000], mean action: 0.710 [0.000, 2.000], mean observation: 0.065 [-0.140, 0.361], loss: 0.000022, mean_absolute_error: 0.477687, mean_q: 0.715610\n",
      " 37866/50000: episode: 853, duration: 0.129s, episode steps: 36, steps per second: 278, episode reward: 0.900, mean reward: 0.025 [-0.005, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.085 [-0.160, 0.455], loss: 0.000032, mean_absolute_error: 0.468872, mean_q: 0.703497\n",
      " 37892/50000: episode: 854, duration: 0.083s, episode steps: 26, steps per second: 312, episode reward: 0.947, mean reward: 0.036 [-0.003, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.059 [-0.120, 0.310], loss: 0.000031, mean_absolute_error: 0.483109, mean_q: 0.722153\n",
      " 37941/50000: episode: 855, duration: 0.228s, episode steps: 49, steps per second: 215, episode reward: 0.857, mean reward: 0.017 [-0.006, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.084 [-0.565, 0.160], loss: 0.000310, mean_absolute_error: 0.474177, mean_q: 0.710321\n",
      " 37994/50000: episode: 856, duration: 0.185s, episode steps: 53, steps per second: 287, episode reward: 0.735, mean reward: 0.014 [-0.009, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.177 [-0.852, 0.230], loss: 0.000028, mean_absolute_error: 0.466414, mean_q: 0.697079\n",
      " 37995/50000: episode: 857, duration: 0.006s, episode steps: 1, steps per second: 177, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.010, 0.081], loss: 0.000042, mean_absolute_error: 0.462380, mean_q: 0.697552\n",
      " 38034/50000: episode: 858, duration: 0.154s, episode steps: 39, steps per second: 253, episode reward: 0.881, mean reward: 0.023 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.102 [-0.492, 0.130], loss: 0.000023, mean_absolute_error: 0.482147, mean_q: 0.720887\n",
      " 38086/50000: episode: 859, duration: 0.223s, episode steps: 52, steps per second: 233, episode reward: 0.768, mean reward: 0.015 [-0.008, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.160 [-0.180, 0.752], loss: 0.000021, mean_absolute_error: 0.477182, mean_q: 0.715990\n",
      " 38145/50000: episode: 860, duration: 0.202s, episode steps: 59, steps per second: 292, episode reward: 0.698, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.187 [-0.892, 0.240], loss: 0.000023, mean_absolute_error: 0.473266, mean_q: 0.709703\n",
      " 38146/50000: episode: 861, duration: 0.009s, episode steps: 1, steps per second: 118, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [-0.010, 0.041], loss: 0.000007, mean_absolute_error: 0.451537, mean_q: 0.678010\n",
      " 38152/50000: episode: 862, duration: 0.024s, episode steps: 6, steps per second: 248, episode reward: 0.994, mean reward: 0.166 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.119, 0.060], loss: 0.000014, mean_absolute_error: 0.482986, mean_q: 0.721158\n",
      " 38185/50000: episode: 863, duration: 0.141s, episode steps: 33, steps per second: 235, episode reward: 0.919, mean reward: 0.028 [-0.004, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.074 [-0.140, 0.394], loss: 0.000462, mean_absolute_error: 0.481448, mean_q: 0.720078\n",
      " 38225/50000: episode: 864, duration: 0.163s, episode steps: 40, steps per second: 245, episode reward: 0.886, mean reward: 0.022 [-0.005, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.088 [-0.150, 0.488], loss: 0.000044, mean_absolute_error: 0.474152, mean_q: 0.708893\n",
      " 38270/50000: episode: 865, duration: 0.150s, episode steps: 45, steps per second: 300, episode reward: 0.900, mean reward: 0.020 [-0.004, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.061 [-0.140, 0.431], loss: 0.000053, mean_absolute_error: 0.475043, mean_q: 0.712096\n",
      " 38289/50000: episode: 866, duration: 0.089s, episode steps: 19, steps per second: 214, episode reward: 0.969, mean reward: 0.051 [-0.002, 1.000], mean action: 1.474 [0.000, 2.000], mean observation: -0.046 [-0.230, 0.110], loss: 0.000036, mean_absolute_error: 0.474784, mean_q: 0.711671\n",
      " 38311/50000: episode: 867, duration: 0.088s, episode steps: 22, steps per second: 250, episode reward: 0.957, mean reward: 0.043 [-0.003, 1.000], mean action: 1.409 [0.000, 2.000], mean observation: -0.059 [-0.276, 0.120], loss: 0.000026, mean_absolute_error: 0.467685, mean_q: 0.700639\n",
      " 38312/50000: episode: 868, duration: 0.007s, episode steps: 1, steps per second: 148, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.020 [-0.051, 0.010], loss: 0.000018, mean_absolute_error: 0.461225, mean_q: 0.698389\n",
      " 38354/50000: episode: 869, duration: 0.162s, episode steps: 42, steps per second: 260, episode reward: 0.857, mean reward: 0.020 [-0.005, 1.000], mean action: 1.190 [0.000, 2.000], mean observation: -0.119 [-0.539, 0.150], loss: 0.000026, mean_absolute_error: 0.476834, mean_q: 0.714906\n",
      " 38400/50000: episode: 870, duration: 0.163s, episode steps: 46, steps per second: 282, episode reward: 0.814, mean reward: 0.018 [-0.007, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.140 [-0.180, 0.681], loss: 0.000021, mean_absolute_error: 0.478562, mean_q: 0.718593\n",
      " 38401/50000: episode: 871, duration: 0.006s, episode steps: 1, steps per second: 174, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.007 [-0.024, 0.010], loss: 0.000030, mean_absolute_error: 0.493029, mean_q: 0.744295\n",
      " 38451/50000: episode: 872, duration: 0.206s, episode steps: 50, steps per second: 243, episode reward: 0.760, mean reward: 0.015 [-0.008, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.171 [-0.783, 0.230], loss: 0.000032, mean_absolute_error: 0.469227, mean_q: 0.702567\n",
      " 38505/50000: episode: 873, duration: 0.185s, episode steps: 54, steps per second: 293, episode reward: 0.763, mean reward: 0.014 [-0.008, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.144 [-0.809, 0.240], loss: 0.000027, mean_absolute_error: 0.471542, mean_q: 0.704342\n",
      " 38546/50000: episode: 874, duration: 0.177s, episode steps: 41, steps per second: 232, episode reward: 0.863, mean reward: 0.021 [-0.006, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.110 [-0.180, 0.555], loss: 0.000032, mean_absolute_error: 0.475458, mean_q: 0.711082\n",
      " 38578/50000: episode: 875, duration: 0.112s, episode steps: 32, steps per second: 286, episode reward: 0.938, mean reward: 0.029 [-0.004, 1.000], mean action: 1.281 [0.000, 2.000], mean observation: -0.037 [-0.355, 0.180], loss: 0.000338, mean_absolute_error: 0.485637, mean_q: 0.726856\n",
      " 38612/50000: episode: 876, duration: 0.115s, episode steps: 34, steps per second: 296, episode reward: 0.910, mean reward: 0.027 [-0.004, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.083 [-0.160, 0.422], loss: 0.000278, mean_absolute_error: 0.478221, mean_q: 0.716181\n",
      " 38637/50000: episode: 877, duration: 0.094s, episode steps: 25, steps per second: 265, episode reward: 0.970, mean reward: 0.039 [-0.001, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.057 [-0.050, 0.137], loss: 0.000093, mean_absolute_error: 0.474629, mean_q: 0.708854\n",
      " 38674/50000: episode: 878, duration: 0.128s, episode steps: 37, steps per second: 290, episode reward: 0.901, mean reward: 0.024 [-0.004, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.083 [-0.160, 0.449], loss: 0.000035, mean_absolute_error: 0.481440, mean_q: 0.722157\n",
      " 38731/50000: episode: 879, duration: 0.220s, episode steps: 57, steps per second: 259, episode reward: 0.705, mean reward: 0.012 [-0.009, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.187 [-0.230, 0.903], loss: 0.000024, mean_absolute_error: 0.478160, mean_q: 0.718387\n",
      " 38754/50000: episode: 880, duration: 0.083s, episode steps: 23, steps per second: 277, episode reward: 0.962, mean reward: 0.042 [-0.002, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.045 [-0.110, 0.249], loss: 0.000026, mean_absolute_error: 0.477659, mean_q: 0.713314\n",
      " 38791/50000: episode: 881, duration: 0.136s, episode steps: 37, steps per second: 273, episode reward: 0.902, mean reward: 0.024 [-0.004, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.082 [-0.435, 0.140], loss: 0.000137, mean_absolute_error: 0.474651, mean_q: 0.712659\n",
      " 38793/50000: episode: 882, duration: 0.011s, episode steps: 2, steps per second: 179, episode reward: 0.999, mean reward: 0.499 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.042 [-0.101, 0.020], loss: 0.000021, mean_absolute_error: 0.497967, mean_q: 0.749407\n",
      " 38832/50000: episode: 883, duration: 0.144s, episode steps: 39, steps per second: 270, episode reward: 0.866, mean reward: 0.022 [-0.005, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.117 [-0.160, 0.531], loss: 0.000089, mean_absolute_error: 0.478848, mean_q: 0.720255\n",
      " 38871/50000: episode: 884, duration: 0.146s, episode steps: 39, steps per second: 268, episode reward: 0.878, mean reward: 0.023 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.100 [-0.513, 0.170], loss: 0.000256, mean_absolute_error: 0.473880, mean_q: 0.708349\n",
      " 38924/50000: episode: 885, duration: 0.176s, episode steps: 53, steps per second: 301, episode reward: 0.737, mean reward: 0.014 [-0.009, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.175 [-0.856, 0.240], loss: 0.000027, mean_absolute_error: 0.478252, mean_q: 0.717483\n",
      " 38968/50000: episode: 886, duration: 0.156s, episode steps: 44, steps per second: 282, episode reward: 0.821, mean reward: 0.019 [-0.007, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.139 [-0.190, 0.666], loss: 0.000021, mean_absolute_error: 0.477822, mean_q: 0.715548\n",
      " 38987/50000: episode: 887, duration: 0.082s, episode steps: 19, steps per second: 233, episode reward: 0.967, mean reward: 0.051 [-0.002, 1.000], mean action: 0.632 [0.000, 2.000], mean observation: 0.052 [-0.120, 0.243], loss: 0.000020, mean_absolute_error: 0.474628, mean_q: 0.712111\n",
      " 39042/50000: episode: 888, duration: 0.173s, episode steps: 55, steps per second: 318, episode reward: 0.737, mean reward: 0.013 [-0.008, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.169 [-0.210, 0.838], loss: 0.000098, mean_absolute_error: 0.478025, mean_q: 0.714509\n",
      " 39055/50000: episode: 889, duration: 0.050s, episode steps: 13, steps per second: 262, episode reward: 0.982, mean reward: 0.076 [-0.002, 1.000], mean action: 1.692 [0.000, 2.000], mean observation: -0.041 [-0.175, 0.100], loss: 0.000024, mean_absolute_error: 0.471544, mean_q: 0.709741\n",
      " 39090/50000: episode: 890, duration: 0.126s, episode steps: 35, steps per second: 278, episode reward: 0.889, mean reward: 0.025 [-0.005, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.101 [-0.499, 0.170], loss: 0.000031, mean_absolute_error: 0.482927, mean_q: 0.726351\n",
      " 39147/50000: episode: 891, duration: 0.208s, episode steps: 57, steps per second: 274, episode reward: 0.678, mean reward: 0.012 [-0.010, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.205 [-0.968, 0.240], loss: 0.000023, mean_absolute_error: 0.481114, mean_q: 0.721362\n",
      " 39180/50000: episode: 892, duration: 0.112s, episode steps: 33, steps per second: 294, episode reward: 0.913, mean reward: 0.028 [-0.004, 1.000], mean action: 0.758 [0.000, 2.000], mean observation: 0.086 [-0.130, 0.410], loss: 0.000024, mean_absolute_error: 0.476448, mean_q: 0.713438\n",
      " 39234/50000: episode: 893, duration: 0.185s, episode steps: 54, steps per second: 292, episode reward: 0.734, mean reward: 0.014 [-0.009, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.169 [-0.872, 0.240], loss: 0.000020, mean_absolute_error: 0.475431, mean_q: 0.713129\n",
      " 39251/50000: episode: 894, duration: 0.085s, episode steps: 17, steps per second: 201, episode reward: 0.974, mean reward: 0.057 [-0.002, 1.000], mean action: 1.412 [0.000, 2.000], mean observation: -0.049 [-0.199, 0.100], loss: 0.000464, mean_absolute_error: 0.482658, mean_q: 0.722436\n",
      " 39294/50000: episode: 895, duration: 0.147s, episode steps: 43, steps per second: 293, episode reward: 0.839, mean reward: 0.020 [-0.006, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.120 [-0.639, 0.210], loss: 0.000055, mean_absolute_error: 0.485581, mean_q: 0.729595\n",
      " 39339/50000: episode: 896, duration: 0.167s, episode steps: 45, steps per second: 270, episode reward: 0.827, mean reward: 0.018 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.135 [-0.190, 0.622], loss: 0.000027, mean_absolute_error: 0.482673, mean_q: 0.724131\n",
      " 39388/50000: episode: 897, duration: 0.182s, episode steps: 49, steps per second: 270, episode reward: 0.809, mean reward: 0.017 [-0.006, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.140 [-0.170, 0.639], loss: 0.000024, mean_absolute_error: 0.479932, mean_q: 0.720435\n",
      " 39397/50000: episode: 898, duration: 0.038s, episode steps: 9, steps per second: 238, episode reward: 0.991, mean reward: 0.110 [-0.001, 1.000], mean action: 0.333 [0.000, 2.000], mean observation: 0.038 [-0.060, 0.128], loss: 0.000060, mean_absolute_error: 0.464773, mean_q: 0.691414\n",
      " 39401/50000: episode: 899, duration: 0.014s, episode steps: 4, steps per second: 288, episode reward: 0.997, mean reward: 0.249 [-0.001, 1.000], mean action: 1.750 [1.000, 2.000], mean observation: -0.043 [-0.106, 0.030], loss: 0.000153, mean_absolute_error: 0.466863, mean_q: 0.700854\n",
      " 39402/50000: episode: 900, duration: 0.006s, episode steps: 1, steps per second: 161, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.026 [-0.010, 0.061], loss: 0.000116, mean_absolute_error: 0.499530, mean_q: 0.753593\n",
      " 39445/50000: episode: 901, duration: 0.127s, episode steps: 43, steps per second: 338, episode reward: 0.849, mean reward: 0.020 [-0.006, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.122 [-0.180, 0.553], loss: 0.000036, mean_absolute_error: 0.478396, mean_q: 0.717245\n",
      " 39446/50000: episode: 902, duration: 0.007s, episode steps: 1, steps per second: 153, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.034 [0.000, 0.068], loss: 0.000014, mean_absolute_error: 0.446190, mean_q: 0.661655\n",
      " 39471/50000: episode: 903, duration: 0.110s, episode steps: 25, steps per second: 227, episode reward: 0.958, mean reward: 0.038 [-0.002, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.057 [-0.090, 0.242], loss: 0.000022, mean_absolute_error: 0.472440, mean_q: 0.706871\n",
      " 39492/50000: episode: 904, duration: 0.080s, episode steps: 21, steps per second: 261, episode reward: 0.968, mean reward: 0.046 [-0.002, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: 0.051 [-0.060, 0.206], loss: 0.000024, mean_absolute_error: 0.485724, mean_q: 0.730691\n",
      " 39542/50000: episode: 905, duration: 0.182s, episode steps: 50, steps per second: 275, episode reward: 0.782, mean reward: 0.016 [-0.007, 1.000], mean action: 0.880 [0.000, 2.000], mean observation: 0.155 [-0.190, 0.742], loss: 0.000035, mean_absolute_error: 0.475809, mean_q: 0.713197\n",
      " 39582/50000: episode: 906, duration: 0.148s, episode steps: 40, steps per second: 269, episode reward: 0.854, mean reward: 0.021 [-0.006, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.123 [-0.575, 0.180], loss: 0.000024, mean_absolute_error: 0.475613, mean_q: 0.714325\n",
      " 39615/50000: episode: 907, duration: 0.115s, episode steps: 33, steps per second: 286, episode reward: 0.917, mean reward: 0.028 [-0.004, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.081 [-0.388, 0.130], loss: 0.000140, mean_absolute_error: 0.482352, mean_q: 0.723818\n",
      " 39616/50000: episode: 908, duration: 0.008s, episode steps: 1, steps per second: 122, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.013 [0.010, 0.017], loss: 0.000025, mean_absolute_error: 0.480190, mean_q: 0.714669\n",
      " 39652/50000: episode: 909, duration: 0.145s, episode steps: 36, steps per second: 248, episode reward: 0.881, mean reward: 0.024 [-0.005, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.104 [-0.200, 0.530], loss: 0.000154, mean_absolute_error: 0.478664, mean_q: 0.719352\n",
      " 39712/50000: episode: 910, duration: 0.231s, episode steps: 60, steps per second: 260, episode reward: 0.771, mean reward: 0.013 [-0.007, 1.000], mean action: 0.967 [0.000, 2.000], mean observation: 0.140 [-0.180, 0.723], loss: 0.000070, mean_absolute_error: 0.476340, mean_q: 0.713749\n",
      " 39764/50000: episode: 911, duration: 0.172s, episode steps: 52, steps per second: 303, episode reward: 0.740, mean reward: 0.014 [-0.008, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.179 [-0.210, 0.847], loss: 0.000026, mean_absolute_error: 0.479461, mean_q: 0.719633\n",
      " 39790/50000: episode: 912, duration: 0.107s, episode steps: 26, steps per second: 243, episode reward: 0.956, mean reward: 0.037 [-0.003, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.040 [-0.120, 0.270], loss: 0.000351, mean_absolute_error: 0.478834, mean_q: 0.718602\n",
      " 39813/50000: episode: 913, duration: 0.083s, episode steps: 23, steps per second: 277, episode reward: 0.951, mean reward: 0.041 [-0.003, 1.000], mean action: 1.391 [0.000, 2.000], mean observation: -0.060 [-0.305, 0.150], loss: 0.000036, mean_absolute_error: 0.473003, mean_q: 0.709979\n",
      " 39854/50000: episode: 914, duration: 0.140s, episode steps: 41, steps per second: 292, episode reward: 0.889, mean reward: 0.022 [-0.005, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.093 [-0.140, 0.454], loss: 0.000022, mean_absolute_error: 0.478386, mean_q: 0.718600\n",
      " 39862/50000: episode: 915, duration: 0.028s, episode steps: 8, steps per second: 290, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.043 [-0.040, 0.115], loss: 0.001012, mean_absolute_error: 0.485231, mean_q: 0.723694\n",
      " 39896/50000: episode: 916, duration: 0.132s, episode steps: 34, steps per second: 258, episode reward: 0.910, mean reward: 0.027 [-0.004, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.084 [-0.130, 0.420], loss: 0.000060, mean_absolute_error: 0.478114, mean_q: 0.717276\n",
      " 39900/50000: episode: 917, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: 0.997, mean reward: 0.249 [-0.001, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.042 [-0.020, 0.104], loss: 0.000030, mean_absolute_error: 0.465815, mean_q: 0.698313\n",
      " 39965/50000: episode: 918, duration: 0.248s, episode steps: 65, steps per second: 262, episode reward: 0.670, mean reward: 0.010 [-0.010, 1.000], mean action: 0.969 [0.000, 2.000], mean observation: 0.188 [-0.240, 0.970], loss: 0.000210, mean_absolute_error: 0.481848, mean_q: 0.722364\n",
      " 39992/50000: episode: 919, duration: 0.082s, episode steps: 27, steps per second: 331, episode reward: 0.947, mean reward: 0.035 [-0.003, 1.000], mean action: 0.815 [0.000, 2.000], mean observation: 0.064 [-0.090, 0.287], loss: 0.000023, mean_absolute_error: 0.475722, mean_q: 0.715904\n",
      " 40055/50000: episode: 920, duration: 0.195s, episode steps: 63, steps per second: 323, episode reward: 0.655, mean reward: 0.010 [-0.010, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.196 [-0.984, 0.240], loss: 0.000023, mean_absolute_error: 0.479114, mean_q: 0.719117\n",
      " 40105/50000: episode: 921, duration: 0.185s, episode steps: 50, steps per second: 271, episode reward: 0.759, mean reward: 0.015 [-0.008, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.170 [-0.801, 0.240], loss: 0.000021, mean_absolute_error: 0.477949, mean_q: 0.715871\n",
      " 40147/50000: episode: 922, duration: 0.153s, episode steps: 42, steps per second: 275, episode reward: 0.860, mean reward: 0.020 [-0.006, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.108 [-0.170, 0.555], loss: 0.000028, mean_absolute_error: 0.481814, mean_q: 0.723499\n",
      " 40167/50000: episode: 923, duration: 0.073s, episode steps: 20, steps per second: 274, episode reward: 0.968, mean reward: 0.048 [-0.002, 1.000], mean action: 1.350 [0.000, 2.000], mean observation: -0.053 [-0.216, 0.090], loss: 0.000023, mean_absolute_error: 0.480699, mean_q: 0.721124\n",
      " 40207/50000: episode: 924, duration: 0.130s, episode steps: 40, steps per second: 307, episode reward: 0.896, mean reward: 0.022 [-0.005, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.076 [-0.452, 0.150], loss: 0.000023, mean_absolute_error: 0.484117, mean_q: 0.727864\n",
      " 40208/50000: episode: 925, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.005 [-0.000, 0.010], loss: 0.000018, mean_absolute_error: 0.508821, mean_q: 0.770252\n",
      " 40229/50000: episode: 926, duration: 0.073s, episode steps: 21, steps per second: 287, episode reward: 0.970, mean reward: 0.046 [-0.002, 1.000], mean action: 0.762 [0.000, 2.000], mean observation: 0.051 [-0.060, 0.195], loss: 0.000021, mean_absolute_error: 0.478875, mean_q: 0.718266\n",
      " 40285/50000: episode: 927, duration: 0.209s, episode steps: 56, steps per second: 268, episode reward: 0.706, mean reward: 0.013 [-0.009, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.191 [-0.899, 0.220], loss: 0.000024, mean_absolute_error: 0.480231, mean_q: 0.719922\n",
      " 40321/50000: episode: 928, duration: 0.136s, episode steps: 36, steps per second: 265, episode reward: 0.895, mean reward: 0.025 [-0.005, 1.000], mean action: 0.861 [0.000, 2.000], mean observation: 0.096 [-0.160, 0.472], loss: 0.000019, mean_absolute_error: 0.486243, mean_q: 0.730041\n",
      " 40348/50000: episode: 929, duration: 0.103s, episode steps: 27, steps per second: 261, episode reward: 0.945, mean reward: 0.035 [-0.003, 1.000], mean action: 0.741 [0.000, 2.000], mean observation: 0.064 [-0.100, 0.299], loss: 0.000326, mean_absolute_error: 0.477627, mean_q: 0.713463\n",
      " 40398/50000: episode: 930, duration: 0.205s, episode steps: 50, steps per second: 244, episode reward: 0.810, mean reward: 0.016 [-0.007, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.114 [-0.711, 0.220], loss: 0.000058, mean_absolute_error: 0.479687, mean_q: 0.717326\n",
      " 40453/50000: episode: 931, duration: 0.239s, episode steps: 55, steps per second: 230, episode reward: 0.781, mean reward: 0.014 [-0.007, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.118 [-0.747, 0.240], loss: 0.000169, mean_absolute_error: 0.482473, mean_q: 0.723454\n",
      " 40485/50000: episode: 932, duration: 0.122s, episode steps: 32, steps per second: 261, episode reward: 0.927, mean reward: 0.029 [-0.004, 1.000], mean action: 0.906 [0.000, 2.000], mean observation: 0.075 [-0.110, 0.358], loss: 0.000016, mean_absolute_error: 0.481152, mean_q: 0.721079\n",
      " 40528/50000: episode: 933, duration: 0.156s, episode steps: 43, steps per second: 276, episode reward: 0.863, mean reward: 0.020 [-0.005, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.108 [-0.170, 0.549], loss: 0.000023, mean_absolute_error: 0.482398, mean_q: 0.724317\n",
      " 40583/50000: episode: 934, duration: 0.186s, episode steps: 55, steps per second: 296, episode reward: 0.735, mean reward: 0.013 [-0.009, 1.000], mean action: 0.909 [0.000, 2.000], mean observation: 0.172 [-0.230, 0.857], loss: 0.000017, mean_absolute_error: 0.480402, mean_q: 0.719604\n",
      " 40612/50000: episode: 935, duration: 0.122s, episode steps: 29, steps per second: 238, episode reward: 0.927, mean reward: 0.032 [-0.004, 1.000], mean action: 1.276 [0.000, 2.000], mean observation: -0.080 [-0.366, 0.150], loss: 0.000020, mean_absolute_error: 0.480579, mean_q: 0.721411\n",
      " 40680/50000: episode: 936, duration: 0.284s, episode steps: 68, steps per second: 239, episode reward: 0.621, mean reward: 0.009 [-0.009, 1.000], mean action: 0.897 [0.000, 2.000], mean observation: 0.217 [-0.240, 0.950], loss: 0.000017, mean_absolute_error: 0.484016, mean_q: 0.725819\n",
      " 40735/50000: episode: 937, duration: 0.291s, episode steps: 55, steps per second: 189, episode reward: 0.715, mean reward: 0.013 [-0.009, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.180 [-0.907, 0.230], loss: 0.000021, mean_absolute_error: 0.479062, mean_q: 0.719379\n",
      " 40771/50000: episode: 938, duration: 0.168s, episode steps: 36, steps per second: 215, episode reward: 0.891, mean reward: 0.025 [-0.005, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.092 [-0.490, 0.190], loss: 0.000019, mean_absolute_error: 0.484182, mean_q: 0.723945\n",
      " 40825/50000: episode: 939, duration: 0.226s, episode steps: 54, steps per second: 239, episode reward: 0.751, mean reward: 0.014 [-0.008, 1.000], mean action: 0.926 [0.000, 2.000], mean observation: 0.166 [-0.230, 0.810], loss: 0.000018, mean_absolute_error: 0.484177, mean_q: 0.725742\n",
      " 40826/50000: episode: 940, duration: 0.006s, episode steps: 1, steps per second: 162, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.007 [-0.025, 0.010], loss: 0.000015, mean_absolute_error: 0.501723, mean_q: 0.749717\n",
      " 40889/50000: episode: 941, duration: 0.208s, episode steps: 63, steps per second: 303, episode reward: 0.688, mean reward: 0.011 [-0.009, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.182 [-0.240, 0.939], loss: 0.000026, mean_absolute_error: 0.485519, mean_q: 0.726864\n",
      " 40937/50000: episode: 942, duration: 0.197s, episode steps: 48, steps per second: 244, episode reward: 0.804, mean reward: 0.017 [-0.007, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.141 [-0.190, 0.693], loss: 0.000324, mean_absolute_error: 0.483116, mean_q: 0.719479\n",
      " 41001/50000: episode: 943, duration: 0.237s, episode steps: 64, steps per second: 270, episode reward: 0.648, mean reward: 0.010 [-0.010, 1.000], mean action: 0.891 [0.000, 2.000], mean observation: 0.207 [-0.250, 0.977], loss: 0.000023, mean_absolute_error: 0.481043, mean_q: 0.722426\n",
      " 41035/50000: episode: 944, duration: 0.098s, episode steps: 34, steps per second: 346, episode reward: 0.939, mean reward: 0.028 [-0.003, 1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.034 [-0.336, 0.180], loss: 0.000024, mean_absolute_error: 0.484432, mean_q: 0.726473\n",
      " 41063/50000: episode: 945, duration: 0.082s, episode steps: 28, steps per second: 339, episode reward: 0.926, mean reward: 0.033 [-0.004, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.081 [-0.388, 0.170], loss: 0.000047, mean_absolute_error: 0.474279, mean_q: 0.708049\n",
      " 41127/50000: episode: 946, duration: 0.260s, episode steps: 64, steps per second: 246, episode reward: 0.703, mean reward: 0.011 [-0.009, 1.000], mean action: 0.969 [0.000, 2.000], mean observation: 0.170 [-0.230, 0.902], loss: 0.000023, mean_absolute_error: 0.481852, mean_q: 0.721471\n",
      " 41150/50000: episode: 947, duration: 0.070s, episode steps: 23, steps per second: 330, episode reward: 0.954, mean reward: 0.041 [-0.003, 1.000], mean action: 1.348 [0.000, 2.000], mean observation: -0.060 [-0.285, 0.140], loss: 0.000023, mean_absolute_error: 0.480580, mean_q: 0.716466\n",
      " 41151/50000: episode: 948, duration: 0.006s, episode steps: 1, steps per second: 165, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [-0.010, 0.098], loss: 0.000071, mean_absolute_error: 0.412736, mean_q: 0.592086\n",
      " 41189/50000: episode: 949, duration: 0.130s, episode steps: 38, steps per second: 292, episode reward: 0.864, mean reward: 0.023 [-0.006, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.120 [-0.559, 0.200], loss: 0.000032, mean_absolute_error: 0.488982, mean_q: 0.732384\n",
      " 41240/50000: episode: 950, duration: 0.217s, episode steps: 51, steps per second: 235, episode reward: 0.815, mean reward: 0.016 [-0.007, 1.000], mean action: 0.941 [0.000, 2.000], mean observation: 0.128 [-0.200, 0.653], loss: 0.000020, mean_absolute_error: 0.483241, mean_q: 0.722219\n",
      " 41287/50000: episode: 951, duration: 0.193s, episode steps: 47, steps per second: 244, episode reward: 0.845, mean reward: 0.018 [-0.006, 1.000], mean action: 0.936 [0.000, 2.000], mean observation: 0.115 [-0.150, 0.574], loss: 0.000022, mean_absolute_error: 0.481751, mean_q: 0.721673\n",
      " 41308/50000: episode: 952, duration: 0.087s, episode steps: 21, steps per second: 240, episode reward: 0.967, mean reward: 0.046 [-0.002, 1.000], mean action: 0.762 [0.000, 2.000], mean observation: 0.053 [-0.070, 0.211], loss: 0.000014, mean_absolute_error: 0.487478, mean_q: 0.731271\n",
      " 41346/50000: episode: 953, duration: 0.157s, episode steps: 38, steps per second: 242, episode reward: 0.876, mean reward: 0.023 [-0.005, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.107 [-0.170, 0.530], loss: 0.000390, mean_absolute_error: 0.485440, mean_q: 0.724746\n",
      " 41389/50000: episode: 954, duration: 0.193s, episode steps: 43, steps per second: 223, episode reward: 0.862, mean reward: 0.020 [-0.005, 1.000], mean action: 0.884 [0.000, 2.000], mean observation: 0.109 [-0.180, 0.548], loss: 0.000023, mean_absolute_error: 0.484209, mean_q: 0.725414\n",
      " 41449/50000: episode: 955, duration: 0.191s, episode steps: 60, steps per second: 314, episode reward: 0.693, mean reward: 0.012 [-0.009, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.176 [-0.940, 0.230], loss: 0.000023, mean_absolute_error: 0.478314, mean_q: 0.714320\n",
      " 41489/50000: episode: 956, duration: 0.156s, episode steps: 40, steps per second: 257, episode reward: 0.880, mean reward: 0.022 [-0.005, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.103 [-0.140, 0.481], loss: 0.000018, mean_absolute_error: 0.487478, mean_q: 0.731277\n",
      " 41490/50000: episode: 957, duration: 0.008s, episode steps: 1, steps per second: 128, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.012 [0.000, 0.024], loss: 0.000033, mean_absolute_error: 0.454540, mean_q: 0.663033\n",
      " 41542/50000: episode: 958, duration: 0.171s, episode steps: 52, steps per second: 304, episode reward: 0.814, mean reward: 0.016 [-0.006, 1.000], mean action: 0.865 [0.000, 2.000], mean observation: 0.129 [-0.150, 0.620], loss: 0.000318, mean_absolute_error: 0.480248, mean_q: 0.718270\n",
      " 41588/50000: episode: 959, duration: 0.171s, episode steps: 46, steps per second: 268, episode reward: 0.835, mean reward: 0.018 [-0.006, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.130 [-0.170, 0.560], loss: 0.000043, mean_absolute_error: 0.487482, mean_q: 0.727790\n",
      " 41632/50000: episode: 960, duration: 0.168s, episode steps: 44, steps per second: 261, episode reward: 0.835, mean reward: 0.019 [-0.006, 1.000], mean action: 0.841 [0.000, 2.000], mean observation: 0.128 [-0.180, 0.629], loss: 0.000089, mean_absolute_error: 0.481251, mean_q: 0.720922\n",
      " 41639/50000: episode: 961, duration: 0.029s, episode steps: 7, steps per second: 241, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.429 [0.000, 2.000], mean observation: 0.039 [-0.040, 0.118], loss: 0.000040, mean_absolute_error: 0.491352, mean_q: 0.735099\n",
      " 41698/50000: episode: 962, duration: 0.273s, episode steps: 59, steps per second: 216, episode reward: 0.715, mean reward: 0.012 [-0.009, 1.000], mean action: 0.915 [0.000, 2.000], mean observation: 0.176 [-0.220, 0.885], loss: 0.000041, mean_absolute_error: 0.488460, mean_q: 0.731268\n",
      " 41748/50000: episode: 963, duration: 0.395s, episode steps: 50, steps per second: 127, episode reward: 0.763, mean reward: 0.015 [-0.008, 1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.167 [-0.810, 0.210], loss: 0.000027, mean_absolute_error: 0.486697, mean_q: 0.727932\n",
      " 41814/50000: episode: 964, duration: 0.833s, episode steps: 66, steps per second: 79, episode reward: 0.718, mean reward: 0.011 [-0.008, 1.000], mean action: 0.879 [0.000, 2.000], mean observation: 0.158 [-0.200, 0.846], loss: 0.000131, mean_absolute_error: 0.482242, mean_q: 0.721897\n",
      " 41839/50000: episode: 965, duration: 0.247s, episode steps: 25, steps per second: 101, episode reward: 0.947, mean reward: 0.038 [-0.003, 1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.059 [-0.322, 0.160], loss: 0.000071, mean_absolute_error: 0.483382, mean_q: 0.721347\n",
      " 41904/50000: episode: 966, duration: 0.477s, episode steps: 65, steps per second: 136, episode reward: 0.700, mean reward: 0.011 [-0.009, 1.000], mean action: 0.908 [0.000, 2.000], mean observation: 0.170 [-0.240, 0.886], loss: 0.000095, mean_absolute_error: 0.487351, mean_q: 0.729899\n",
      " 41963/50000: episode: 967, duration: 0.445s, episode steps: 59, steps per second: 133, episode reward: 0.671, mean reward: 0.011 [-0.010, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.204 [-0.961, 0.230], loss: 0.000163, mean_absolute_error: 0.481294, mean_q: 0.720407\n",
      " 41964/50000: episode: 968, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.039 [-0.089, 0.010], loss: 0.000055, mean_absolute_error: 0.470568, mean_q: 0.710705\n",
      " 41995/50000: episode: 969, duration: 0.153s, episode steps: 31, steps per second: 203, episode reward: 0.919, mean reward: 0.030 [-0.004, 1.000], mean action: 1.258 [0.000, 2.000], mean observation: -0.085 [-0.383, 0.170], loss: 0.000030, mean_absolute_error: 0.487713, mean_q: 0.730874\n",
      " 42036/50000: episode: 970, duration: 0.209s, episode steps: 41, steps per second: 196, episode reward: 0.888, mean reward: 0.022 [-0.005, 1.000], mean action: 0.902 [0.000, 2.000], mean observation: 0.095 [-0.120, 0.451], loss: 0.000021, mean_absolute_error: 0.484358, mean_q: 0.724212\n",
      " 42047/50000: episode: 971, duration: 0.049s, episode steps: 11, steps per second: 222, episode reward: 0.986, mean reward: 0.090 [-0.002, 1.000], mean action: 1.818 [0.000, 2.000], mean observation: -0.038 [-0.158, 0.090], loss: 0.000025, mean_absolute_error: 0.493448, mean_q: 0.742352\n",
      " 42055/50000: episode: 972, duration: 0.032s, episode steps: 8, steps per second: 247, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 1.750 [0.000, 2.000], mean observation: -0.037 [-0.131, 0.070], loss: 0.000027, mean_absolute_error: 0.495424, mean_q: 0.739334\n",
      " 42093/50000: episode: 973, duration: 0.112s, episode steps: 38, steps per second: 338, episode reward: 0.899, mean reward: 0.024 [-0.004, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.088 [-0.120, 0.418], loss: 0.000144, mean_absolute_error: 0.482959, mean_q: 0.718704\n",
      " 42154/50000: episode: 974, duration: 0.246s, episode steps: 61, steps per second: 248, episode reward: 0.705, mean reward: 0.012 [-0.009, 1.000], mean action: 1.131 [0.000, 2.000], mean observation: -0.178 [-0.875, 0.200], loss: 0.000067, mean_absolute_error: 0.484368, mean_q: 0.721993\n",
      " 42204/50000: episode: 975, duration: 0.149s, episode steps: 50, steps per second: 335, episode reward: 0.776, mean reward: 0.016 [-0.007, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.159 [-0.745, 0.230], loss: 0.000026, mean_absolute_error: 0.489566, mean_q: 0.732202\n",
      " 42227/50000: episode: 976, duration: 0.083s, episode steps: 23, steps per second: 278, episode reward: 0.959, mean reward: 0.042 [-0.003, 1.000], mean action: 1.348 [0.000, 2.000], mean observation: -0.056 [-0.259, 0.110], loss: 0.000024, mean_absolute_error: 0.484871, mean_q: 0.725454\n",
      " 42228/50000: episode: 977, duration: 0.006s, episode steps: 1, steps per second: 164, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.040 [-0.010, 0.090], loss: 0.000044, mean_absolute_error: 0.472987, mean_q: 0.713412\n",
      " 42254/50000: episode: 978, duration: 0.094s, episode steps: 26, steps per second: 276, episode reward: 0.956, mean reward: 0.037 [-0.002, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.059 [-0.070, 0.240], loss: 0.000035, mean_absolute_error: 0.487545, mean_q: 0.729099\n",
      " 42272/50000: episode: 979, duration: 0.075s, episode steps: 18, steps per second: 240, episode reward: 0.975, mean reward: 0.054 [-0.002, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.049 [-0.070, 0.183], loss: 0.000032, mean_absolute_error: 0.482003, mean_q: 0.720805\n",
      " 42325/50000: episode: 980, duration: 0.194s, episode steps: 53, steps per second: 273, episode reward: 0.783, mean reward: 0.015 [-0.007, 1.000], mean action: 0.906 [0.000, 2.000], mean observation: 0.147 [-0.180, 0.721], loss: 0.000073, mean_absolute_error: 0.485529, mean_q: 0.725989\n",
      " 42376/50000: episode: 981, duration: 0.159s, episode steps: 51, steps per second: 321, episode reward: 0.759, mean reward: 0.015 [-0.008, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.168 [-0.801, 0.210], loss: 0.000023, mean_absolute_error: 0.482714, mean_q: 0.720759\n",
      " 42408/50000: episode: 982, duration: 0.122s, episode steps: 32, steps per second: 263, episode reward: 0.906, mean reward: 0.028 [-0.005, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.091 [-0.455, 0.190], loss: 0.000034, mean_absolute_error: 0.483027, mean_q: 0.721239\n",
      " 42460/50000: episode: 983, duration: 0.188s, episode steps: 52, steps per second: 277, episode reward: 0.732, mean reward: 0.014 [-0.009, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.185 [-0.861, 0.220], loss: 0.000058, mean_absolute_error: 0.487174, mean_q: 0.728917\n",
      " 42492/50000: episode: 984, duration: 0.117s, episode steps: 32, steps per second: 274, episode reward: 0.912, mean reward: 0.028 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.088 [-0.423, 0.170], loss: 0.000021, mean_absolute_error: 0.488677, mean_q: 0.730883\n",
      " 42531/50000: episode: 985, duration: 0.141s, episode steps: 39, steps per second: 277, episode reward: 0.881, mean reward: 0.023 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.108 [-0.451, 0.150], loss: 0.000083, mean_absolute_error: 0.486015, mean_q: 0.725077\n",
      " 42570/50000: episode: 986, duration: 0.141s, episode steps: 39, steps per second: 276, episode reward: 0.862, mean reward: 0.022 [-0.006, 1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.118 [-0.566, 0.200], loss: 0.000037, mean_absolute_error: 0.485800, mean_q: 0.729613\n",
      " 42579/50000: episode: 987, duration: 0.035s, episode steps: 9, steps per second: 257, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.039 [-0.050, 0.133], loss: 0.000036, mean_absolute_error: 0.491352, mean_q: 0.732704\n",
      " 42640/50000: episode: 988, duration: 0.220s, episode steps: 61, steps per second: 278, episode reward: 0.644, mean reward: 0.011 [-0.010, 1.000], mean action: 1.131 [0.000, 2.000], mean observation: -0.219 [-0.993, 0.220], loss: 0.000030, mean_absolute_error: 0.485153, mean_q: 0.722970\n",
      " 42641/50000: episode: 989, duration: 0.008s, episode steps: 1, steps per second: 130, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.011 [-0.032, 0.010], loss: 0.000050, mean_absolute_error: 0.467245, mean_q: 0.694878\n",
      " 42677/50000: episode: 990, duration: 0.143s, episode steps: 36, steps per second: 252, episode reward: 0.897, mean reward: 0.025 [-0.005, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.083 [-0.483, 0.180], loss: 0.000040, mean_absolute_error: 0.481427, mean_q: 0.717319\n",
      " 42723/50000: episode: 991, duration: 0.175s, episode steps: 46, steps per second: 263, episode reward: 0.847, mean reward: 0.018 [-0.005, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.119 [-0.150, 0.544], loss: 0.000350, mean_absolute_error: 0.492292, mean_q: 0.734829\n",
      " 42787/50000: episode: 992, duration: 0.219s, episode steps: 64, steps per second: 292, episode reward: 0.648, mean reward: 0.010 [-0.010, 1.000], mean action: 0.922 [0.000, 2.000], mean observation: 0.208 [-0.210, 0.956], loss: 0.000031, mean_absolute_error: 0.491992, mean_q: 0.736891\n",
      " 42814/50000: episode: 993, duration: 0.103s, episode steps: 27, steps per second: 261, episode reward: 0.941, mean reward: 0.035 [-0.003, 1.000], mean action: 0.741 [0.000, 2.000], mean observation: 0.070 [-0.110, 0.321], loss: 0.000033, mean_absolute_error: 0.492759, mean_q: 0.736084\n",
      " 42815/50000: episode: 994, duration: 0.007s, episode steps: 1, steps per second: 141, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [-0.010, 0.049], loss: 0.000021, mean_absolute_error: 0.473048, mean_q: 0.715876\n",
      " 42824/50000: episode: 995, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 0.991, mean reward: 0.110 [-0.001, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.041 [-0.040, 0.124], loss: 0.000026, mean_absolute_error: 0.482190, mean_q: 0.720405\n",
      " 42852/50000: episode: 996, duration: 0.153s, episode steps: 28, steps per second: 183, episode reward: 0.937, mean reward: 0.033 [-0.003, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.070 [-0.338, 0.140], loss: 0.000018, mean_absolute_error: 0.490638, mean_q: 0.737868\n",
      " 42911/50000: episode: 997, duration: 0.213s, episode steps: 59, steps per second: 277, episode reward: 0.747, mean reward: 0.013 [-0.007, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.161 [-0.160, 0.735], loss: 0.000056, mean_absolute_error: 0.490127, mean_q: 0.734838\n",
      " 42970/50000: episode: 998, duration: 0.241s, episode steps: 59, steps per second: 244, episode reward: 0.713, mean reward: 0.012 [-0.009, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.178 [-0.210, 0.883], loss: 0.000021, mean_absolute_error: 0.484943, mean_q: 0.725313\n",
      " 43029/50000: episode: 999, duration: 0.235s, episode steps: 59, steps per second: 251, episode reward: 0.730, mean reward: 0.012 [-0.008, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.170 [-0.210, 0.794], loss: 0.000027, mean_absolute_error: 0.489299, mean_q: 0.731485\n",
      " 43084/50000: episode: 1000, duration: 0.246s, episode steps: 55, steps per second: 224, episode reward: 0.715, mean reward: 0.013 [-0.009, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.188 [-0.889, 0.230], loss: 0.000020, mean_absolute_error: 0.494643, mean_q: 0.739748\n",
      " 43109/50000: episode: 1001, duration: 0.115s, episode steps: 25, steps per second: 218, episode reward: 0.945, mean reward: 0.038 [-0.003, 1.000], mean action: 1.320 [0.000, 2.000], mean observation: -0.066 [-0.322, 0.150], loss: 0.000024, mean_absolute_error: 0.483165, mean_q: 0.719290\n",
      " 43144/50000: episode: 1002, duration: 0.146s, episode steps: 35, steps per second: 239, episode reward: 0.906, mean reward: 0.026 [-0.004, 1.000], mean action: 0.771 [0.000, 2.000], mean observation: 0.090 [-0.120, 0.417], loss: 0.000023, mean_absolute_error: 0.493476, mean_q: 0.740598\n",
      " 43182/50000: episode: 1003, duration: 0.157s, episode steps: 38, steps per second: 243, episode reward: 0.881, mean reward: 0.023 [-0.005, 1.000], mean action: 1.211 [0.000, 2.000], mean observation: -0.109 [-0.464, 0.170], loss: 0.000022, mean_absolute_error: 0.488422, mean_q: 0.729517\n",
      " 43222/50000: episode: 1004, duration: 0.168s, episode steps: 40, steps per second: 238, episode reward: 0.871, mean reward: 0.022 [-0.005, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.115 [-0.480, 0.170], loss: 0.000031, mean_absolute_error: 0.487089, mean_q: 0.727421\n",
      " 43248/50000: episode: 1005, duration: 0.112s, episode steps: 26, steps per second: 231, episode reward: 0.939, mean reward: 0.036 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.071 [-0.344, 0.160], loss: 0.000020, mean_absolute_error: 0.490424, mean_q: 0.734290\n",
      " 43266/50000: episode: 1006, duration: 0.059s, episode steps: 18, steps per second: 304, episode reward: 0.971, mean reward: 0.054 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.050 [-0.219, 0.110], loss: 0.000028, mean_absolute_error: 0.488523, mean_q: 0.728275\n",
      " 43288/50000: episode: 1007, duration: 0.134s, episode steps: 22, steps per second: 165, episode reward: 0.958, mean reward: 0.044 [-0.003, 1.000], mean action: 1.364 [0.000, 2.000], mean observation: -0.056 [-0.277, 0.140], loss: 0.000021, mean_absolute_error: 0.489651, mean_q: 0.729727\n",
      " 43346/50000: episode: 1008, duration: 0.270s, episode steps: 58, steps per second: 215, episode reward: 0.680, mean reward: 0.012 [-0.010, 1.000], mean action: 1.121 [0.000, 2.000], mean observation: -0.202 [-0.971, 0.240], loss: 0.000029, mean_absolute_error: 0.491858, mean_q: 0.736737\n",
      " 43351/50000: episode: 1009, duration: 0.023s, episode steps: 5, steps per second: 215, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.037 [-0.110, 0.050], loss: 0.000014, mean_absolute_error: 0.479959, mean_q: 0.716712\n",
      " 43379/50000: episode: 1010, duration: 0.113s, episode steps: 28, steps per second: 248, episode reward: 0.937, mean reward: 0.033 [-0.003, 1.000], mean action: 0.714 [0.000, 2.000], mean observation: 0.071 [-0.120, 0.339], loss: 0.000018, mean_absolute_error: 0.487649, mean_q: 0.727949\n",
      " 43380/50000: episode: 1011, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.008 [-0.025, 0.010], loss: 0.000007, mean_absolute_error: 0.465021, mean_q: 0.705483\n",
      " 43442/50000: episode: 1012, duration: 0.344s, episode steps: 62, steps per second: 180, episode reward: 0.657, mean reward: 0.011 [-0.010, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.207 [-0.230, 0.970], loss: 0.000024, mean_absolute_error: 0.489449, mean_q: 0.730427\n",
      " 43503/50000: episode: 1013, duration: 0.538s, episode steps: 61, steps per second: 113, episode reward: 0.656, mean reward: 0.011 [-0.010, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.207 [-0.995, 0.230], loss: 0.000490, mean_absolute_error: 0.494505, mean_q: 0.739523\n",
      " 43559/50000: episode: 1014, duration: 0.477s, episode steps: 56, steps per second: 117, episode reward: 0.729, mean reward: 0.013 [-0.008, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.176 [-0.190, 0.839], loss: 0.000027, mean_absolute_error: 0.490114, mean_q: 0.732290\n",
      " 43597/50000: episode: 1015, duration: 0.302s, episode steps: 38, steps per second: 126, episode reward: 0.887, mean reward: 0.023 [-0.005, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.102 [-0.459, 0.170], loss: 0.000028, mean_absolute_error: 0.489405, mean_q: 0.731416\n",
      " 43650/50000: episode: 1016, duration: 0.281s, episode steps: 53, steps per second: 189, episode reward: 0.762, mean reward: 0.014 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.157 [-0.180, 0.781], loss: 0.000330, mean_absolute_error: 0.492155, mean_q: 0.734748\n",
      " 43685/50000: episode: 1017, duration: 0.159s, episode steps: 35, steps per second: 221, episode reward: 0.919, mean reward: 0.026 [-0.004, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.071 [-0.130, 0.383], loss: 0.000036, mean_absolute_error: 0.483546, mean_q: 0.718667\n",
      " 43742/50000: episode: 1018, duration: 0.305s, episode steps: 57, steps per second: 187, episode reward: 0.831, mean reward: 0.015 [-0.006, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.092 [-0.180, 0.610], loss: 0.000091, mean_absolute_error: 0.489562, mean_q: 0.732226\n",
      " 43795/50000: episode: 1019, duration: 0.383s, episode steps: 53, steps per second: 138, episode reward: 0.748, mean reward: 0.014 [-0.008, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.175 [-0.777, 0.190], loss: 0.000023, mean_absolute_error: 0.489516, mean_q: 0.728895\n",
      " 43841/50000: episode: 1020, duration: 0.263s, episode steps: 46, steps per second: 175, episode reward: 0.861, mean reward: 0.019 [-0.005, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.102 [-0.140, 0.510], loss: 0.000031, mean_absolute_error: 0.490108, mean_q: 0.732042\n",
      " 43848/50000: episode: 1021, duration: 0.039s, episode steps: 7, steps per second: 180, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 1.857 [1.000, 2.000], mean observation: -0.039 [-0.125, 0.060], loss: 0.000021, mean_absolute_error: 0.503444, mean_q: 0.758695\n",
      " 43849/50000: episode: 1022, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.041 [-0.092, 0.010], loss: 0.000016, mean_absolute_error: 0.478579, mean_q: 0.725312\n",
      " 43880/50000: episode: 1023, duration: 0.171s, episode steps: 31, steps per second: 181, episode reward: 0.920, mean reward: 0.030 [-0.004, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.081 [-0.398, 0.150], loss: 0.000025, mean_absolute_error: 0.486922, mean_q: 0.724778\n",
      " 43930/50000: episode: 1024, duration: 0.432s, episode steps: 50, steps per second: 116, episode reward: 0.776, mean reward: 0.016 [-0.008, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.158 [-0.761, 0.200], loss: 0.000026, mean_absolute_error: 0.484319, mean_q: 0.721385\n",
      " 43967/50000: episode: 1025, duration: 0.366s, episode steps: 37, steps per second: 101, episode reward: 0.884, mean reward: 0.024 [-0.005, 1.000], mean action: 0.784 [0.000, 2.000], mean observation: 0.106 [-0.160, 0.487], loss: 0.000026, mean_absolute_error: 0.485667, mean_q: 0.725008\n",
      " 44001/50000: episode: 1026, duration: 0.251s, episode steps: 34, steps per second: 136, episode reward: 0.908, mean reward: 0.027 [-0.004, 1.000], mean action: 0.765 [0.000, 2.000], mean observation: 0.091 [-0.130, 0.403], loss: 0.000113, mean_absolute_error: 0.482907, mean_q: 0.720501\n",
      " 44002/50000: episode: 1027, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.014 [0.010, 0.017], loss: 0.000014, mean_absolute_error: 0.513384, mean_q: 0.778022\n",
      " 44020/50000: episode: 1028, duration: 0.139s, episode steps: 18, steps per second: 130, episode reward: 0.971, mean reward: 0.054 [-0.002, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.044 [-0.120, 0.222], loss: 0.000034, mean_absolute_error: 0.491666, mean_q: 0.738023\n",
      " 44021/50000: episode: 1029, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [-0.010, 0.046], loss: 0.000038, mean_absolute_error: 0.476898, mean_q: 0.703657\n",
      " 44076/50000: episode: 1030, duration: 0.425s, episode steps: 55, steps per second: 129, episode reward: 0.721, mean reward: 0.013 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.186 [-0.852, 0.220], loss: 0.000024, mean_absolute_error: 0.493426, mean_q: 0.738584\n",
      " 44118/50000: episode: 1031, duration: 0.301s, episode steps: 42, steps per second: 140, episode reward: 0.864, mean reward: 0.021 [-0.005, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.111 [-0.150, 0.530], loss: 0.000023, mean_absolute_error: 0.486637, mean_q: 0.726725\n",
      " 44176/50000: episode: 1032, duration: 0.365s, episode steps: 58, steps per second: 159, episode reward: 0.705, mean reward: 0.012 [-0.009, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.181 [-0.230, 0.912], loss: 0.000050, mean_absolute_error: 0.495586, mean_q: 0.740803\n",
      " 44183/50000: episode: 1033, duration: 0.066s, episode steps: 7, steps per second: 105, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.070, 0.122], loss: 0.000029, mean_absolute_error: 0.485370, mean_q: 0.731102\n",
      " 44213/50000: episode: 1034, duration: 0.225s, episode steps: 30, steps per second: 133, episode reward: 0.925, mean reward: 0.031 [-0.004, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.079 [-0.387, 0.160], loss: 0.000075, mean_absolute_error: 0.483209, mean_q: 0.721684\n",
      " 44244/50000: episode: 1035, duration: 0.220s, episode steps: 31, steps per second: 141, episode reward: 0.935, mean reward: 0.030 [-0.003, 1.000], mean action: 0.710 [0.000, 2.000], mean observation: 0.069 [-0.130, 0.296], loss: 0.000030, mean_absolute_error: 0.492254, mean_q: 0.735518\n",
      " 44281/50000: episode: 1036, duration: 0.219s, episode steps: 37, steps per second: 169, episode reward: 0.899, mean reward: 0.024 [-0.004, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.087 [-0.150, 0.446], loss: 0.000022, mean_absolute_error: 0.494454, mean_q: 0.739820\n",
      " 44321/50000: episode: 1037, duration: 0.231s, episode steps: 40, steps per second: 173, episode reward: 0.857, mean reward: 0.021 [-0.006, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.119 [-0.583, 0.180], loss: 0.000060, mean_absolute_error: 0.489460, mean_q: 0.729723\n",
      " 44322/50000: episode: 1038, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.002 [-0.014, 0.010], loss: 0.000059, mean_absolute_error: 0.510390, mean_q: 0.770057\n",
      " 44379/50000: episode: 1039, duration: 0.374s, episode steps: 57, steps per second: 152, episode reward: 0.705, mean reward: 0.012 [-0.009, 1.000], mean action: 1.123 [0.000, 2.000], mean observation: -0.189 [-0.900, 0.240], loss: 0.000028, mean_absolute_error: 0.491486, mean_q: 0.735691\n",
      " 44416/50000: episode: 1040, duration: 0.162s, episode steps: 37, steps per second: 228, episode reward: 0.937, mean reward: 0.025 [-0.003, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.035 [-0.140, 0.327], loss: 0.000121, mean_absolute_error: 0.482568, mean_q: 0.720052\n",
      " 44426/50000: episode: 1041, duration: 0.044s, episode steps: 10, steps per second: 229, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 1.600 [1.000, 2.000], mean observation: -0.039 [-0.141, 0.060], loss: 0.000032, mean_absolute_error: 0.486665, mean_q: 0.722456\n",
      " 44476/50000: episode: 1042, duration: 0.237s, episode steps: 50, steps per second: 211, episode reward: 0.757, mean reward: 0.015 [-0.008, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.174 [-0.250, 0.798], loss: 0.000224, mean_absolute_error: 0.486898, mean_q: 0.723379\n",
      " 44477/50000: episode: 1043, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.019 [-0.048, 0.010], loss: 0.000024, mean_absolute_error: 0.454071, mean_q: 0.684964\n",
      " 44519/50000: episode: 1044, duration: 0.190s, episode steps: 42, steps per second: 221, episode reward: 0.834, mean reward: 0.020 [-0.007, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.134 [-0.651, 0.220], loss: 0.000112, mean_absolute_error: 0.493514, mean_q: 0.737718\n",
      " 44562/50000: episode: 1045, duration: 0.252s, episode steps: 43, steps per second: 171, episode reward: 0.845, mean reward: 0.020 [-0.006, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.127 [-0.572, 0.160], loss: 0.000026, mean_absolute_error: 0.489445, mean_q: 0.731130\n",
      " 44577/50000: episode: 1046, duration: 0.110s, episode steps: 15, steps per second: 136, episode reward: 0.981, mean reward: 0.065 [-0.002, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: 0.044 [-0.050, 0.160], loss: 0.000025, mean_absolute_error: 0.486982, mean_q: 0.728250\n",
      " 44603/50000: episode: 1047, duration: 0.171s, episode steps: 26, steps per second: 152, episode reward: 0.941, mean reward: 0.036 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.070 [-0.333, 0.150], loss: 0.000027, mean_absolute_error: 0.480398, mean_q: 0.716468\n",
      " 44663/50000: episode: 1048, duration: 0.269s, episode steps: 60, steps per second: 223, episode reward: 0.655, mean reward: 0.011 [-0.010, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.212 [-0.230, 0.996], loss: 0.000026, mean_absolute_error: 0.487746, mean_q: 0.730011\n",
      " 44713/50000: episode: 1049, duration: 0.241s, episode steps: 50, steps per second: 208, episode reward: 0.783, mean reward: 0.016 [-0.007, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.157 [-0.180, 0.706], loss: 0.000237, mean_absolute_error: 0.492173, mean_q: 0.736153\n",
      " 44757/50000: episode: 1050, duration: 0.193s, episode steps: 44, steps per second: 227, episode reward: 0.824, mean reward: 0.019 [-0.007, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.137 [-0.665, 0.190], loss: 0.000096, mean_absolute_error: 0.478567, mean_q: 0.715320\n",
      " 44809/50000: episode: 1051, duration: 0.229s, episode steps: 52, steps per second: 227, episode reward: 0.741, mean reward: 0.014 [-0.008, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.177 [-0.250, 0.826], loss: 0.000099, mean_absolute_error: 0.482043, mean_q: 0.718486\n",
      " 44838/50000: episode: 1052, duration: 0.125s, episode steps: 29, steps per second: 231, episode reward: 0.937, mean reward: 0.032 [-0.003, 1.000], mean action: 0.724 [0.000, 2.000], mean observation: 0.070 [-0.120, 0.334], loss: 0.000046, mean_absolute_error: 0.488344, mean_q: 0.729019\n",
      " 44892/50000: episode: 1053, duration: 0.200s, episode steps: 54, steps per second: 271, episode reward: 0.757, mean reward: 0.014 [-0.008, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.157 [-0.808, 0.220], loss: 0.000205, mean_absolute_error: 0.492612, mean_q: 0.734703\n",
      " 44912/50000: episode: 1054, duration: 0.084s, episode steps: 20, steps per second: 238, episode reward: 0.968, mean reward: 0.048 [-0.002, 1.000], mean action: 0.550 [0.000, 2.000], mean observation: 0.050 [-0.090, 0.224], loss: 0.000076, mean_absolute_error: 0.488020, mean_q: 0.724902\n",
      " 44943/50000: episode: 1055, duration: 0.131s, episode steps: 31, steps per second: 236, episode reward: 0.921, mean reward: 0.030 [-0.004, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.081 [-0.391, 0.150], loss: 0.000033, mean_absolute_error: 0.494972, mean_q: 0.743801\n",
      " 44977/50000: episode: 1056, duration: 0.132s, episode steps: 34, steps per second: 258, episode reward: 0.921, mean reward: 0.027 [-0.004, 1.000], mean action: 0.765 [0.000, 2.000], mean observation: 0.079 [-0.140, 0.358], loss: 0.000028, mean_absolute_error: 0.492834, mean_q: 0.736640\n",
      " 44994/50000: episode: 1057, duration: 0.080s, episode steps: 17, steps per second: 214, episode reward: 0.973, mean reward: 0.057 [-0.002, 1.000], mean action: 1.529 [0.000, 2.000], mean observation: -0.045 [-0.210, 0.110], loss: 0.000026, mean_absolute_error: 0.490896, mean_q: 0.736585\n",
      " 45029/50000: episode: 1058, duration: 0.151s, episode steps: 35, steps per second: 232, episode reward: 0.904, mean reward: 0.026 [-0.004, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.094 [-0.411, 0.140], loss: 0.000073, mean_absolute_error: 0.484970, mean_q: 0.727792\n",
      " 45056/50000: episode: 1059, duration: 0.132s, episode steps: 27, steps per second: 205, episode reward: 0.936, mean reward: 0.035 [-0.004, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.070 [-0.355, 0.150], loss: 0.000029, mean_absolute_error: 0.487318, mean_q: 0.729715\n",
      " 45117/50000: episode: 1060, duration: 0.264s, episode steps: 61, steps per second: 231, episode reward: 0.695, mean reward: 0.011 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.184 [-0.210, 0.868], loss: 0.000046, mean_absolute_error: 0.481088, mean_q: 0.717445\n",
      " 45123/50000: episode: 1061, duration: 0.026s, episode steps: 6, steps per second: 229, episode reward: 0.994, mean reward: 0.166 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.037 [-0.116, 0.060], loss: 0.000029, mean_absolute_error: 0.504185, mean_q: 0.754359\n",
      " 45124/50000: episode: 1062, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.010 [-0.030, 0.010], loss: 0.000036, mean_absolute_error: 0.498931, mean_q: 0.755726\n",
      " 45170/50000: episode: 1063, duration: 0.198s, episode steps: 46, steps per second: 233, episode reward: 0.804, mean reward: 0.017 [-0.007, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.148 [-0.706, 0.210], loss: 0.000025, mean_absolute_error: 0.489518, mean_q: 0.734634\n",
      " 45217/50000: episode: 1064, duration: 0.215s, episode steps: 47, steps per second: 219, episode reward: 0.802, mean reward: 0.017 [-0.007, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.143 [-0.711, 0.200], loss: 0.000331, mean_absolute_error: 0.493012, mean_q: 0.735876\n",
      " 45252/50000: episode: 1065, duration: 0.128s, episode steps: 35, steps per second: 272, episode reward: 0.898, mean reward: 0.026 [-0.005, 1.000], mean action: 1.229 [0.000, 2.000], mean observation: -0.095 [-0.456, 0.150], loss: 0.000137, mean_absolute_error: 0.495403, mean_q: 0.740578\n",
      " 45302/50000: episode: 1066, duration: 0.226s, episode steps: 50, steps per second: 221, episode reward: 0.777, mean reward: 0.016 [-0.008, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.158 [-0.756, 0.200], loss: 0.000038, mean_absolute_error: 0.485132, mean_q: 0.725044\n",
      " 45372/50000: episode: 1067, duration: 0.234s, episode steps: 70, steps per second: 300, episode reward: 0.636, mean reward: 0.009 [-0.009, 1.000], mean action: 0.886 [0.000, 2.000], mean observation: 0.202 [-0.190, 0.917], loss: 0.000359, mean_absolute_error: 0.487849, mean_q: 0.725518\n",
      " 45424/50000: episode: 1068, duration: 0.208s, episode steps: 52, steps per second: 250, episode reward: 0.776, mean reward: 0.015 [-0.007, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.157 [-0.707, 0.220], loss: 0.000091, mean_absolute_error: 0.488358, mean_q: 0.727714\n",
      " 45485/50000: episode: 1069, duration: 0.249s, episode steps: 61, steps per second: 245, episode reward: 0.695, mean reward: 0.011 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.179 [-0.240, 0.923], loss: 0.000029, mean_absolute_error: 0.480922, mean_q: 0.716658\n",
      " 45532/50000: episode: 1070, duration: 0.196s, episode steps: 47, steps per second: 239, episode reward: 0.810, mean reward: 0.017 [-0.007, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.141 [-0.674, 0.190], loss: 0.000029, mean_absolute_error: 0.492247, mean_q: 0.737110\n",
      " 45574/50000: episode: 1071, duration: 0.179s, episode steps: 42, steps per second: 235, episode reward: 0.842, mean reward: 0.020 [-0.006, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.126 [-0.619, 0.180], loss: 0.000036, mean_absolute_error: 0.485855, mean_q: 0.727718\n",
      " 45638/50000: episode: 1072, duration: 0.248s, episode steps: 64, steps per second: 258, episode reward: 0.632, mean reward: 0.010 [-0.010, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.218 [-0.983, 0.230], loss: 0.000028, mean_absolute_error: 0.486627, mean_q: 0.726632\n",
      " 45698/50000: episode: 1073, duration: 0.259s, episode steps: 60, steps per second: 231, episode reward: 0.662, mean reward: 0.011 [-0.010, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.208 [-0.980, 0.230], loss: 0.000038, mean_absolute_error: 0.492057, mean_q: 0.733410\n",
      " 45723/50000: episode: 1074, duration: 0.116s, episode steps: 25, steps per second: 216, episode reward: 0.958, mean reward: 0.038 [-0.003, 1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.044 [-0.262, 0.120], loss: 0.000032, mean_absolute_error: 0.487876, mean_q: 0.724426\n",
      " 45782/50000: episode: 1075, duration: 0.236s, episode steps: 59, steps per second: 250, episode reward: 0.678, mean reward: 0.011 [-0.009, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.204 [-0.230, 0.918], loss: 0.000026, mean_absolute_error: 0.490230, mean_q: 0.730869\n",
      " 45807/50000: episode: 1076, duration: 0.109s, episode steps: 25, steps per second: 230, episode reward: 0.945, mean reward: 0.038 [-0.003, 1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.066 [-0.324, 0.150], loss: 0.000034, mean_absolute_error: 0.492895, mean_q: 0.736200\n",
      " 45808/50000: episode: 1077, duration: 0.006s, episode steps: 1, steps per second: 182, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.005 [-0.010, 0.000], loss: 0.000023, mean_absolute_error: 0.521407, mean_q: 0.787594\n",
      " 45809/50000: episode: 1078, duration: 0.006s, episode steps: 1, steps per second: 162, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [-0.010, 0.045], loss: 0.000009, mean_absolute_error: 0.495191, mean_q: 0.750710\n",
      " 45874/50000: episode: 1079, duration: 0.233s, episode steps: 65, steps per second: 279, episode reward: 0.706, mean reward: 0.011 [-0.009, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.155 [-0.250, 0.915], loss: 0.000047, mean_absolute_error: 0.488772, mean_q: 0.726526\n",
      " 45918/50000: episode: 1080, duration: 0.162s, episode steps: 44, steps per second: 272, episode reward: 0.826, mean reward: 0.019 [-0.007, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.136 [-0.651, 0.170], loss: 0.000030, mean_absolute_error: 0.482033, mean_q: 0.717190\n",
      " 45919/50000: episode: 1081, duration: 0.006s, episode steps: 1, steps per second: 169, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.008 [-0.017, 0.000], loss: 0.000007, mean_absolute_error: 0.492533, mean_q: 0.712030\n",
      " 45988/50000: episode: 1082, duration: 0.256s, episode steps: 69, steps per second: 269, episode reward: 0.655, mean reward: 0.009 [-0.010, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.182 [-0.240, 0.960], loss: 0.000030, mean_absolute_error: 0.488048, mean_q: 0.727269\n",
      " 46030/50000: episode: 1083, duration: 0.148s, episode steps: 42, steps per second: 284, episode reward: 0.866, mean reward: 0.021 [-0.005, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.109 [-0.160, 0.534], loss: 0.000034, mean_absolute_error: 0.490232, mean_q: 0.732014\n",
      " 46085/50000: episode: 1084, duration: 0.207s, episode steps: 55, steps per second: 265, episode reward: 0.711, mean reward: 0.013 [-0.009, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.190 [-0.909, 0.250], loss: 0.000025, mean_absolute_error: 0.484435, mean_q: 0.720678\n",
      " 46135/50000: episode: 1085, duration: 0.175s, episode steps: 50, steps per second: 285, episode reward: 0.870, mean reward: 0.017 [-0.005, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.077 [-0.130, 0.508], loss: 0.000035, mean_absolute_error: 0.488428, mean_q: 0.725426\n",
      " 46201/50000: episode: 1086, duration: 0.251s, episode steps: 66, steps per second: 262, episode reward: 0.714, mean reward: 0.011 [-0.009, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.139 [-0.250, 0.889], loss: 0.000053, mean_absolute_error: 0.490044, mean_q: 0.731879\n",
      " 46264/50000: episode: 1087, duration: 0.187s, episode steps: 63, steps per second: 337, episode reward: 0.724, mean reward: 0.011 [-0.008, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.154 [-0.220, 0.832], loss: 0.000032, mean_absolute_error: 0.486927, mean_q: 0.726201\n",
      " 46285/50000: episode: 1088, duration: 0.083s, episode steps: 21, steps per second: 253, episode reward: 0.961, mean reward: 0.046 [-0.003, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.056 [-0.263, 0.120], loss: 0.000025, mean_absolute_error: 0.495287, mean_q: 0.738731\n",
      " 46337/50000: episode: 1089, duration: 0.214s, episode steps: 52, steps per second: 243, episode reward: 0.746, mean reward: 0.014 [-0.008, 1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.177 [-0.800, 0.220], loss: 0.000023, mean_absolute_error: 0.486785, mean_q: 0.727195\n",
      " 46338/50000: episode: 1090, duration: 0.008s, episode steps: 1, steps per second: 130, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [-0.010, 0.023], loss: 0.000036, mean_absolute_error: 0.524171, mean_q: 0.780412\n",
      " 46385/50000: episode: 1091, duration: 0.186s, episode steps: 47, steps per second: 253, episode reward: 0.823, mean reward: 0.018 [-0.006, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.128 [-0.642, 0.180], loss: 0.000161, mean_absolute_error: 0.490121, mean_q: 0.729605\n",
      " 46394/50000: episode: 1092, duration: 0.046s, episode steps: 9, steps per second: 195, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.140, 0.090], loss: 0.000125, mean_absolute_error: 0.496178, mean_q: 0.741109\n",
      " 46442/50000: episode: 1093, duration: 0.187s, episode steps: 48, steps per second: 257, episode reward: 0.856, mean reward: 0.018 [-0.006, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.089 [-0.160, 0.562], loss: 0.000046, mean_absolute_error: 0.490121, mean_q: 0.729431\n",
      " 46476/50000: episode: 1094, duration: 0.141s, episode steps: 34, steps per second: 241, episode reward: 0.920, mean reward: 0.027 [-0.004, 1.000], mean action: 0.794 [0.000, 2.000], mean observation: 0.078 [-0.130, 0.369], loss: 0.000167, mean_absolute_error: 0.489062, mean_q: 0.728133\n",
      " 46483/50000: episode: 1095, duration: 0.030s, episode steps: 7, steps per second: 232, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 1.714 [1.000, 2.000], mean observation: -0.039 [-0.117, 0.050], loss: 0.000022, mean_absolute_error: 0.490821, mean_q: 0.726994\n",
      " 46558/50000: episode: 1096, duration: 0.296s, episode steps: 75, steps per second: 253, episode reward: 0.631, mean reward: 0.008 [-0.010, 1.000], mean action: 0.880 [0.000, 2.000], mean observation: 0.167 [-0.260, 0.999], loss: 0.000081, mean_absolute_error: 0.490482, mean_q: 0.732583\n",
      " 46611/50000: episode: 1097, duration: 0.185s, episode steps: 53, steps per second: 287, episode reward: 0.746, mean reward: 0.014 [-0.008, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.174 [-0.797, 0.200], loss: 0.000034, mean_absolute_error: 0.487696, mean_q: 0.726573\n",
      " 46671/50000: episode: 1098, duration: 0.215s, episode steps: 60, steps per second: 280, episode reward: 0.663, mean reward: 0.011 [-0.010, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.205 [-0.994, 0.240], loss: 0.000029, mean_absolute_error: 0.492261, mean_q: 0.731197\n",
      " 46700/50000: episode: 1099, duration: 0.117s, episode steps: 29, steps per second: 248, episode reward: 0.937, mean reward: 0.032 [-0.003, 1.000], mean action: 0.690 [0.000, 2.000], mean observation: 0.071 [-0.100, 0.328], loss: 0.000029, mean_absolute_error: 0.487725, mean_q: 0.728570\n",
      " 46753/50000: episode: 1100, duration: 0.179s, episode steps: 53, steps per second: 297, episode reward: 0.766, mean reward: 0.014 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.157 [-0.210, 0.776], loss: 0.000029, mean_absolute_error: 0.495085, mean_q: 0.738297\n",
      " 46788/50000: episode: 1101, duration: 0.142s, episode steps: 35, steps per second: 246, episode reward: 0.903, mean reward: 0.026 [-0.004, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.091 [-0.150, 0.446], loss: 0.000033, mean_absolute_error: 0.490847, mean_q: 0.732035\n",
      " 46848/50000: episode: 1102, duration: 0.216s, episode steps: 60, steps per second: 278, episode reward: 0.704, mean reward: 0.012 [-0.009, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.177 [-0.230, 0.884], loss: 0.000032, mean_absolute_error: 0.487888, mean_q: 0.726228\n",
      " 46910/50000: episode: 1103, duration: 0.250s, episode steps: 62, steps per second: 248, episode reward: 0.741, mean reward: 0.012 [-0.008, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.143 [-0.210, 0.800], loss: 0.000056, mean_absolute_error: 0.489016, mean_q: 0.731170\n",
      " 46944/50000: episode: 1104, duration: 0.130s, episode steps: 34, steps per second: 262, episode reward: 0.912, mean reward: 0.027 [-0.004, 1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.084 [-0.412, 0.150], loss: 0.000026, mean_absolute_error: 0.489744, mean_q: 0.729007\n",
      " 46992/50000: episode: 1105, duration: 0.194s, episode steps: 48, steps per second: 247, episode reward: 0.794, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.153 [-0.704, 0.190], loss: 0.000023, mean_absolute_error: 0.491638, mean_q: 0.733747\n",
      " 47033/50000: episode: 1106, duration: 0.150s, episode steps: 41, steps per second: 273, episode reward: 0.856, mean reward: 0.021 [-0.006, 1.000], mean action: 0.805 [0.000, 2.000], mean observation: 0.118 [-0.180, 0.573], loss: 0.000024, mean_absolute_error: 0.487526, mean_q: 0.728759\n",
      " 47034/50000: episode: 1107, duration: 0.006s, episode steps: 1, steps per second: 169, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.034 [-0.079, 0.010], loss: 0.000033, mean_absolute_error: 0.487898, mean_q: 0.722201\n",
      " 47085/50000: episode: 1108, duration: 0.192s, episode steps: 51, steps per second: 265, episode reward: 0.751, mean reward: 0.015 [-0.008, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.173 [-0.827, 0.240], loss: 0.000201, mean_absolute_error: 0.489484, mean_q: 0.729846\n",
      " 47097/50000: episode: 1109, duration: 0.045s, episode steps: 12, steps per second: 268, episode reward: 0.985, mean reward: 0.082 [-0.002, 1.000], mean action: 0.250 [0.000, 2.000], mean observation: 0.038 [-0.090, 0.159], loss: 0.000041, mean_absolute_error: 0.485439, mean_q: 0.725743\n",
      " 47142/50000: episode: 1110, duration: 0.172s, episode steps: 45, steps per second: 262, episode reward: 0.899, mean reward: 0.020 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.054 [-0.150, 0.452], loss: 0.000042, mean_absolute_error: 0.494097, mean_q: 0.736865\n",
      " 47157/50000: episode: 1111, duration: 0.054s, episode steps: 15, steps per second: 280, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 1.467 [0.000, 2.000], mean observation: -0.043 [-0.180, 0.080], loss: 0.000040, mean_absolute_error: 0.479195, mean_q: 0.710696\n",
      " 47158/50000: episode: 1112, duration: 0.008s, episode steps: 1, steps per second: 130, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.000 [-0.010, 0.010], loss: 0.000021, mean_absolute_error: 0.459556, mean_q: 0.673199\n",
      " 47206/50000: episode: 1113, duration: 0.185s, episode steps: 48, steps per second: 260, episode reward: 0.812, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.137 [-0.657, 0.180], loss: 0.000028, mean_absolute_error: 0.493492, mean_q: 0.737009\n",
      " 47273/50000: episode: 1114, duration: 0.277s, episode steps: 67, steps per second: 242, episode reward: 0.676, mean reward: 0.010 [-0.010, 1.000], mean action: 0.866 [0.000, 2.000], mean observation: 0.167 [-0.250, 0.960], loss: 0.000112, mean_absolute_error: 0.492964, mean_q: 0.738582\n",
      " 47303/50000: episode: 1115, duration: 0.118s, episode steps: 30, steps per second: 253, episode reward: 0.934, mean reward: 0.031 [-0.003, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.076 [-0.308, 0.130], loss: 0.000046, mean_absolute_error: 0.488130, mean_q: 0.729404\n",
      " 47335/50000: episode: 1116, duration: 0.137s, episode steps: 32, steps per second: 233, episode reward: 0.925, mean reward: 0.029 [-0.004, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.069 [-0.130, 0.371], loss: 0.000031, mean_absolute_error: 0.485812, mean_q: 0.720043\n",
      " 47370/50000: episode: 1117, duration: 0.129s, episode steps: 35, steps per second: 271, episode reward: 0.890, mean reward: 0.025 [-0.005, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.104 [-0.160, 0.478], loss: 0.000045, mean_absolute_error: 0.484608, mean_q: 0.724476\n",
      " 47390/50000: episode: 1118, duration: 0.076s, episode steps: 20, steps per second: 264, episode reward: 0.972, mean reward: 0.049 [-0.002, 1.000], mean action: 0.550 [0.000, 2.000], mean observation: 0.037 [-0.110, 0.192], loss: 0.000029, mean_absolute_error: 0.497211, mean_q: 0.743550\n",
      " 47439/50000: episode: 1119, duration: 0.196s, episode steps: 49, steps per second: 250, episode reward: 0.782, mean reward: 0.016 [-0.008, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.153 [-0.230, 0.759], loss: 0.000026, mean_absolute_error: 0.484601, mean_q: 0.722450\n",
      " 47463/50000: episode: 1120, duration: 0.135s, episode steps: 24, steps per second: 177, episode reward: 0.955, mean reward: 0.040 [-0.002, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.064 [-0.110, 0.245], loss: 0.000086, mean_absolute_error: 0.485773, mean_q: 0.723053\n",
      " 47499/50000: episode: 1121, duration: 0.186s, episode steps: 36, steps per second: 193, episode reward: 0.884, mean reward: 0.025 [-0.005, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.104 [-0.516, 0.180], loss: 0.000445, mean_absolute_error: 0.494522, mean_q: 0.736223\n",
      " 47519/50000: episode: 1122, duration: 0.099s, episode steps: 20, steps per second: 202, episode reward: 0.965, mean reward: 0.048 [-0.002, 1.000], mean action: 0.550 [0.000, 2.000], mean observation: 0.054 [-0.110, 0.242], loss: 0.000027, mean_absolute_error: 0.494506, mean_q: 0.743134\n",
      " 47520/50000: episode: 1123, duration: 0.008s, episode steps: 1, steps per second: 125, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.006 [-0.010, 0.021], loss: 0.000020, mean_absolute_error: 0.455856, mean_q: 0.677733\n",
      " 47521/50000: episode: 1124, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.039 [-0.089, 0.010], loss: 0.000055, mean_absolute_error: 0.466249, mean_q: 0.708264\n",
      " 47534/50000: episode: 1125, duration: 0.073s, episode steps: 13, steps per second: 178, episode reward: 0.984, mean reward: 0.076 [-0.001, 1.000], mean action: 0.308 [0.000, 2.000], mean observation: 0.041 [-0.090, 0.149], loss: 0.000024, mean_absolute_error: 0.496379, mean_q: 0.745497\n",
      " 47560/50000: episode: 1126, duration: 0.172s, episode steps: 26, steps per second: 151, episode reward: 0.947, mean reward: 0.036 [-0.003, 1.000], mean action: 0.692 [0.000, 2.000], mean observation: 0.067 [-0.120, 0.291], loss: 0.000023, mean_absolute_error: 0.480608, mean_q: 0.716378\n",
      " 47608/50000: episode: 1127, duration: 0.286s, episode steps: 48, steps per second: 168, episode reward: 0.805, mean reward: 0.017 [-0.007, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.142 [-0.180, 0.687], loss: 0.000022, mean_absolute_error: 0.489996, mean_q: 0.732293\n",
      " 47650/50000: episode: 1128, duration: 0.247s, episode steps: 42, steps per second: 170, episode reward: 0.842, mean reward: 0.020 [-0.006, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.128 [-0.613, 0.180], loss: 0.000030, mean_absolute_error: 0.485893, mean_q: 0.723428\n",
      " 47709/50000: episode: 1129, duration: 0.235s, episode steps: 59, steps per second: 251, episode reward: 0.674, mean reward: 0.011 [-0.010, 1.000], mean action: 1.119 [0.000, 2.000], mean observation: -0.201 [-0.992, 0.250], loss: 0.000028, mean_absolute_error: 0.494316, mean_q: 0.739017\n",
      " 47737/50000: episode: 1130, duration: 0.135s, episode steps: 28, steps per second: 208, episode reward: 0.930, mean reward: 0.033 [-0.004, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.075 [-0.379, 0.160], loss: 0.000025, mean_absolute_error: 0.498053, mean_q: 0.746624\n",
      " 47778/50000: episode: 1131, duration: 0.198s, episode steps: 41, steps per second: 207, episode reward: 0.843, mean reward: 0.021 [-0.006, 1.000], mean action: 0.805 [0.000, 2.000], mean observation: 0.130 [-0.190, 0.615], loss: 0.000028, mean_absolute_error: 0.490942, mean_q: 0.732960\n",
      " 47794/50000: episode: 1132, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 0.976, mean reward: 0.061 [-0.002, 1.000], mean action: 1.562 [0.000, 2.000], mean observation: -0.048 [-0.190, 0.110], loss: 0.000021, mean_absolute_error: 0.489606, mean_q: 0.727973\n",
      " 47842/50000: episode: 1133, duration: 0.257s, episode steps: 48, steps per second: 187, episode reward: 0.786, mean reward: 0.016 [-0.008, 1.000], mean action: 0.854 [0.000, 2.000], mean observation: 0.156 [-0.220, 0.751], loss: 0.000027, mean_absolute_error: 0.496258, mean_q: 0.739010\n",
      " 47872/50000: episode: 1134, duration: 0.143s, episode steps: 30, steps per second: 210, episode reward: 0.925, mean reward: 0.031 [-0.004, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.076 [-0.160, 0.387], loss: 0.000026, mean_absolute_error: 0.484261, mean_q: 0.722317\n",
      " 47908/50000: episode: 1135, duration: 0.149s, episode steps: 36, steps per second: 242, episode reward: 0.910, mean reward: 0.025 [-0.004, 1.000], mean action: 0.778 [0.000, 2.000], mean observation: 0.087 [-0.110, 0.380], loss: 0.000027, mean_absolute_error: 0.490120, mean_q: 0.732832\n",
      " 47941/50000: episode: 1136, duration: 0.124s, episode steps: 33, steps per second: 267, episode reward: 0.902, mean reward: 0.027 [-0.005, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.093 [-0.466, 0.180], loss: 0.000061, mean_absolute_error: 0.491979, mean_q: 0.736007\n",
      " 48000/50000: episode: 1137, duration: 0.249s, episode steps: 59, steps per second: 237, episode reward: 0.728, mean reward: 0.012 [-0.008, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.173 [-0.210, 0.773], loss: 0.000450, mean_absolute_error: 0.493564, mean_q: 0.738086\n",
      " 48001/50000: episode: 1138, duration: 0.008s, episode steps: 1, steps per second: 131, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [-0.010, 0.046], loss: 0.000041, mean_absolute_error: 0.489827, mean_q: 0.743349\n",
      " 48002/50000: episode: 1139, duration: 0.007s, episode steps: 1, steps per second: 153, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.039 [-0.088, 0.010], loss: 0.000163, mean_absolute_error: 0.421965, mean_q: 0.610121\n",
      " 48063/50000: episode: 1140, duration: 0.213s, episode steps: 61, steps per second: 286, episode reward: 0.686, mean reward: 0.011 [-0.010, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.182 [-0.260, 0.964], loss: 0.000028, mean_absolute_error: 0.493657, mean_q: 0.739490\n",
      " 48094/50000: episode: 1141, duration: 0.134s, episode steps: 31, steps per second: 231, episode reward: 0.942, mean reward: 0.030 [-0.003, 1.000], mean action: 0.806 [0.000, 2.000], mean observation: 0.068 [-0.080, 0.268], loss: 0.000133, mean_absolute_error: 0.486986, mean_q: 0.727872\n",
      " 48142/50000: episode: 1142, duration: 0.185s, episode steps: 48, steps per second: 260, episode reward: 0.790, mean reward: 0.016 [-0.007, 1.000], mean action: 0.854 [0.000, 2.000], mean observation: 0.154 [-0.230, 0.727], loss: 0.000024, mean_absolute_error: 0.491449, mean_q: 0.735837\n",
      " 48197/50000: episode: 1143, duration: 0.207s, episode steps: 55, steps per second: 266, episode reward: 0.707, mean reward: 0.013 [-0.009, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.192 [-0.916, 0.260], loss: 0.000027, mean_absolute_error: 0.486616, mean_q: 0.724048\n",
      " 48236/50000: episode: 1144, duration: 0.146s, episode steps: 39, steps per second: 268, episode reward: 0.873, mean reward: 0.022 [-0.005, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.105 [-0.535, 0.180], loss: 0.000070, mean_absolute_error: 0.489170, mean_q: 0.727166\n",
      " 48237/50000: episode: 1145, duration: 0.008s, episode steps: 1, steps per second: 124, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.039 [-0.088, 0.010], loss: 0.000163, mean_absolute_error: 0.484697, mean_q: 0.729646\n",
      " 48283/50000: episode: 1146, duration: 0.175s, episode steps: 46, steps per second: 262, episode reward: 0.809, mean reward: 0.018 [-0.007, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.141 [-0.220, 0.707], loss: 0.000032, mean_absolute_error: 0.495946, mean_q: 0.741756\n",
      " 48306/50000: episode: 1147, duration: 0.082s, episode steps: 23, steps per second: 281, episode reward: 0.961, mean reward: 0.042 [-0.002, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.049 [-0.100, 0.247], loss: 0.000021, mean_absolute_error: 0.491916, mean_q: 0.738413\n",
      " 48355/50000: episode: 1148, duration: 0.199s, episode steps: 49, steps per second: 246, episode reward: 0.776, mean reward: 0.016 [-0.008, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.159 [-0.240, 0.782], loss: 0.000029, mean_absolute_error: 0.490871, mean_q: 0.733784\n",
      " 48356/50000: episode: 1149, duration: 0.008s, episode steps: 1, steps per second: 125, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.004 [-0.002, 0.010], loss: 0.000024, mean_absolute_error: 0.454799, mean_q: 0.646772\n",
      " 48397/50000: episode: 1150, duration: 0.145s, episode steps: 41, steps per second: 282, episode reward: 0.844, mean reward: 0.021 [-0.006, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.128 [-0.190, 0.622], loss: 0.000021, mean_absolute_error: 0.493766, mean_q: 0.737376\n",
      " 48433/50000: episode: 1151, duration: 0.130s, episode steps: 36, steps per second: 277, episode reward: 0.917, mean reward: 0.025 [-0.004, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.066 [-0.160, 0.393], loss: 0.000018, mean_absolute_error: 0.486893, mean_q: 0.724258\n",
      " 48474/50000: episode: 1152, duration: 0.158s, episode steps: 41, steps per second: 259, episode reward: 0.840, mean reward: 0.020 [-0.006, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.130 [-0.633, 0.210], loss: 0.000285, mean_absolute_error: 0.487139, mean_q: 0.724705\n",
      " 48517/50000: episode: 1153, duration: 0.169s, episode steps: 43, steps per second: 254, episode reward: 0.862, mean reward: 0.020 [-0.006, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.098 [-0.200, 0.569], loss: 0.000026, mean_absolute_error: 0.494406, mean_q: 0.740159\n",
      " 48565/50000: episode: 1154, duration: 0.172s, episode steps: 48, steps per second: 278, episode reward: 0.820, mean reward: 0.017 [-0.007, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.124 [-0.210, 0.668], loss: 0.000032, mean_absolute_error: 0.488423, mean_q: 0.730452\n",
      " 48612/50000: episode: 1155, duration: 0.189s, episode steps: 47, steps per second: 249, episode reward: 0.784, mean reward: 0.017 [-0.008, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.159 [-0.765, 0.250], loss: 0.000028, mean_absolute_error: 0.490639, mean_q: 0.732092\n",
      " 48631/50000: episode: 1156, duration: 0.069s, episode steps: 19, steps per second: 277, episode reward: 0.971, mean reward: 0.051 [-0.002, 1.000], mean action: 0.632 [0.000, 2.000], mean observation: 0.049 [-0.080, 0.207], loss: 0.000042, mean_absolute_error: 0.498638, mean_q: 0.745831\n",
      " 48650/50000: episode: 1157, duration: 0.067s, episode steps: 19, steps per second: 284, episode reward: 0.969, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 2.000], mean observation: 0.046 [-0.110, 0.228], loss: 0.000017, mean_absolute_error: 0.478179, mean_q: 0.715793\n",
      " 48687/50000: episode: 1158, duration: 0.122s, episode steps: 37, steps per second: 303, episode reward: 0.885, mean reward: 0.024 [-0.005, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.103 [-0.491, 0.160], loss: 0.000027, mean_absolute_error: 0.483902, mean_q: 0.723325\n",
      " 48711/50000: episode: 1159, duration: 0.104s, episode steps: 24, steps per second: 231, episode reward: 0.952, mean reward: 0.040 [-0.003, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.061 [-0.287, 0.120], loss: 0.000016, mean_absolute_error: 0.489432, mean_q: 0.735366\n",
      " 48762/50000: episode: 1160, duration: 0.210s, episode steps: 51, steps per second: 243, episode reward: 0.755, mean reward: 0.015 [-0.008, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.174 [-0.788, 0.220], loss: 0.000023, mean_absolute_error: 0.492120, mean_q: 0.734054\n",
      " 48808/50000: episode: 1161, duration: 0.164s, episode steps: 46, steps per second: 281, episode reward: 0.820, mean reward: 0.018 [-0.007, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.130 [-0.200, 0.677], loss: 0.000018, mean_absolute_error: 0.492689, mean_q: 0.735578\n",
      " 48864/50000: episode: 1162, duration: 0.219s, episode steps: 56, steps per second: 256, episode reward: 0.715, mean reward: 0.013 [-0.009, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.181 [-0.250, 0.904], loss: 0.000018, mean_absolute_error: 0.488647, mean_q: 0.727558\n",
      " 48865/50000: episode: 1163, duration: 0.007s, episode steps: 1, steps per second: 148, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.085, 0.010], loss: 0.000051, mean_absolute_error: 0.452325, mean_q: 0.685075\n",
      " 48905/50000: episode: 1164, duration: 0.135s, episode steps: 40, steps per second: 296, episode reward: 0.859, mean reward: 0.021 [-0.006, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.117 [-0.575, 0.170], loss: 0.000229, mean_absolute_error: 0.485406, mean_q: 0.723767\n",
      " 48921/50000: episode: 1165, duration: 0.049s, episode steps: 16, steps per second: 327, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 0.438 [0.000, 2.000], mean observation: 0.038 [-0.110, 0.194], loss: 0.000036, mean_absolute_error: 0.485992, mean_q: 0.723635\n",
      " 48940/50000: episode: 1166, duration: 0.069s, episode steps: 19, steps per second: 275, episode reward: 0.968, mean reward: 0.051 [-0.002, 1.000], mean action: 1.421 [0.000, 2.000], mean observation: -0.051 [-0.227, 0.110], loss: 0.000025, mean_absolute_error: 0.495470, mean_q: 0.744129\n",
      " 48982/50000: episode: 1167, duration: 0.177s, episode steps: 42, steps per second: 237, episode reward: 0.847, mean reward: 0.020 [-0.006, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.125 [-0.577, 0.170], loss: 0.000255, mean_absolute_error: 0.499985, mean_q: 0.747864\n",
      " 49031/50000: episode: 1168, duration: 0.203s, episode steps: 49, steps per second: 241, episode reward: 0.777, mean reward: 0.016 [-0.008, 1.000], mean action: 0.837 [0.000, 2.000], mean observation: 0.159 [-0.210, 0.777], loss: 0.000029, mean_absolute_error: 0.490314, mean_q: 0.733966\n",
      " 49089/50000: episode: 1169, duration: 0.226s, episode steps: 58, steps per second: 257, episode reward: 0.669, mean reward: 0.012 [-0.010, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.207 [-0.270, 0.994], loss: 0.000270, mean_absolute_error: 0.486829, mean_q: 0.728003\n",
      " 49137/50000: episode: 1170, duration: 0.198s, episode steps: 48, steps per second: 242, episode reward: 0.778, mean reward: 0.016 [-0.008, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.162 [-0.210, 0.774], loss: 0.000033, mean_absolute_error: 0.495658, mean_q: 0.744283\n",
      " 49176/50000: episode: 1171, duration: 0.152s, episode steps: 39, steps per second: 256, episode reward: 0.862, mean reward: 0.022 [-0.006, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.117 [-0.571, 0.190], loss: 0.000030, mean_absolute_error: 0.496941, mean_q: 0.743512\n",
      " 49237/50000: episode: 1172, duration: 0.207s, episode steps: 61, steps per second: 295, episode reward: 0.652, mean reward: 0.011 [-0.010, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.213 [-0.974, 0.240], loss: 0.000172, mean_absolute_error: 0.491224, mean_q: 0.738599\n",
      " 49283/50000: episode: 1173, duration: 0.183s, episode steps: 46, steps per second: 251, episode reward: 0.828, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.129 [-0.631, 0.180], loss: 0.000046, mean_absolute_error: 0.488848, mean_q: 0.730004\n",
      " 49301/50000: episode: 1174, duration: 0.079s, episode steps: 18, steps per second: 227, episode reward: 0.970, mean reward: 0.054 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.048 [-0.232, 0.120], loss: 0.000112, mean_absolute_error: 0.501105, mean_q: 0.752863\n",
      " 49330/50000: episode: 1175, duration: 0.109s, episode steps: 29, steps per second: 265, episode reward: 0.928, mean reward: 0.032 [-0.004, 1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.078 [-0.375, 0.140], loss: 0.000029, mean_absolute_error: 0.499559, mean_q: 0.751862\n",
      " 49378/50000: episode: 1176, duration: 0.245s, episode steps: 48, steps per second: 196, episode reward: 0.797, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.148 [-0.710, 0.200], loss: 0.000025, mean_absolute_error: 0.491253, mean_q: 0.737245\n",
      " 49425/50000: episode: 1177, duration: 0.197s, episode steps: 47, steps per second: 239, episode reward: 0.821, mean reward: 0.017 [-0.006, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.133 [-0.190, 0.638], loss: 0.000025, mean_absolute_error: 0.491721, mean_q: 0.735138\n",
      " 49451/50000: episode: 1178, duration: 0.105s, episode steps: 26, steps per second: 247, episode reward: 0.940, mean reward: 0.036 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.070 [-0.339, 0.150], loss: 0.000022, mean_absolute_error: 0.489000, mean_q: 0.731790\n",
      " 49504/50000: episode: 1179, duration: 0.205s, episode steps: 53, steps per second: 259, episode reward: 0.748, mean reward: 0.014 [-0.008, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.170 [-0.800, 0.220], loss: 0.000021, mean_absolute_error: 0.494130, mean_q: 0.741018\n",
      " 49557/50000: episode: 1180, duration: 0.226s, episode steps: 53, steps per second: 235, episode reward: 0.760, mean reward: 0.014 [-0.008, 1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.161 [-0.803, 0.210], loss: 0.000023, mean_absolute_error: 0.492667, mean_q: 0.737388\n",
      " 49558/50000: episode: 1181, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.026 [-0.061, 0.010], loss: 0.000010, mean_absolute_error: 0.477464, mean_q: 0.708831\n",
      " 49613/50000: episode: 1182, duration: 0.282s, episode steps: 55, steps per second: 195, episode reward: 0.730, mean reward: 0.013 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.178 [-0.839, 0.200], loss: 0.000026, mean_absolute_error: 0.493336, mean_q: 0.737650\n",
      " 49633/50000: episode: 1183, duration: 0.102s, episode steps: 20, steps per second: 195, episode reward: 0.964, mean reward: 0.048 [-0.002, 1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.053 [-0.250, 0.110], loss: 0.000019, mean_absolute_error: 0.494260, mean_q: 0.745675\n",
      " 49673/50000: episode: 1184, duration: 0.189s, episode steps: 40, steps per second: 212, episode reward: 0.870, mean reward: 0.022 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.108 [-0.180, 0.538], loss: 0.000022, mean_absolute_error: 0.497044, mean_q: 0.747911\n",
      " 49730/50000: episode: 1185, duration: 0.310s, episode steps: 57, steps per second: 184, episode reward: 0.697, mean reward: 0.012 [-0.009, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.192 [-0.230, 0.938], loss: 0.000026, mean_absolute_error: 0.494934, mean_q: 0.742051\n",
      " 49746/50000: episode: 1186, duration: 0.101s, episode steps: 16, steps per second: 159, episode reward: 0.979, mean reward: 0.061 [-0.002, 1.000], mean action: 1.562 [0.000, 2.000], mean observation: -0.034 [-0.175, 0.100], loss: 0.000242, mean_absolute_error: 0.493068, mean_q: 0.737659\n",
      " 49767/50000: episode: 1187, duration: 0.097s, episode steps: 21, steps per second: 216, episode reward: 0.963, mean reward: 0.046 [-0.003, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.046 [-0.252, 0.140], loss: 0.000039, mean_absolute_error: 0.503159, mean_q: 0.753261\n",
      " 49776/50000: episode: 1188, duration: 0.041s, episode steps: 9, steps per second: 222, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.039 [-0.060, 0.138], loss: 0.000036, mean_absolute_error: 0.495997, mean_q: 0.746328\n",
      " 49797/50000: episode: 1189, duration: 0.099s, episode steps: 21, steps per second: 212, episode reward: 0.965, mean reward: 0.046 [-0.002, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.049 [-0.100, 0.236], loss: 0.000025, mean_absolute_error: 0.493782, mean_q: 0.742586\n",
      " 49839/50000: episode: 1190, duration: 0.192s, episode steps: 42, steps per second: 219, episode reward: 0.835, mean reward: 0.020 [-0.006, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.134 [-0.210, 0.633], loss: 0.000024, mean_absolute_error: 0.496496, mean_q: 0.747088\n",
      " 49874/50000: episode: 1191, duration: 0.145s, episode steps: 35, steps per second: 241, episode reward: 0.891, mean reward: 0.025 [-0.005, 1.000], mean action: 1.229 [0.000, 2.000], mean observation: -0.101 [-0.497, 0.190], loss: 0.000019, mean_absolute_error: 0.497715, mean_q: 0.749321\n",
      " 49935/50000: episode: 1192, duration: 0.284s, episode steps: 61, steps per second: 215, episode reward: 0.679, mean reward: 0.011 [-0.009, 1.000], mean action: 0.885 [0.000, 2.000], mean observation: 0.194 [-0.240, 0.947], loss: 0.000026, mean_absolute_error: 0.494336, mean_q: 0.740008\n",
      " 49976/50000: episode: 1193, duration: 0.198s, episode steps: 41, steps per second: 207, episode reward: 0.860, mean reward: 0.021 [-0.006, 1.000], mean action: 0.805 [0.000, 2.000], mean observation: 0.115 [-0.170, 0.565], loss: 0.000022, mean_absolute_error: 0.501946, mean_q: 0.753351\n",
      "done, took 191.517 seconds\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "env = PointOnLine()\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# DQNのネットワーク定義\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# experience replay用のmemory\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "# 行動方策はオーソドックスなepsilon-greedy。ほかに、各行動のQ値によって確率を決定するBoltzmannQPolicyが利用可能\n",
    "policy = EpsGreedyQPolicy(eps=0.1) \n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2, nb_max_episode_steps=300)\n",
    "#学習の様子を描画したいときは、Envに_render()を実装して、visualize=True にします,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 0.995, steps: 6\n",
      "Episode 2: reward: 0.715, steps: 60\n",
      "Episode 3: reward: 0.778, steps: 53\n",
      "Episode 4: reward: 0.839, steps: 41\n",
      "Episode 5: reward: 0.844, steps: 46\n",
      "Episode 6: reward: 0.678, steps: 62\n",
      "Episode 7: reward: 0.906, steps: 33\n",
      "Episode 8: reward: 1.000, steps: 1\n",
      "Episode 9: reward: 0.889, steps: 39\n",
      "Episode 10: reward: 0.958, steps: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f5072be7e10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAF5CAYAAACxwgF3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5072883470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import rl.callbacks\n",
    "class EpisodeLogger(rl.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.observations = {}\n",
    "        self.rewards = {}\n",
    "        self.actions = {}\n",
    "\n",
    "    def on_episode_begin(self, episode, logs):\n",
    "        self.observations[episode] = []\n",
    "        self.rewards[episode] = []\n",
    "        self.actions[episode] = []\n",
    "\n",
    "    def on_step_end(self, step, logs):\n",
    "        episode = logs['episode']\n",
    "        self.observations[episode].append(logs['observation'])\n",
    "        self.rewards[episode].append(logs['reward'])\n",
    "        self.actions[episode].append(logs['action'])\n",
    "\n",
    "cb_ep = EpisodeLogger()\n",
    "dqn.test(env, nb_episodes=10, visualize=False, callbacks=[cb_ep])\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for obs in cb_ep.observations.values():\n",
    "    plt.plot([o[0] for o in obs])\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
