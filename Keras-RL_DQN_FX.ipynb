{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Python] Keras-RLで簡単に強化学習(DQN)を試す](http://qiita.com/inoory/items/e63ade6f21766c7c2393)を参考に、エージェントを作成する。FXの自動取引を行い、利益を出すのが目標。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use('tkagg')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib\n",
    "from logging import getLogger, DEBUG, INFO, WARN, ERROR, CRITICAL\n",
    "import os\n",
    "import logging\n",
    "from logging import StreamHandler, LogRecord\n",
    "\n",
    "from hist_data import HistData, BitcoinHistData\n",
    "from fx_trade import FXTrade\n",
    "from bitcoin_trade import BitcoinTrade\n",
    "from deep_fx import DeepFX\n",
    "from debug_tools import DebugTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import crcmod\n",
    "class LogRecordWithHexThereadID(logging.LogRecord):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hex_threadid = self._calc_hex(self.process)\n",
    "\n",
    "    def _calc_hex(self, digit_value):\n",
    "        return hex(digit_value)\n",
    "\n",
    "def init_logger(sd_loglevel=logging.WARN, stream_loglevel=logging.CRITICAL):\n",
    "    logging.setLogRecordFactory(LogRecordWithHexThereadID)\n",
    "    logger = logging.getLogger('deepfx')\n",
    "    logger.setLevel(sd_loglevel)\n",
    "    formatter = logging.Formatter('[%(hex_threadid)s] %(message)s')\n",
    "\n",
    "    if sd_loglevel:\n",
    "        import google\n",
    "        from google.cloud.logging import Client\n",
    "        from google.cloud.logging.handlers import CloudLoggingHandler\n",
    "        client = google.cloud.logging.Client \\\n",
    "            .from_service_account_json(os.environ.get('GOOGLE_SERVICE_ACCOUNT_JSON_PATH'))\n",
    "        handler = CloudLoggingHandler(client, name='deepfx')\n",
    "        handler.setLevel(sd_loglevel)\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        handler = None\n",
    "\n",
    "    if stream_loglevel:\n",
    "        handler = StreamHandler()\n",
    "        handler.setLevel(stream_loglevel)\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        handler = None\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepfx_logger = init_logger(stream_loglevel=None)\n",
    "deepfx_logger.critical('DeepFX Started: %s' % DebugTools.now_str())\n",
    "deepfx_logger.debug   ('loglevel debug    test')\n",
    "deepfx_logger.info    ('loglevel info     test')\n",
    "deepfx_logger.warning ('loglevel warn     test')\n",
    "deepfx_logger.error   ('loglevel error    test')\n",
    "deepfx_logger.critical('loglevel critical test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_for_fx = False\n",
    "is_for_bitcoin = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header is included\n"
     ]
    }
   ],
   "source": [
    "if is_for_fx:\n",
    "    hd = HistData(csv_path = 'historical_data/DAT_ASCII_USDJPY_M1_201710_h1.csv',\n",
    "                     begin_date='2017-10-02T00:00:00',\n",
    "                     end_date='2017-10-02T01:59:59')\n",
    "elif is_for_bitcoin:\n",
    "    hd = HistData(csv_path = 'historical_data/coincheckJPY_1-min_data_2014-10-31_to_2017-10-20_h1.csv',\n",
    "                     begin_date='2017-09-01T00:00:00',\n",
    "                     end_date='2017-09-30T23:59:59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd.data()\n",
    "len(hd.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_for_fx:\n",
    "    env = FXTrade(1000000, 0.08, hd, logger=deepfx_logger)\n",
    "    #env = FXTrade(1000000, 0.08, h, logger=logger)\n",
    "    prepared_model_filename = None #'Keras-RL_DQN_FX_model_meanq1.440944e+06_episode00003.h5'\n",
    "    dfx = DeepFX(env, prepared_model_filename=prepared_model_filename, steps = 100000, logger=deepfx_logger)\n",
    "elif is_for_bitcoin:\n",
    "    env = BitcoinTrade(10000000, None, hd, logger=deepfx_logger, amount_unit=0.001)\n",
    "    #env = FXTrade(1000000, 0.08, h, logger=logger)\n",
    "    prepared_model_filename = None #'Keras-RL_DQN_FX_model_meanq1.440944e+06_episode00003.h5'\n",
    "    dfx = DeepFX(env, prepared_model_filename=prepared_model_filename, steps = 10000000, logger=deepfx_logger)\n",
    "    #dfx = DeepFX(env, prepared_model_filename=prepared_model_filename, steps = 1000, logger=deepfx_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 12        \n",
      "=================================================================\n",
      "Total params: 21\n",
      "Trainable params: 21\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training for 500000 steps ...\n",
      "Training for 500000 steps ...\n",
      "      1/500000: episode: 1, duration: 0.635s, episode steps: 1, steps per second: 2, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      1/500000: episode: 1, duration: 0.636s, episode steps: 1, steps per second: 2, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      2/500000: episode: 2, duration: 0.007s, episode steps: 1, steps per second: 136, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      2/500000: episode: 2, duration: 0.008s, episode steps: 1, steps per second: 120, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      3/500000: episode: 3, duration: 0.009s, episode steps: 1, steps per second: 109, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      3/500000: episode: 3, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      4/500000: episode: 4, duration: 0.007s, episode steps: 1, steps per second: 152, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      4/500000: episode: 4, duration: 0.008s, episode steps: 1, steps per second: 128, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      5/500000: episode: 5, duration: 0.006s, episode steps: 1, steps per second: 175, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      5/500000: episode: 5, duration: 0.007s, episode steps: 1, steps per second: 140, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      6/500000: episode: 6, duration: 0.006s, episode steps: 1, steps per second: 169, episode reward: 241170548069.919, mean reward: 241170548069.919 [241170548069.919, 241170548069.919], mean action: 2.000 [2.000, 2.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      6/500000: episode: 6, duration: 0.007s, episode steps: 1, steps per second: 147, episode reward: 241170548069.919, mean reward: 241170548069.919 [241170548069.919, 241170548069.919], mean action: 2.000 [2.000, 2.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      7/500000: episode: 7, duration: 0.006s, episode steps: 1, steps per second: 154, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      7/500000: episode: 7, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      8/500000: episode: 8, duration: 0.006s, episode steps: 1, steps per second: 164, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      8/500000: episode: 8, duration: 0.007s, episode steps: 1, steps per second: 142, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      9/500000: episode: 9, duration: 0.007s, episode steps: 1, steps per second: 134, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "      9/500000: episode: 9, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     10/500000: episode: 10, duration: 0.005s, episode steps: 1, steps per second: 200, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     10/500000: episode: 10, duration: 0.006s, episode steps: 1, steps per second: 171, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     11/500000: episode: 11, duration: 0.007s, episode steps: 1, steps per second: 141, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     11/500000: episode: 11, duration: 0.008s, episode steps: 1, steps per second: 125, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     12/500000: episode: 12, duration: 0.006s, episode steps: 1, steps per second: 173, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     12/500000: episode: 12, duration: 0.007s, episode steps: 1, steps per second: 151, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     13/500000: episode: 13, duration: 0.006s, episode steps: 1, steps per second: 155, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     13/500000: episode: 13, duration: 0.008s, episode steps: 1, steps per second: 130, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     14/500000: episode: 14, duration: 0.006s, episode steps: 1, steps per second: 175, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     14/500000: episode: 14, duration: 0.007s, episode steps: 1, steps per second: 152, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     15/500000: episode: 15, duration: 0.006s, episode steps: 1, steps per second: 168, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     15/500000: episode: 15, duration: 0.008s, episode steps: 1, steps per second: 126, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     16/500000: episode: 16, duration: 0.006s, episode steps: 1, steps per second: 168, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     16/500000: episode: 16, duration: 0.007s, episode steps: 1, steps per second: 138, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     17/500000: episode: 17, duration: 0.007s, episode steps: 1, steps per second: 136, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     17/500000: episode: 17, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     18/500000: episode: 18, duration: 0.007s, episode steps: 1, steps per second: 137, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     18/500000: episode: 18, duration: 0.008s, episode steps: 1, steps per second: 121, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     19/500000: episode: 19, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     19/500000: episode: 19, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     20/500000: episode: 20, duration: 0.008s, episode steps: 1, steps per second: 127, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     20/500000: episode: 20, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     21/500000: episode: 21, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     21/500000: episode: 21, duration: 0.015s, episode steps: 1, steps per second: 67, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22/500000: episode: 22, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     22/500000: episode: 22, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     23/500000: episode: 23, duration: 0.013s, episode steps: 1, steps per second: 76, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     23/500000: episode: 23, duration: 0.015s, episode steps: 1, steps per second: 69, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     24/500000: episode: 24, duration: 0.009s, episode steps: 1, steps per second: 111, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     24/500000: episode: 24, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     25/500000: episode: 25, duration: 0.010s, episode steps: 1, steps per second: 105, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     25/500000: episode: 25, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     26/500000: episode: 26, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     26/500000: episode: 26, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     27/500000: episode: 27, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     27/500000: episode: 27, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     28/500000: episode: 28, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     28/500000: episode: 28, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     29/500000: episode: 29, duration: 0.008s, episode steps: 1, steps per second: 118, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     29/500000: episode: 29, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     30/500000: episode: 30, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     30/500000: episode: 30, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     31/500000: episode: 31, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     31/500000: episode: 31, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     32/500000: episode: 32, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     32/500000: episode: 32, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     33/500000: episode: 33, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     33/500000: episode: 33, duration: 0.012s, episode steps: 1, steps per second: 81, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     34/500000: episode: 34, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     34/500000: episode: 34, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     35/500000: episode: 35, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     35/500000: episode: 35, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     36/500000: episode: 36, duration: 0.006s, episode steps: 1, steps per second: 162, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     36/500000: episode: 36, duration: 0.007s, episode steps: 1, steps per second: 134, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     37/500000: episode: 37, duration: 0.008s, episode steps: 1, steps per second: 129, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     37/500000: episode: 37, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     38/500000: episode: 38, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     38/500000: episode: 38, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     39/500000: episode: 39, duration: 0.018s, episode steps: 1, steps per second: 56, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     39/500000: episode: 39, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     40/500000: episode: 40, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     40/500000: episode: 40, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     41/500000: episode: 41, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     41/500000: episode: 41, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     42/500000: episode: 42, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     42/500000: episode: 42, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     43/500000: episode: 43, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     43/500000: episode: 43, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     44/500000: episode: 44, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     44/500000: episode: 44, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     45/500000: episode: 45, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     45/500000: episode: 45, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     46/500000: episode: 46, duration: 0.009s, episode steps: 1, steps per second: 110, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     46/500000: episode: 46, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     47/500000: episode: 47, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     47/500000: episode: 47, duration: 0.014s, episode steps: 1, steps per second: 74, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     48/500000: episode: 48, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     48/500000: episode: 48, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     49/500000: episode: 49, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     49/500000: episode: 49, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     50/500000: episode: 50, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     50/500000: episode: 50, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     51/500000: episode: 51, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     51/500000: episode: 51, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     52/500000: episode: 52, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     52/500000: episode: 52, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     53/500000: episode: 53, duration: 0.008s, episode steps: 1, steps per second: 124, episode reward: 241170548069.919, mean reward: 241170548069.919 [241170548069.919, 241170548069.919], mean action: 2.000 [2.000, 2.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     53/500000: episode: 53, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 241170548069.919, mean reward: 241170548069.919 [241170548069.919, 241170548069.919], mean action: 2.000 [2.000, 2.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     54/500000: episode: 54, duration: 0.008s, episode steps: 1, steps per second: 129, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     54/500000: episode: 54, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     55/500000: episode: 55, duration: 0.009s, episode steps: 1, steps per second: 115, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     55/500000: episode: 55, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     56/500000: episode: 56, duration: 0.007s, episode steps: 1, steps per second: 143, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     56/500000: episode: 56, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     57/500000: episode: 57, duration: 0.008s, episode steps: 1, steps per second: 126, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     57/500000: episode: 57, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     58/500000: episode: 58, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     58/500000: episode: 58, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     59/500000: episode: 59, duration: 0.014s, episode steps: 1, steps per second: 69, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     59/500000: episode: 59, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     60/500000: episode: 60, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     60/500000: episode: 60, duration: 0.017s, episode steps: 1, steps per second: 60, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     61/500000: episode: 61, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     61/500000: episode: 61, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     62/500000: episode: 62, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     62/500000: episode: 62, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     63/500000: episode: 63, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     63/500000: episode: 63, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     64/500000: episode: 64, duration: 0.008s, episode steps: 1, steps per second: 130, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     64/500000: episode: 64, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     65/500000: episode: 65, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     65/500000: episode: 65, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     66/500000: episode: 66, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     66/500000: episode: 66, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     67/500000: episode: 67, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     67/500000: episode: 67, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     68/500000: episode: 68, duration: 0.009s, episode steps: 1, steps per second: 114, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     68/500000: episode: 68, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     69/500000: episode: 69, duration: 0.008s, episode steps: 1, steps per second: 124, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     69/500000: episode: 69, duration: 0.009s, episode steps: 1, steps per second: 108, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     70/500000: episode: 70, duration: 0.008s, episode steps: 1, steps per second: 122, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     70/500000: episode: 70, duration: 0.009s, episode steps: 1, steps per second: 106, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     71/500000: episode: 71, duration: 0.006s, episode steps: 1, steps per second: 168, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     71/500000: episode: 71, duration: 0.007s, episode steps: 1, steps per second: 146, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     72/500000: episode: 72, duration: 0.007s, episode steps: 1, steps per second: 136, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     72/500000: episode: 72, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     73/500000: episode: 73, duration: 0.007s, episode steps: 1, steps per second: 134, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     73/500000: episode: 73, duration: 0.009s, episode steps: 1, steps per second: 116, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     74/500000: episode: 74, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     74/500000: episode: 74, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     75/500000: episode: 75, duration: 0.008s, episode steps: 1, steps per second: 118, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     75/500000: episode: 75, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     76/500000: episode: 76, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     76/500000: episode: 76, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     77/500000: episode: 77, duration: 0.051s, episode steps: 1, steps per second: 19, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     77/500000: episode: 77, duration: 0.053s, episode steps: 1, steps per second: 19, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     78/500000: episode: 78, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     78/500000: episode: 78, duration: 0.019s, episode steps: 1, steps per second: 52, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     79/500000: episode: 79, duration: 0.020s, episode steps: 1, steps per second: 51, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     79/500000: episode: 79, duration: 0.022s, episode steps: 1, steps per second: 45, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     80/500000: episode: 80, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     80/500000: episode: 80, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     81/500000: episode: 81, duration: 0.021s, episode steps: 1, steps per second: 48, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     81/500000: episode: 81, duration: 0.023s, episode steps: 1, steps per second: 44, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     82/500000: episode: 82, duration: 0.007s, episode steps: 1, steps per second: 152, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     82/500000: episode: 82, duration: 0.008s, episode steps: 1, steps per second: 126, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     83/500000: episode: 83, duration: 0.010s, episode steps: 1, steps per second: 104, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     83/500000: episode: 83, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     84/500000: episode: 84, duration: 0.031s, episode steps: 1, steps per second: 32, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     84/500000: episode: 84, duration: 0.034s, episode steps: 1, steps per second: 30, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     85/500000: episode: 85, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     85/500000: episode: 85, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     86/500000: episode: 86, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     86/500000: episode: 86, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     87/500000: episode: 87, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     87/500000: episode: 87, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     88/500000: episode: 88, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     88/500000: episode: 88, duration: 0.016s, episode steps: 1, steps per second: 62, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     89/500000: episode: 89, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     89/500000: episode: 89, duration: 0.013s, episode steps: 1, steps per second: 80, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     90/500000: episode: 90, duration: 0.010s, episode steps: 1, steps per second: 101, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     90/500000: episode: 90, duration: 0.013s, episode steps: 1, steps per second: 78, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     91/500000: episode: 91, duration: 0.012s, episode steps: 1, steps per second: 82, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     91/500000: episode: 91, duration: 0.014s, episode steps: 1, steps per second: 73, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     92/500000: episode: 92, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     92/500000: episode: 92, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     93/500000: episode: 93, duration: 0.007s, episode steps: 1, steps per second: 142, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     93/500000: episode: 93, duration: 0.008s, episode steps: 1, steps per second: 119, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     94/500000: episode: 94, duration: 0.008s, episode steps: 1, steps per second: 120, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     94/500000: episode: 94, duration: 0.009s, episode steps: 1, steps per second: 107, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     95/500000: episode: 95, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     95/500000: episode: 95, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     96/500000: episode: 96, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     96/500000: episode: 96, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     97/500000: episode: 97, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     97/500000: episode: 97, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     98/500000: episode: 98, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     98/500000: episode: 98, duration: 0.012s, episode steps: 1, steps per second: 84, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     99/500000: episode: 99, duration: 0.007s, episode steps: 1, steps per second: 139, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "     99/500000: episode: 99, duration: 0.008s, episode steps: 1, steps per second: 126, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "    100/500000: episode: 100, duration: 0.009s, episode steps: 1, steps per second: 112, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "    100/500000: episode: 100, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101/500000: episode: 101, duration: 0.271s, episode steps: 1, steps per second: 4, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "    101/500000: episode: 101, duration: 0.271s, episode steps: 1, steps per second: 4, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: --, mean_q: --\n",
      "    102/500000: episode: 102, duration: 0.269s, episode steps: 1, steps per second: 4, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908800016524299468800.000000, mean_q: 193248.000000\n",
      "    102/500000: episode: 102, duration: 0.270s, episode steps: 1, steps per second: 4, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908800016524299468800.000000, mean_q: 193248.000000\n",
      "    103/500000: episode: 103, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908800016524299468800.000000, mean_q: 194510.031250\n",
      "    103/500000: episode: 103, duration: 0.187s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908800016524299468800.000000, mean_q: 194510.031250\n",
      "    104/500000: episode: 104, duration: 0.190s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48191814762496.000000, mean_q: 195785.250000\n",
      "    104/500000: episode: 104, duration: 0.190s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48191814762496.000000, mean_q: 195785.250000\n",
      "    105/500000: episode: 105, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817600033048598937600.000000, mean_q: 197026.390625\n",
      "    105/500000: episode: 105, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817600033048598937600.000000, mean_q: 197026.390625\n",
      "    106/500000: episode: 106, duration: 0.215s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48297377005568.000000, mean_q: 198321.593750\n",
      "    106/500000: episode: 106, duration: 0.215s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48297377005568.000000, mean_q: 198321.593750\n",
      "    107/500000: episode: 107, duration: 0.198s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48024139071488.000000, mean_q: 199577.843750\n",
      "    107/500000: episode: 107, duration: 0.199s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48024139071488.000000, mean_q: 199577.843750\n",
      "    108/500000: episode: 108, duration: 0.183s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817600033048598937600.000000, mean_q: 200808.234375\n",
      "    108/500000: episode: 108, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817600033048598937600.000000, mean_q: 200808.234375\n",
      "    109/500000: episode: 109, duration: 0.220s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799875786811113472.000000, mean_q: 202090.062500\n",
      "    109/500000: episode: 109, duration: 0.221s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799875786811113472.000000, mean_q: 202090.062500\n",
      "    110/500000: episode: 110, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799875786811113472.000000, mean_q: 203413.140625\n",
      "    110/500000: episode: 110, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799875786811113472.000000, mean_q: 203413.140625\n",
      "    111/500000: episode: 111, duration: 0.200s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799875786811113472.000000, mean_q: 204737.625000\n",
      "    111/500000: episode: 111, duration: 0.201s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799875786811113472.000000, mean_q: 204737.625000\n",
      "    112/500000: episode: 112, duration: 0.200s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799875786811113472.000000, mean_q: 206037.140625\n",
      "    112/500000: episode: 112, duration: 0.201s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799875786811113472.000000, mean_q: 206037.140625\n",
      "    113/500000: episode: 113, duration: 0.200s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48209137238016.000000, mean_q: 207318.734375\n",
      "    113/500000: episode: 113, duration: 0.201s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48209137238016.000000, mean_q: 207318.734375\n",
      "    114/500000: episode: 114, duration: 0.191s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48197036670976.000000, mean_q: 208545.984375\n",
      "    114/500000: episode: 114, duration: 0.192s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48197036670976.000000, mean_q: 208545.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    115/500000: episode: 115, duration: 0.206s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48315731279872.000000, mean_q: 209729.390625\n",
      "    115/500000: episode: 115, duration: 0.207s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48315731279872.000000, mean_q: 209729.390625\n",
      "    116/500000: episode: 116, duration: 0.188s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48043554504704.000000, mean_q: 210898.890625\n",
      "    116/500000: episode: 116, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48043554504704.000000, mean_q: 210898.890625\n",
      "    117/500000: episode: 117, duration: 0.205s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47901900275712.000000, mean_q: 212059.234375\n",
      "    117/500000: episode: 117, duration: 0.207s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47901900275712.000000, mean_q: 212059.234375\n",
      "    118/500000: episode: 118, duration: 0.178s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48151444586496.000000, mean_q: 213194.015625\n",
      "    118/500000: episode: 118, duration: 0.179s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48151444586496.000000, mean_q: 213194.015625\n",
      "    119/500000: episode: 119, duration: 0.173s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48010302062592.000000, mean_q: 214303.500000\n",
      "    119/500000: episode: 119, duration: 0.174s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48010302062592.000000, mean_q: 214303.500000\n",
      "    120/500000: episode: 120, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817599751573622226944.000000, mean_q: 215418.484375\n",
      "    120/500000: episode: 120, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817599751573622226944.000000, mean_q: 215418.484375\n",
      "    121/500000: episode: 121, duration: 0.180s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 216650.515625\n",
      "    121/500000: episode: 121, duration: 0.182s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 216650.515625\n",
      "    122/500000: episode: 122, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47844723523584.000000, mean_q: 217901.890625\n",
      "    122/500000: episode: 122, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47844723523584.000000, mean_q: 217901.890625\n",
      "    123/500000: episode: 123, duration: 0.209s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 219151.531250\n",
      "    123/500000: episode: 123, duration: 0.211s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 219151.531250\n",
      "    124/500000: episode: 124, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 220463.468750\n",
      "    124/500000: episode: 124, duration: 0.196s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 220463.468750\n",
      "    125/500000: episode: 125, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 221777.531250\n",
      "    125/500000: episode: 125, duration: 0.190s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 221777.531250\n",
      "    126/500000: episode: 126, duration: 0.178s, episode steps: 1, steps per second: 6, episode reward: 241170548069.919, mean reward: 241170548069.919 [241170548069.919, 241170548069.919], mean action: 2.000 [2.000, 2.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 223079.625000\n",
      "    126/500000: episode: 126, duration: 0.179s, episode steps: 1, steps per second: 6, episode reward: 241170548069.919, mean reward: 241170548069.919 [241170548069.919, 241170548069.919], mean action: 2.000 [2.000, 2.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 223079.625000\n",
      "    127/500000: episode: 127, duration: 0.200s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 224373.390625\n",
      "    127/500000: episode: 127, duration: 0.202s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 224373.390625\n",
      "    128/500000: episode: 128, duration: 0.201s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 2726399275516712452096.000000, mean_q: 225678.234375\n",
      "    128/500000: episode: 128, duration: 0.202s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 2726399275516712452096.000000, mean_q: 225678.234375\n",
      "    129/500000: episode: 129, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 227092.265625\n",
      "    129/500000: episode: 129, duration: 0.196s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799735049322758144.000000, mean_q: 227092.265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    130/500000: episode: 130, duration: 0.191s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48133878841344.000000, mean_q: 228509.015625\n",
      "    130/500000: episode: 130, duration: 0.192s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48133878841344.000000, mean_q: 228509.015625\n",
      "    131/500000: episode: 131, duration: 0.172s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47989976465408.000000, mean_q: 229845.515625\n",
      "    131/500000: episode: 131, duration: 0.173s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47989976465408.000000, mean_q: 229845.515625\n",
      "    132/500000: episode: 132, duration: 0.182s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799664680578580480.000000, mean_q: 231117.281250\n",
      "    132/500000: episode: 132, duration: 0.183s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799664680578580480.000000, mean_q: 231117.281250\n",
      "    133/500000: episode: 133, duration: 0.177s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48096495009792.000000, mean_q: 232379.859375\n",
      "    133/500000: episode: 133, duration: 0.178s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48096495009792.000000, mean_q: 232379.859375\n",
      "    134/500000: episode: 134, duration: 0.183s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47822514683904.000000, mean_q: 233590.281250\n",
      "    134/500000: episode: 134, duration: 0.183s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47822514683904.000000, mean_q: 233590.281250\n",
      "    135/500000: episode: 135, duration: 0.188s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48073254371328.000000, mean_q: 234785.250000\n",
      "    135/500000: episode: 135, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 48073254371328.000000, mean_q: 234785.250000\n",
      "    136/500000: episode: 136, duration: 0.187s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47668000718848.000000, mean_q: 235984.281250\n",
      "    136/500000: episode: 136, duration: 0.188s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47668000718848.000000, mean_q: 235984.281250\n",
      "    137/500000: episode: 137, duration: 0.199s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799664680578580480.000000, mean_q: 237232.656250\n",
      "    137/500000: episode: 137, duration: 0.202s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799664680578580480.000000, mean_q: 237232.656250\n",
      "    138/500000: episode: 138, duration: 0.200s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47642893615104.000000, mean_q: 238556.156250\n",
      "    138/500000: episode: 138, duration: 0.201s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47642893615104.000000, mean_q: 238556.156250\n",
      "    139/500000: episode: 139, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47892920270848.000000, mean_q: 239866.609375\n",
      "    139/500000: episode: 139, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47892920270848.000000, mean_q: 239866.609375\n",
      "    140/500000: episode: 140, duration: 0.187s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47748954980352.000000, mean_q: 241161.531250\n",
      "    140/500000: episode: 140, duration: 0.188s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47748954980352.000000, mean_q: 241161.531250\n",
      "    141/500000: episode: 141, duration: 0.174s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47867892858880.000000, mean_q: 242466.609375\n",
      "    141/500000: episode: 141, duration: 0.175s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47867892858880.000000, mean_q: 242466.609375\n",
      "    142/500000: episode: 142, duration: 0.279s, episode steps: 1, steps per second: 4, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799523943090225152.000000, mean_q: 243794.984375\n",
      "    142/500000: episode: 142, duration: 0.281s, episode steps: 1, steps per second: 4, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799523943090225152.000000, mean_q: 243794.984375\n",
      "    143/500000: episode: 143, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47841883979776.000000, mean_q: 245163.515625\n",
      "    143/500000: episode: 143, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47841883979776.000000, mean_q: 245163.515625\n",
      "    144/500000: episode: 144, duration: 0.188s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799523943090225152.000000, mean_q: 246482.906250\n",
      "    144/500000: episode: 144, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799523943090225152.000000, mean_q: 246482.906250\n",
      "    145/500000: episode: 145, duration: 0.176s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817599047886180450304.000000, mean_q: 247814.375000\n",
      "    145/500000: episode: 145, duration: 0.177s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817599047886180450304.000000, mean_q: 247814.375000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    146/500000: episode: 146, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799523943090225152.000000, mean_q: 249187.875000\n",
      "    146/500000: episode: 146, duration: 0.187s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799523943090225152.000000, mean_q: 249187.875000\n",
      "    147/500000: episode: 147, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47790101102592.000000, mean_q: 250557.156250\n",
      "    147/500000: episode: 147, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47790101102592.000000, mean_q: 250557.156250\n",
      "    148/500000: episode: 148, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799383205601869824.000000, mean_q: 251898.515625\n",
      "    148/500000: episode: 148, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799383205601869824.000000, mean_q: 251898.515625\n",
      "    149/500000: episode: 149, duration: 0.180s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799383205601869824.000000, mean_q: 253245.734375\n",
      "    149/500000: episode: 149, duration: 0.181s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799383205601869824.000000, mean_q: 253245.734375\n",
      "    150/500000: episode: 150, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47486450270208.000000, mean_q: 254596.406250\n",
      "    150/500000: episode: 150, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47486450270208.000000, mean_q: 254596.406250\n",
      "    151/500000: episode: 151, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47473456316416.000000, mean_q: 255928.531250\n",
      "    151/500000: episode: 151, duration: 0.196s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47473456316416.000000, mean_q: 255928.531250\n",
      "    152/500000: episode: 152, duration: 0.197s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47460625940480.000000, mean_q: 257245.890625\n",
      "    152/500000: episode: 152, duration: 0.198s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47460625940480.000000, mean_q: 257245.890625\n",
      "    153/500000: episode: 153, duration: 0.203s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47447896227840.000000, mean_q: 258553.015625\n",
      "    153/500000: episode: 153, duration: 0.203s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47447896227840.000000, mean_q: 258553.015625\n",
      "    154/500000: episode: 154, duration: 0.210s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47568348250112.000000, mean_q: 259845.484375\n",
      "    154/500000: episode: 154, duration: 0.211s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47568348250112.000000, mean_q: 259845.484375\n",
      "    155/500000: episode: 155, duration: 0.182s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598766411203739648.000000, mean_q: 261127.468750\n",
      "    155/500000: episode: 155, duration: 0.183s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598766411203739648.000000, mean_q: 261127.468750\n",
      "    156/500000: episode: 156, duration: 0.178s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799383205601869824.000000, mean_q: 262510.031250\n",
      "    156/500000: episode: 156, duration: 0.179s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799383205601869824.000000, mean_q: 262510.031250\n",
      "    157/500000: episode: 157, duration: 0.175s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799242468113514496.000000, mean_q: 263928.906250\n",
      "    157/500000: episode: 157, duration: 0.179s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799242468113514496.000000, mean_q: 263928.906250\n",
      "    158/500000: episode: 158, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598484936227028992.000000, mean_q: 265335.250000\n",
      "    158/500000: episode: 158, duration: 0.197s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598484936227028992.000000, mean_q: 265335.250000\n",
      "    159/500000: episode: 159, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47501608484864.000000, mean_q: 266753.031250\n",
      "    159/500000: episode: 159, duration: 0.190s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47501608484864.000000, mean_q: 266753.031250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    160/500000: episode: 160, duration: 0.175s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47488664862720.000000, mean_q: 268094.656250\n",
      "    160/500000: episode: 160, duration: 0.176s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47488664862720.000000, mean_q: 268094.656250\n",
      "    161/500000: episode: 161, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799242468113514496.000000, mean_q: 269372.156250\n",
      "    161/500000: episode: 161, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799242468113514496.000000, mean_q: 269372.156250\n",
      "    162/500000: episode: 162, duration: 0.191s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47330208251904.000000, mean_q: 270640.906250\n",
      "    162/500000: episode: 162, duration: 0.192s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47330208251904.000000, mean_q: 270640.906250\n",
      "    163/500000: episode: 163, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47318388703232.000000, mean_q: 271857.375000\n",
      "    163/500000: episode: 163, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47318388703232.000000, mean_q: 271857.375000\n",
      "    164/500000: episode: 164, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47977125117952.000000, mean_q: 273032.625000\n",
      "    164/500000: episode: 164, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47977125117952.000000, mean_q: 273032.625000\n",
      "    165/500000: episode: 165, duration: 0.179s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799242468113514496.000000, mean_q: 274161.375000\n",
      "    165/500000: episode: 165, duration: 0.180s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799242468113514496.000000, mean_q: 274161.375000\n",
      "    166/500000: episode: 166, duration: 0.178s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799242468113514496.000000, mean_q: 275352.281250\n",
      "    166/500000: episode: 166, duration: 0.179s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799242468113514496.000000, mean_q: 275352.281250\n",
      "    167/500000: episode: 167, duration: 0.175s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47406028685312.000000, mean_q: 276617.718750\n",
      "    167/500000: episode: 167, duration: 0.176s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47406028685312.000000, mean_q: 276617.718750\n",
      "    168/500000: episode: 168, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799172099369336832.000000, mean_q: 277882.781250\n",
      "    168/500000: episode: 168, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799172099369336832.000000, mean_q: 277882.781250\n",
      "    169/500000: episode: 169, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598344198738673664.000000, mean_q: 279167.656250\n",
      "    169/500000: episode: 169, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598344198738673664.000000, mean_q: 279167.656250\n",
      "    170/500000: episode: 170, duration: 0.178s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47234414542848.000000, mean_q: 280491.843750\n",
      "    170/500000: episode: 170, duration: 0.179s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47234414542848.000000, mean_q: 280491.843750\n",
      "    171/500000: episode: 171, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47355822866432.000000, mean_q: 281785.281250\n",
      "    171/500000: episode: 171, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47355822866432.000000, mean_q: 281785.281250\n",
      "    172/500000: episode: 172, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799172099369336832.000000, mean_q: 283073.781250\n",
      "    172/500000: episode: 172, duration: 0.196s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799172099369336832.000000, mean_q: 283073.781250\n",
      "    173/500000: episode: 173, duration: 0.174s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47464857993216.000000, mean_q: 284378.468750\n",
      "    173/500000: episode: 173, duration: 0.175s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47464857993216.000000, mean_q: 284378.468750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    174/500000: episode: 174, duration: 0.172s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47452602236928.000000, mean_q: 285648.093750\n",
      "    174/500000: episode: 174, duration: 0.174s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47452602236928.000000, mean_q: 285648.093750\n",
      "    175/500000: episode: 175, duration: 0.172s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47306204250112.000000, mean_q: 286914.093750\n",
      "    175/500000: episode: 175, duration: 0.173s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47306204250112.000000, mean_q: 286914.093750\n",
      "    176/500000: episode: 176, duration: 0.201s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598344198738673664.000000, mean_q: 288155.500000\n",
      "    176/500000: episode: 176, duration: 0.202s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598344198738673664.000000, mean_q: 288155.500000\n",
      "    177/500000: episode: 177, duration: 0.188s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598344198738673664.000000, mean_q: 289454.500000\n",
      "    177/500000: episode: 177, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598344198738673664.000000, mean_q: 289454.500000\n",
      "    178/500000: episode: 178, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598062723761963008.000000, mean_q: 290824.781250\n",
      "    178/500000: episode: 178, duration: 0.188s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598062723761963008.000000, mean_q: 290824.781250\n",
      "    179/500000: episode: 179, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799031361880981504.000000, mean_q: 292238.281250\n",
      "    179/500000: episode: 179, duration: 0.190s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799031361880981504.000000, mean_q: 292238.281250\n",
      "    180/500000: episode: 180, duration: 0.223s, episode steps: 1, steps per second: 4, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47106861563904.000000, mean_q: 293625.218750\n",
      "    180/500000: episode: 180, duration: 0.224s, episode steps: 1, steps per second: 4, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47106861563904.000000, mean_q: 293625.218750\n",
      "    181/500000: episode: 181, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799031361880981504.000000, mean_q: 294943.156250\n",
      "    181/500000: episode: 181, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799031361880981504.000000, mean_q: 294943.156250\n",
      "    182/500000: episode: 182, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47081133703168.000000, mean_q: 296275.093750\n",
      "    182/500000: episode: 182, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47081133703168.000000, mean_q: 296275.093750\n",
      "    183/500000: episode: 183, duration: 0.208s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799031361880981504.000000, mean_q: 297598.468750\n",
      "    183/500000: episode: 183, duration: 0.209s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908799031361880981504.000000, mean_q: 297598.468750\n",
      "    184/500000: episode: 184, duration: 0.181s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47190433071104.000000, mean_q: 298935.656250\n",
      "    184/500000: episode: 184, duration: 0.182s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47190433071104.000000, mean_q: 298935.656250\n",
      "    185/500000: episode: 185, duration: 0.182s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598062723761963008.000000, mean_q: 300260.531250\n",
      "    185/500000: episode: 185, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817598062723761963008.000000, mean_q: 300260.531250\n",
      "    186/500000: episode: 186, duration: 0.181s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47163866349568.000000, mean_q: 301697.968750\n",
      "    186/500000: episode: 186, duration: 0.183s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47163866349568.000000, mean_q: 301697.968750\n",
      "    187/500000: episode: 187, duration: 0.196s, episode steps: 1, steps per second: 5, episode reward: 241170548069.919, mean reward: 241170548069.919 [241170548069.919, 241170548069.919], mean action: 2.000 [2.000, 2.000], mean observation: 0.500 [0.000, 1.000], loss: 47150587183104.000000, mean_q: 303080.968750\n",
      "    187/500000: episode: 187, duration: 0.197s, episode steps: 1, steps per second: 5, episode reward: 241170548069.919, mean reward: 241170548069.919 [241170548069.919, 241170548069.919], mean action: 2.000 [2.000, 2.000], mean observation: 0.500 [0.000, 1.000], loss: 47150587183104.000000, mean_q: 303080.968750\n",
      "    188/500000: episode: 188, duration: 0.177s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47137995882496.000000, mean_q: 304393.093750\n",
      "    188/500000: episode: 188, duration: 0.179s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47137995882496.000000, mean_q: 304393.093750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    189/500000: episode: 189, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: 1817597781248785252352.000000, mean_q: 305647.531250\n",
      "    189/500000: episode: 189, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: 1817597781248785252352.000000, mean_q: 305647.531250\n",
      "    190/500000: episode: 190, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798890624392626176.000000, mean_q: 307004.718750\n",
      "    190/500000: episode: 190, duration: 0.187s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798890624392626176.000000, mean_q: 307004.718750\n",
      "    191/500000: episode: 191, duration: 0.196s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817597781248785252352.000000, mean_q: 308389.718750\n",
      "    191/500000: episode: 191, duration: 0.198s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817597781248785252352.000000, mean_q: 308389.718750\n",
      "    192/500000: episode: 192, duration: 0.187s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798890624392626176.000000, mean_q: 309828.250000\n",
      "    192/500000: episode: 192, duration: 0.188s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798890624392626176.000000, mean_q: 309828.250000\n",
      "    193/500000: episode: 193, duration: 0.175s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47207524859904.000000, mean_q: 311311.031250\n",
      "    193/500000: episode: 193, duration: 0.176s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47207524859904.000000, mean_q: 311311.031250\n",
      "    194/500000: episode: 194, duration: 0.197s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798890624392626176.000000, mean_q: 312749.281250\n",
      "    194/500000: episode: 194, duration: 0.199s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798890624392626176.000000, mean_q: 312749.281250\n",
      "    195/500000: episode: 195, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46907783118848.000000, mean_q: 314157.375000\n",
      "    195/500000: episode: 195, duration: 0.191s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46907783118848.000000, mean_q: 314157.375000\n",
      "    196/500000: episode: 196, duration: 0.212s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46894336180224.000000, mean_q: 315545.125000\n",
      "    196/500000: episode: 196, duration: 0.213s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46894336180224.000000, mean_q: 315545.125000\n",
      "    197/500000: episode: 197, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47017602580480.000000, mean_q: 316910.218750\n",
      "    197/500000: episode: 197, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47017602580480.000000, mean_q: 316910.218750\n",
      "    198/500000: episode: 198, duration: 0.182s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817597499773808541696.000000, mean_q: 318207.031250\n",
      "    198/500000: episode: 198, duration: 0.183s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817597499773808541696.000000, mean_q: 318207.031250\n",
      "    199/500000: episode: 199, duration: 0.187s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46992243818496.000000, mean_q: 319550.031250\n",
      "    199/500000: episode: 199, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46992243818496.000000, mean_q: 319550.031250\n",
      "    200/500000: episode: 200, duration: 0.190s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46842914013184.000000, mean_q: 320856.375000\n",
      "    200/500000: episode: 200, duration: 0.191s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46842914013184.000000, mean_q: 320856.375000\n",
      "    201/500000: episode: 201, duration: 0.176s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: 908798749886904270848.000000, mean_q: 322188.906250\n",
      "    201/500000: episode: 201, duration: 0.176s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: 908798749886904270848.000000, mean_q: 322188.906250\n",
      "    202/500000: episode: 202, duration: 0.206s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798749886904270848.000000, mean_q: 323562.968750\n",
      "    202/500000: episode: 202, duration: 0.207s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798749886904270848.000000, mean_q: 323562.968750\n",
      "    203/500000: episode: 203, duration: 0.192s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798749886904270848.000000, mean_q: 324924.906250\n",
      "    203/500000: episode: 203, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798749886904270848.000000, mean_q: 324924.906250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    204/500000: episode: 204, duration: 0.204s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46790535544832.000000, mean_q: 326268.531250\n",
      "    204/500000: episode: 204, duration: 0.205s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46790535544832.000000, mean_q: 326268.531250\n",
      "    205/500000: episode: 205, duration: 0.191s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798679518160093184.000000, mean_q: 327551.406250\n",
      "    205/500000: episode: 205, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798679518160093184.000000, mean_q: 327551.406250\n",
      "    206/500000: episode: 206, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47315314278400.000000, mean_q: 328893.218750\n",
      "    206/500000: episode: 206, duration: 0.196s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 47315314278400.000000, mean_q: 328893.218750\n",
      "    207/500000: episode: 207, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46752124108800.000000, mean_q: 330240.781250\n",
      "    207/500000: episode: 207, duration: 0.187s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46752124108800.000000, mean_q: 330240.781250\n",
      "    208/500000: episode: 208, duration: 0.183s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46876598468608.000000, mean_q: 331609.218750\n",
      "    208/500000: episode: 208, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46876598468608.000000, mean_q: 331609.218750\n",
      "    209/500000: episode: 209, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46725288951808.000000, mean_q: 333016.218750\n",
      "    209/500000: episode: 209, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46725288951808.000000, mean_q: 333016.218750\n",
      "    210/500000: episode: 210, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46711867179008.000000, mean_q: 334403.906250\n",
      "    210/500000: episode: 210, duration: 0.196s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46711867179008.000000, mean_q: 334403.906250\n",
      "    211/500000: episode: 211, duration: 0.178s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798679518160093184.000000, mean_q: 335727.281250\n",
      "    211/500000: episode: 211, duration: 0.179s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798679518160093184.000000, mean_q: 335727.281250\n",
      "    212/500000: episode: 212, duration: 0.182s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798679518160093184.000000, mean_q: 337050.093750\n",
      "    212/500000: episode: 212, duration: 0.183s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798679518160093184.000000, mean_q: 337050.093750\n",
      "    213/500000: episode: 213, duration: 0.179s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46812077490176.000000, mean_q: 338372.968750\n",
      "    213/500000: episode: 213, duration: 0.181s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46812077490176.000000, mean_q: 338372.968750\n",
      "    214/500000: episode: 214, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798679518160093184.000000, mean_q: 339640.375000\n",
      "    214/500000: episode: 214, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798679518160093184.000000, mean_q: 339640.375000\n",
      "    215/500000: episode: 215, duration: 0.199s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817597359036320186368.000000, mean_q: 340943.343750\n",
      "    215/500000: episode: 215, duration: 0.200s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817597359036320186368.000000, mean_q: 340943.343750\n",
      "    216/500000: episode: 216, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798538780671737856.000000, mean_q: 342442.406250\n",
      "    216/500000: episode: 216, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798538780671737856.000000, mean_q: 342442.406250\n",
      "    217/500000: episode: 217, duration: 0.176s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46897540628480.000000, mean_q: 344007.531250\n",
      "    217/500000: episode: 217, duration: 0.177s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46897540628480.000000, mean_q: 344007.531250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    218/500000: episode: 218, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798538780671737856.000000, mean_q: 345497.031250\n",
      "    218/500000: episode: 218, duration: 0.197s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798538780671737856.000000, mean_q: 345497.031250\n",
      "    219/500000: episode: 219, duration: 0.174s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 2726395616342015213568.000000, mean_q: 347001.093750\n",
      "    219/500000: episode: 219, duration: 0.175s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 2726395616342015213568.000000, mean_q: 347001.093750\n",
      "    220/500000: episode: 220, duration: 0.192s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46714278903808.000000, mean_q: 348607.218750\n",
      "    220/500000: episode: 220, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46714278903808.000000, mean_q: 348607.218750\n",
      "    221/500000: episode: 221, duration: 0.178s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798538780671737856.000000, mean_q: 350130.562500\n",
      "    221/500000: episode: 221, duration: 0.180s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798538780671737856.000000, mean_q: 350130.562500\n",
      "    222/500000: episode: 222, duration: 0.192s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798398043183382528.000000, mean_q: 351607.687500\n",
      "    222/500000: episode: 222, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798398043183382528.000000, mean_q: 351607.687500\n",
      "    223/500000: episode: 223, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817596796086366765056.000000, mean_q: 353047.718750\n",
      "    223/500000: episode: 223, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817596796086366765056.000000, mean_q: 353047.718750\n",
      "    224/500000: episode: 224, duration: 0.176s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798398043183382528.000000, mean_q: 354536.468750\n",
      "    224/500000: episode: 224, duration: 0.178s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798398043183382528.000000, mean_q: 354536.468750\n",
      "    225/500000: episode: 225, duration: 0.182s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798398043183382528.000000, mean_q: 356040.625000\n",
      "    225/500000: episode: 225, duration: 0.183s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798398043183382528.000000, mean_q: 356040.625000\n",
      "    226/500000: episode: 226, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798398043183382528.000000, mean_q: 357524.875000\n",
      "    226/500000: episode: 226, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798398043183382528.000000, mean_q: 357524.875000\n",
      "    227/500000: episode: 227, duration: 0.181s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798398043183382528.000000, mean_q: 359029.500000\n",
      "    227/500000: episode: 227, duration: 0.182s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798398043183382528.000000, mean_q: 359029.500000\n",
      "    228/500000: episode: 228, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46459613347840.000000, mean_q: 360538.125000\n",
      "    228/500000: episode: 228, duration: 0.195s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46459613347840.000000, mean_q: 360538.125000\n",
      "    229/500000: episode: 229, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798398043183382528.000000, mean_q: 361957.218750\n",
      "    229/500000: episode: 229, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798398043183382528.000000, mean_q: 361957.218750\n",
      "    230/500000: episode: 230, duration: 0.191s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46431905775616.000000, mean_q: 363412.875000\n",
      "    230/500000: episode: 230, duration: 0.192s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46431905775616.000000, mean_q: 363412.875000\n",
      "    231/500000: episode: 231, duration: 0.181s, episode steps: 1, steps per second: 6, episode reward: 241170548069.919, mean reward: 241170548069.919 [241170548069.919, 241170548069.919], mean action: 2.000 [2.000, 2.000], mean observation: 0.500 [0.000, 1.000], loss: 46418156847104.000000, mean_q: 364840.468750\n",
      "    231/500000: episode: 231, duration: 0.182s, episode steps: 1, steps per second: 6, episode reward: 241170548069.919, mean reward: 241170548069.919 [241170548069.919, 241170548069.919], mean action: 2.000 [2.000, 2.000], mean observation: 0.500 [0.000, 1.000], loss: 46418156847104.000000, mean_q: 364840.468750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    232/500000: episode: 232, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46686139318272.000000, mean_q: 366198.000000\n",
      "    232/500000: episode: 232, duration: 0.188s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46686139318272.000000, mean_q: 366198.000000\n",
      "    233/500000: episode: 233, duration: 0.192s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798257305695027200.000000, mean_q: 367492.218750\n",
      "    233/500000: episode: 233, duration: 0.193s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798257305695027200.000000, mean_q: 367492.218750\n",
      "    234/500000: episode: 234, duration: 0.182s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817596514611390054400.000000, mean_q: 368819.250000\n",
      "    234/500000: episode: 234, duration: 0.183s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 1817596514611390054400.000000, mean_q: 368819.250000\n",
      "    235/500000: episode: 235, duration: 0.181s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798257305695027200.000000, mean_q: 370225.250000\n",
      "    235/500000: episode: 235, duration: 0.182s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798257305695027200.000000, mean_q: 370225.250000\n",
      "    236/500000: episode: 236, duration: 0.190s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46352868311040.000000, mean_q: 371618.000000\n",
      "    236/500000: episode: 236, duration: 0.194s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46352868311040.000000, mean_q: 371618.000000\n",
      "    237/500000: episode: 237, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46763327094784.000000, mean_q: 372976.125000\n",
      "    237/500000: episode: 237, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46763327094784.000000, mean_q: 372976.125000\n",
      "    238/500000: episode: 238, duration: 0.189s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46327106895872.000000, mean_q: 374294.062500\n",
      "    238/500000: episode: 238, duration: 0.190s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46327106895872.000000, mean_q: 374294.062500\n",
      "    239/500000: episode: 239, duration: 0.190s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798257305695027200.000000, mean_q: 375590.281250\n",
      "    239/500000: episode: 239, duration: 0.191s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798257305695027200.000000, mean_q: 375590.281250\n",
      "    240/500000: episode: 240, duration: 0.190s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: 46301861380096.000000, mean_q: 376917.218750\n",
      "    240/500000: episode: 240, duration: 0.192s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: 46301861380096.000000, mean_q: 376917.218750\n",
      "    241/500000: episode: 241, duration: 0.181s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46430718787584.000000, mean_q: 378249.375000\n",
      "    241/500000: episode: 241, duration: 0.182s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46430718787584.000000, mean_q: 378249.375000\n",
      "    242/500000: episode: 242, duration: 0.191s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46559567806464.000000, mean_q: 379605.468750\n",
      "    242/500000: episode: 242, duration: 0.192s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46559567806464.000000, mean_q: 379605.468750\n",
      "    243/500000: episode: 243, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798186936950849536.000000, mean_q: 380925.937500\n",
      "    243/500000: episode: 243, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798186936950849536.000000, mean_q: 380925.937500\n",
      "    244/500000: episode: 244, duration: 0.219s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798186936950849536.000000, mean_q: 382253.250000\n",
      "    244/500000: episode: 244, duration: 0.220s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798186936950849536.000000, mean_q: 382253.250000\n",
      "    245/500000: episode: 245, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798186936950849536.000000, mean_q: 383613.531250\n",
      "    245/500000: episode: 245, duration: 0.187s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798186936950849536.000000, mean_q: 383613.531250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    246/500000: episode: 246, duration: 0.182s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46650970079232.000000, mean_q: 384996.531250\n",
      "    246/500000: episode: 246, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46650970079232.000000, mean_q: 384996.531250\n",
      "    247/500000: episode: 247, duration: 0.182s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46211562209280.000000, mean_q: 386306.468750\n",
      "    247/500000: episode: 247, duration: 0.183s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46211562209280.000000, mean_q: 386306.468750\n",
      "    248/500000: episode: 248, duration: 0.186s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: 46626731196416.000000, mean_q: 387567.250000\n",
      "    248/500000: episode: 248, duration: 0.187s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.500 [0.000, 1.000], loss: 46626731196416.000000, mean_q: 387567.250000\n",
      "    249/500000: episode: 249, duration: 0.181s, episode steps: 1, steps per second: 6, episode reward: 241170548069.919, mean reward: 241170548069.919 [241170548069.919, 241170548069.919], mean action: 2.000 [2.000, 2.000], mean observation: 0.500 [0.000, 1.000], loss: 908798186936950849536.000000, mean_q: 388772.531250\n",
      "    249/500000: episode: 249, duration: 0.182s, episode steps: 1, steps per second: 6, episode reward: 241170548069.919, mean reward: 241170548069.919 [241170548069.919, 241170548069.919], mean action: 2.000 [2.000, 2.000], mean observation: 0.500 [0.000, 1.000], loss: 908798186936950849536.000000, mean_q: 388772.531250\n",
      "    250/500000: episode: 250, duration: 0.181s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46175734464512.000000, mean_q: 390032.812500\n",
      "    250/500000: episode: 250, duration: 0.182s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46175734464512.000000, mean_q: 390032.812500\n",
      "    251/500000: episode: 251, duration: 0.184s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46305929854976.000000, mean_q: 391312.718750\n",
      "    251/500000: episode: 251, duration: 0.185s, episode steps: 1, steps per second: 5, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 46305929854976.000000, mean_q: 391312.718750\n",
      "    252/500000: episode: 252, duration: 0.177s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798186936950849536.000000, mean_q: 392575.875000\n",
      "    252/500000: episode: 252, duration: 0.178s, episode steps: 1, steps per second: 6, episode reward: 10000000.000, mean reward: 10000000.000 [10000000.000, 10000000.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 908798186936950849536.000000, mean_q: 392575.875000\n",
      "done, took 31.181 seconds\n",
      "done, took 31.180 seconds\n"
     ]
    }
   ],
   "source": [
    "is_to_train = True\n",
    "if is_to_train:\n",
    "    dfx.train(is_for_time_measurement=True)\n",
    "else:\n",
    "    dfx.test(1, [EpisodeLogger()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepfx_logger.critical('DeepFX Finished: %s' % DebugTools.now_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "if os.environ.get('SLACK_WEBHOOK_URL') and os.environ.get('GOOGLE_STACKDRIVER_URL'):\n",
    "    google_stackdriver_url = os.environ.get('GOOGLE_STACKDRIVER_URL')\n",
    "    payload = '{\"username\":\"deepfx\",\"icon_emoji\":\":+1:\",\"channel\":\"deepfx\",\"attachments\":[{\"color\":\"#36a64f\",\"title\":\"DeepFX Finished\",\"title_link\":\"%s\",\"text\":\"<@%s> DeepFX Finished\"}]}' % (google_stackdriver_url, os.environ.get('SLACK_NOTIFY_RECIEVE_USER'))\n",
    "    command = ['curl']\n",
    "    command.append('-XPOST')\n",
    "    command.append('-HContent-Type: application/json')\n",
    "    command.append(\"-d%s\" % payload)\n",
    "    command.append(os.environ.get('SLACK_WEBHOOK_URL'))\n",
    "    print(command)\n",
    "    subprocess.run(command)\n",
    "else:\n",
    "    print('Skipped Slack Notification.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "data = hd.data()['Close']\n",
    "x = data.index\n",
    "y = data.values\n",
    "sd = 1\n",
    "upper, middle, lower = talib.BBANDS(data.values, timeperiod=20, matype=talib.MA_Type.SMA, nbdevup=sd, nbdevdn=sd)\n",
    "[plt.plot(x, val) for val in [y, upper, middle, lower]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Deep Q-LearningでFXしてみた](http://recruit.gmo.jp/engineer/jisedai/blog/deep-q-learning/)\n",
    "- [slide](https://www.slideshare.net/JunichiroKatsuta/deep-qlearningfx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "足の配列について、indexの外を読み出そうとしている節があるので直す。\n",
    "\n",
    "```json\n",
    "{\n",
    " insertId:  \"1l630l2g1k8tnms\"  \n",
    " jsonPayload: {…}  \n",
    " logName:  \"projects/deep-fx/logs/deepfx\"  \n",
    " receiveTimestamp:  \"2017-11-18T17:12:18.459939016Z\"  \n",
    " resource: {…}  \n",
    " severity:  \"WARNING\"  \n",
    " timestamp:  \"2017-11-18T17:12:18.459939016Z\"  \n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
