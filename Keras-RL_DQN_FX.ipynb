{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Python] Keras-RLで簡単に強化学習(DQN)を試す](http://qiita.com/inoory/items/e63ade6f21766c7c2393)を試した。FXに適用していく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行時に、以下の警告ログが出た。TensorFlowを高速化できるようである。\n",
    "\n",
    "[How to compile Tensorflow with SSE4.2 and AVX instructions?](http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions)\n",
    "\n",
    "```\n",
    "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
    "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑これを試したが、逆に学習時間が増えてしまった。一応、その時の `Dockerfile` を残しておく。\n",
    "\n",
    "```\n",
    "FROM jupyter/datascience-notebook\n",
    "\n",
    "USER root\n",
    "RUN apt-get update\n",
    "RUN apt-get -y install python-pip python-dev\n",
    "RUN cd /tmp && wget 'http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz' &&\\\n",
    "        tar -xzvf ta-lib-0.4.0-src.tar.gz &&\\\n",
    "        cd ta-lib &&\\\n",
    "        ./configure --prefix=/usr &&\\\n",
    "        make &&\\\n",
    "        make install\n",
    "\n",
    "RUN apt-get -y install software-properties-common python-software-properties\n",
    "RUN echo \"oracle-java7-installer shared/accepted-oracle-license-v1-1 boolean true\" | debconf-set-selections\n",
    "RUN echo \"deb http://ppa.launchpad.net/webupd8team/java/ubuntu precise main\\ndeb-src http://ppa.launchpad.net/webupd8team/java/ubuntu precise main\" >> /etc/apt/sources.list.d/java.list &&\\\n",
    "        apt-key adv --keyserver keyserver.ubuntu.com --recv-keys EEA14886 &&\\\n",
    "        apt-get update &&\\\n",
    "        apt-get -y install oracle-java8-installer\n",
    "\n",
    "RUN echo \"deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list &&\\\n",
    "        curl https://bazel.build/bazel-release.pub.gpg | apt-key add - &&\\\n",
    "        apt-get update &&\\\n",
    "        apt-get -y install bazel\n",
    "\n",
    "RUN cd /tmp &&\\\n",
    "        git clone https://github.com/tensorflow/tensorflow && \\\n",
    "        cd tensorflow &&\\\n",
    "        git checkout master \n",
    "ENV PYTHON_BIN_PATH /opt/conda/bin/python\n",
    "ENV TF_NEED_JEMALLOC 1\n",
    "ENV CC_OPT_FLAGS -march=native\n",
    "ENV TF_NEED_GCP 0\n",
    "ENV TF_NEED_HDFS 0\n",
    "ENV TF_ENABLE_XLA 0\n",
    "ENV TF_NEED_OPENCL 0\n",
    "ENV TF_NEED_CUDA 0\n",
    "ENV GCC_HOST_COMPILER_PATH /usr/bin/gcc\n",
    "ENV TF_CUDA_VERSION 8.0\n",
    "ENV CUDA_TOOLKIT_PATH /usr/local/cuda\n",
    "ENV TF_CUDNN_VERSION 5\n",
    "ENV CUDNN_INSTALL_PATH /usr/local/cuda\n",
    "ENV TF_CUDA_COMPUTE_CAPABILITIES 3.0\n",
    "ENV USE_DEFAULT_PYTHON_LIB_PATH 1\n",
    "RUN cd /tmp/tensorflow &&\\\n",
    "        ./configure\n",
    "RUN cd /tmp/tensorflow &&\\\n",
    "        bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma \\\n",
    "        --copt=-mfpmath=both --copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package\n",
    "RUN cd /tmp/tensorflow &&\\\n",
    "        bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n",
    "RUN pip install /tmp/tensorflow_pkg/tensorflow-1.0.1-py2-none-any.whl\n",
    "CMD /bin/bash\n",
    "\n",
    "USER jovyan\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install backtrader numpy scipy xgboost TA-Lib pandas tensorflow keras sklearn gym keras-rl\n",
    "RUN python -c \"import matplotlib\"\n",
    "\n",
    "CMD start-notebook.sh --NotebookApp.password='sha1:3e3088c87cd2:56025176c283fc9bb10b026a85eabff47facfc56'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CSVからデータを読み込み、DataFrameを作成する。\n",
    "# HSTデータをここからDL: http://mechanicalforex.com/2015/12/converting-mt4-binary-history-files-hst-to-csv-using-a-python-script.html\n",
    "# HSTデータをCSVに変換するスクリプトはここ：http://mechanicalforex.com/2015/12/converting-mt4-binary-history-files-hst-to-csv-using-a-python-script.html\n",
    "class HistData:\n",
    "    def __init__(self):\n",
    "        self.csv_path = 'USDJPY.hst_.csv'\n",
    "        self.csv_data = pd.read_csv(self.csv_path, index_col=0, parse_dates=True, header=0)\n",
    "    def data(self):\n",
    "        return self.csv_data\n",
    "    def max_value(self):\n",
    "        return self.csv_data[['High']].max()\n",
    "    def min_value(self):\n",
    "        return self.csv_data[['Low']].min()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = HistData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "High    125.845\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.max_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Low    75.56\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.min_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WIP\n",
    "# 売り、買い、そのままのうちどれかを選択し、利益を最大化することを目標とする環境\n",
    "class PointOnLine(gym.core.Env):\n",
    "    def __init__(hist_data, self):\n",
    "        self.action_space = gym.spaces.Discrete(3) # 行動空間。売る、そのまま、買うの3種\n",
    "\n",
    "        high = np.array([1.0, 1.0]) # 観測空間(state)の次元 (位置と速度の2次元) とそれらの最大値\n",
    "        self.observation_space = gym.spaces.Box(low=hist_data.min_value(), high=hist_data.max_value()) # 最小値は、最大値のマイナスがけ\n",
    "\n",
    "    # 各stepごとに呼ばれる\n",
    "    # actionを受け取り、次のstateとreward、episodeが終了したかどうかを返すように実装\n",
    "    def _step(self, action):\n",
    "        # actionを受け取り、次のstateを決定\n",
    "        dt = 0.1\n",
    "        acc = (action - 1) * 0.1\n",
    "        self._vel += acc * dt\n",
    "        self._vel = max(-1.0,  min(self._vel, 1.0))\n",
    "        self._pos += self._vel * dt\n",
    "        self._pos = max(-1.0,  min(self._pos, 1.0))\n",
    "\n",
    "        # 位置と速度の絶対値が十分小さくなったらepisode終了\n",
    "        done = abs(self._pos) < 0.1 and abs(self._vel) < 0.1\n",
    "\n",
    "        if done:\n",
    "            # 終了したときに正の報酬\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            # 時間経過ごとに負の報酬\n",
    "            # ゴールに近づくように、距離が近くなるほど絶対値を減らしておくと、学習が早く進む\n",
    "            reward = -0.01 * abs(self._pos)\n",
    "\n",
    "        # 次のstate、reward、終了したかどうか、追加情報の順に返す\n",
    "        # 追加情報は特にないので空dict\n",
    "        return np.array([self._pos, self._vel]), reward, done, {}\n",
    "\n",
    "    # 各episodeの開始時に呼ばれ、初期stateを返すように実装\n",
    "    def _reset(self):\n",
    "        # 初期stateは、位置はランダム、速度ゼロ\n",
    "        self._pos = np.random.rand()*2 - 1\n",
    "        self._vel = 0.0\n",
    "        return np.array([self._pos, self._vel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 643.0\n",
      "Trainable params: 643.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 50000 steps ...\n",
      "   300/50000: episode: 1, duration: 2.689s, episode steps: 300, steps per second: 112, episode reward: -2.968, mean reward: -0.010 [-0.010, -0.008], mean action: 1.003 [0.000, 2.000], mean observation: 0.520 [-0.020, 1.000], loss: 0.000147, mean_absolute_error: 0.039284, mean_q: 0.068850\n",
      "   600/50000: episode: 2, duration: 2.087s, episode steps: 300, steps per second: 144, episode reward: -2.321, mean reward: -0.008 [-0.010, -0.003], mean action: 1.013 [0.000, 2.000], mean observation: -0.401 [-1.000, 0.220], loss: 0.000053, mean_absolute_error: 0.019491, mean_q: 0.028170\n",
      "   900/50000: episode: 3, duration: 1.766s, episode steps: 300, steps per second: 170, episode reward: -2.804, mean reward: -0.009 [-0.010, -0.004], mean action: 1.053 [0.000, 2.000], mean observation: -0.557 [-1.000, 0.160], loss: 0.000004, mean_absolute_error: 0.011106, mean_q: -0.003775\n",
      "  1200/50000: episode: 4, duration: 2.038s, episode steps: 300, steps per second: 147, episode reward: -2.844, mean reward: -0.009 [-0.010, -0.004], mean action: 0.957 [0.000, 2.000], mean observation: -0.554 [-1.000, -0.010], loss: 0.000002, mean_absolute_error: 0.023334, mean_q: -0.031228\n",
      "  1500/50000: episode: 5, duration: 1.714s, episode steps: 300, steps per second: 175, episode reward: -2.264, mean reward: -0.008 [-0.010, -0.000], mean action: 1.003 [0.000, 2.000], mean observation: -0.394 [-1.000, 0.384], loss: 0.000007, mean_absolute_error: 0.039246, mean_q: -0.053415\n",
      "  1800/50000: episode: 6, duration: 2.099s, episode steps: 300, steps per second: 143, episode reward: -2.517, mean reward: -0.008 [-0.010, -0.000], mean action: 1.007 [0.000, 2.000], mean observation: -0.218 [-1.000, 0.949], loss: 0.000013, mean_absolute_error: 0.052870, mean_q: -0.072952\n",
      "  2100/50000: episode: 7, duration: 2.176s, episode steps: 300, steps per second: 138, episode reward: -2.503, mean reward: -0.008 [-0.010, -0.003], mean action: 1.030 [0.000, 2.000], mean observation: -0.428 [-1.000, 0.190], loss: 0.000011, mean_absolute_error: 0.065811, mean_q: -0.093914\n",
      "  2346/50000: episode: 8, duration: 1.433s, episode steps: 246, steps per second: 172, episode reward: -0.672, mean reward: -0.003 [-0.010, 1.000], mean action: 1.037 [0.000, 2.000], mean observation: -0.336 [-1.000, 0.301], loss: 0.000022, mean_absolute_error: 0.078453, mean_q: -0.111522\n",
      "  2646/50000: episode: 9, duration: 2.031s, episode steps: 300, steps per second: 148, episode reward: -1.748, mean reward: -0.006 [-0.010, -0.002], mean action: 1.023 [0.000, 2.000], mean observation: -0.292 [-1.000, 0.220], loss: 0.000251, mean_absolute_error: 0.087680, mean_q: -0.123846\n",
      "  2812/50000: episode: 10, duration: 1.000s, episode steps: 166, steps per second: 166, episode reward: 0.037, mean reward: 0.000 [-0.010, 1.000], mean action: 1.054 [0.000, 2.000], mean observation: -0.270 [-0.993, 0.332], loss: 0.000137, mean_absolute_error: 0.094260, mean_q: -0.133372\n",
      "  3112/50000: episode: 11, duration: 1.926s, episode steps: 300, steps per second: 156, episode reward: -2.105, mean reward: -0.007 [-0.010, -0.000], mean action: 0.933 [0.000, 2.000], mean observation: -0.261 [-1.000, 0.943], loss: 0.000353, mean_absolute_error: 0.101240, mean_q: -0.142492\n",
      "  3412/50000: episode: 12, duration: 1.912s, episode steps: 300, steps per second: 157, episode reward: -1.396, mean reward: -0.005 [-0.010, -0.002], mean action: 1.040 [0.000, 2.000], mean observation: -0.222 [-0.984, 0.190], loss: 0.000376, mean_absolute_error: 0.108459, mean_q: -0.151446\n",
      "  3466/50000: episode: 13, duration: 0.458s, episode steps: 54, steps per second: 118, episode reward: 0.727, mean reward: 0.013 [-0.009, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.180 [-0.884, 0.230], loss: 0.000043, mean_absolute_error: 0.110844, mean_q: -0.155543\n",
      "  3467/50000: episode: 14, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.028 [-0.010, 0.067], loss: 0.000018, mean_absolute_error: 0.119005, mean_q: -0.168087\n",
      "  3468/50000: episode: 15, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.045 [-0.080, -0.010], loss: 0.000010, mean_absolute_error: 0.111179, mean_q: -0.154877\n",
      "  3768/50000: episode: 16, duration: 1.653s, episode steps: 300, steps per second: 182, episode reward: -1.225, mean reward: -0.004 [-0.008, -0.001], mean action: 1.000 [0.000, 2.000], mean observation: -0.206 [-0.840, 0.160], loss: 0.000863, mean_absolute_error: 0.116461, mean_q: -0.157393\n",
      "  4068/50000: episode: 17, duration: 1.630s, episode steps: 300, steps per second: 184, episode reward: -0.781, mean reward: -0.003 [-0.006, -0.002], mean action: 1.000 [0.000, 2.000], mean observation: -0.124 [-0.566, 0.120], loss: 0.000698, mean_absolute_error: 0.120097, mean_q: -0.160479\n",
      "  4069/50000: episode: 18, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.001 [-0.010, 0.008], loss: 0.000067, mean_absolute_error: 0.109815, mean_q: -0.153109\n",
      "  4369/50000: episode: 19, duration: 1.610s, episode steps: 300, steps per second: 186, episode reward: -0.584, mean reward: -0.002 [-0.002, -0.002], mean action: 1.003 [0.000, 2.000], mean observation: -0.096 [-0.223, 0.030], loss: 0.000452, mean_absolute_error: 0.116782, mean_q: -0.155425\n",
      "  4669/50000: episode: 20, duration: 1.695s, episode steps: 300, steps per second: 177, episode reward: -0.502, mean reward: -0.002 [-0.002, -0.002], mean action: 1.000 [0.000, 2.000], mean observation: -0.082 [-0.242, 0.060], loss: 0.000366, mean_absolute_error: 0.112362, mean_q: -0.146047\n",
      "  4960/50000: episode: 21, duration: 1.526s, episode steps: 291, steps per second: 191, episode reward: -1.184, mean reward: -0.004 [-0.010, 1.000], mean action: 1.031 [0.000, 2.000], mean observation: -0.292 [-1.000, 0.939], loss: 0.000349, mean_absolute_error: 0.116028, mean_q: -0.141800\n",
      "  5260/50000: episode: 22, duration: 1.716s, episode steps: 300, steps per second: 175, episode reward: -0.418, mean reward: -0.001 [-0.002, -0.001], mean action: 0.997 [0.000, 2.000], mean observation: -0.069 [-0.167, 0.020], loss: 0.000294, mean_absolute_error: 0.121317, mean_q: -0.132748\n",
      "  5446/50000: episode: 23, duration: 1.004s, episode steps: 186, steps per second: 185, episode reward: 0.792, mean reward: 0.004 [-0.001, 1.000], mean action: 1.005 [0.000, 2.000], mean observation: -0.056 [-0.123, 0.020], loss: 0.000190, mean_absolute_error: 0.126770, mean_q: -0.112435\n",
      "  5510/50000: episode: 24, duration: 0.336s, episode steps: 64, steps per second: 190, episode reward: 0.919, mean reward: 0.014 [-0.002, 1.000], mean action: 1.016 [0.000, 2.000], mean observation: -0.054 [-0.229, 0.090], loss: 0.000274, mean_absolute_error: 0.129594, mean_q: -0.090940\n",
      "  5522/50000: episode: 25, duration: 0.069s, episode steps: 12, steps per second: 175, episode reward: 0.988, mean reward: 0.082 [-0.001, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.046 [-0.118, 0.030], loss: 0.000033, mean_absolute_error: 0.134523, mean_q: -0.104669\n",
      "  5592/50000: episode: 26, duration: 0.385s, episode steps: 70, steps per second: 182, episode reward: 0.802, mean reward: 0.011 [-0.004, 1.000], mean action: 0.871 [0.000, 2.000], mean observation: 0.110 [-0.140, 0.409], loss: 0.000173, mean_absolute_error: 0.132284, mean_q: -0.089343\n",
      "  5648/50000: episode: 27, duration: 0.297s, episode steps: 56, steps per second: 189, episode reward: 0.871, mean reward: 0.016 [-0.004, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.085 [-0.110, 0.354], loss: 0.000842, mean_absolute_error: 0.136376, mean_q: -0.070562\n",
      "  5672/50000: episode: 28, duration: 0.143s, episode steps: 24, steps per second: 167, episode reward: 0.967, mean reward: 0.040 [-0.002, 1.000], mean action: 1.083 [0.000, 2.000], mean observation: -0.049 [-0.200, 0.080], loss: 0.001004, mean_absolute_error: 0.143858, mean_q: -0.075213\n",
      "  5713/50000: episode: 29, duration: 0.247s, episode steps: 41, steps per second: 166, episode reward: 0.875, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.092 [-0.513, 0.200], loss: 0.001503, mean_absolute_error: 0.143931, mean_q: -0.064086\n",
      "  5765/50000: episode: 30, duration: 0.299s, episode steps: 52, steps per second: 174, episode reward: 0.817, mean reward: 0.016 [-0.006, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.122 [-0.567, 0.210], loss: 0.000465, mean_absolute_error: 0.147334, mean_q: -0.052031\n",
      "  6015/50000: episode: 31, duration: 1.327s, episode steps: 250, steps per second: 188, episode reward: -0.192, mean reward: -0.001 [-0.009, 1.000], mean action: 0.964 [0.000, 2.000], mean observation: -0.017 [-0.872, 0.878], loss: 0.000450, mean_absolute_error: 0.157680, mean_q: -0.032693\n",
      "  6151/50000: episode: 32, duration: 0.713s, episode steps: 136, steps per second: 191, episode reward: 0.511, mean reward: 0.004 [-0.008, 1.000], mean action: 0.934 [0.000, 2.000], mean observation: 0.062 [-0.771, 0.484], loss: 0.000544, mean_absolute_error: 0.175800, mean_q: 0.001717\n",
      "  6214/50000: episode: 33, duration: 0.336s, episode steps: 63, steps per second: 188, episode reward: 0.798, mean reward: 0.013 [-0.005, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.123 [-0.100, 0.520], loss: 0.000205, mean_absolute_error: 0.182031, mean_q: 0.015915\n",
      "  6220/50000: episode: 34, duration: 0.037s, episode steps: 6, steps per second: 164, episode reward: 0.994, mean reward: 0.166 [-0.001, 1.000], mean action: 1.667 [0.000, 2.000], mean observation: -0.039 [-0.117, 0.050], loss: 0.000369, mean_absolute_error: 0.178240, mean_q: -0.000316\n",
      "  6520/50000: episode: 35, duration: 1.581s, episode steps: 300, steps per second: 190, episode reward: -2.783, mean reward: -0.009 [-0.010, -0.004], mean action: 1.497 [0.000, 2.000], mean observation: 0.832 [-0.040, 1.000], loss: 0.000368, mean_absolute_error: 0.197982, mean_q: 0.042870\n",
      "  6570/50000: episode: 36, duration: 0.259s, episode steps: 50, steps per second: 193, episode reward: 0.778, mean reward: 0.016 [-0.008, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.142 [-0.270, 0.805], loss: 0.000209, mean_absolute_error: 0.212764, mean_q: 0.078991\n",
      "  6638/50000: episode: 37, duration: 0.365s, episode steps: 68, steps per second: 186, episode reward: 0.868, mean reward: 0.013 [-0.004, 1.000], mean action: 0.882 [0.000, 2.000], mean observation: 0.024 [-0.380, 0.220], loss: 0.000220, mean_absolute_error: 0.218367, mean_q: 0.084031\n",
      "  6663/50000: episode: 38, duration: 0.131s, episode steps: 25, steps per second: 191, episode reward: 0.972, mean reward: 0.039 [-0.002, 1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.015 [-0.190, 0.140], loss: 0.000318, mean_absolute_error: 0.235375, mean_q: 0.120848\n",
      "  6714/50000: episode: 39, duration: 0.269s, episode steps: 51, steps per second: 189, episode reward: 0.780, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.136 [-0.270, 0.790], loss: 0.000220, mean_absolute_error: 0.228515, mean_q: 0.098741\n",
      "  7014/50000: episode: 40, duration: 1.578s, episode steps: 300, steps per second: 190, episode reward: -2.466, mean reward: -0.008 [-0.010, -0.000], mean action: 1.433 [0.000, 2.000], mean observation: 0.731 [-0.341, 1.000], loss: 0.000314, mean_absolute_error: 0.243667, mean_q: 0.120213\n",
      "  7084/50000: episode: 41, duration: 0.373s, episode steps: 70, steps per second: 188, episode reward: 0.732, mean reward: 0.010 [-0.007, 1.000], mean action: 0.929 [0.000, 2.000], mean observation: 0.148 [-0.140, 0.709], loss: 0.000703, mean_absolute_error: 0.257262, mean_q: 0.151834\n",
      "  7384/50000: episode: 42, duration: 1.559s, episode steps: 300, steps per second: 192, episode reward: -2.624, mean reward: -0.009 [-0.010, -0.000], mean action: 1.400 [0.000, 2.000], mean observation: 0.810 [-0.164, 1.000], loss: 0.000335, mean_absolute_error: 0.273010, mean_q: 0.176438\n",
      "  7385/50000: episode: 43, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.004 [-0.010, 0.002], loss: 0.000805, mean_absolute_error: 0.286086, mean_q: 0.244488\n",
      "  7409/50000: episode: 44, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 0.961, mean reward: 0.040 [-0.002, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.041 [-0.140, 0.243], loss: 0.000202, mean_absolute_error: 0.275029, mean_q: 0.171207\n",
      "  7410/50000: episode: 45, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [-0.010, 0.050], loss: 0.000045, mean_absolute_error: 0.355393, mean_q: 0.366253\n",
      "  7474/50000: episode: 46, duration: 0.344s, episode steps: 64, steps per second: 186, episode reward: 0.718, mean reward: 0.011 [-0.008, 1.000], mean action: 0.859 [0.000, 2.000], mean observation: 0.159 [-0.180, 0.809], loss: 0.000633, mean_absolute_error: 0.279947, mean_q: 0.183401\n",
      "  7513/50000: episode: 47, duration: 0.208s, episode steps: 39, steps per second: 188, episode reward: 0.900, mean reward: 0.023 [-0.004, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.084 [-0.130, 0.407], loss: 0.000763, mean_absolute_error: 0.282441, mean_q: 0.184694\n",
      "  7581/50000: episode: 48, duration: 0.359s, episode steps: 68, steps per second: 190, episode reward: 0.617, mean reward: 0.009 [-0.010, 1.000], mean action: 0.868 [0.000, 2.000], mean observation: 0.215 [-0.190, 0.969], loss: 0.000634, mean_absolute_error: 0.289770, mean_q: 0.216416\n",
      "  7619/50000: episode: 49, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 0.899, mean reward: 0.024 [-0.004, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.089 [-0.150, 0.401], loss: 0.000637, mean_absolute_error: 0.294693, mean_q: 0.229774\n",
      "  7919/50000: episode: 50, duration: 1.592s, episode steps: 300, steps per second: 188, episode reward: -2.279, mean reward: -0.008 [-0.010, -0.000], mean action: 1.413 [0.000, 2.000], mean observation: 0.653 [-0.361, 1.000], loss: 0.000367, mean_absolute_error: 0.302086, mean_q: 0.233139\n",
      "  7920/50000: episode: 51, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.013 [-0.015, -0.010], loss: 0.000070, mean_absolute_error: 0.323882, mean_q: 0.239789\n",
      "  7944/50000: episode: 52, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 0.959, mean reward: 0.040 [-0.003, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.043 [-0.150, 0.259], loss: 0.000058, mean_absolute_error: 0.311999, mean_q: 0.250140\n",
      "  7974/50000: episode: 53, duration: 0.177s, episode steps: 30, steps per second: 169, episode reward: 0.935, mean reward: 0.031 [-0.003, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.066 [-0.324, 0.130], loss: 0.001517, mean_absolute_error: 0.305657, mean_q: 0.219509\n",
      "  8024/50000: episode: 54, duration: 0.264s, episode steps: 50, steps per second: 189, episode reward: 0.796, mean reward: 0.016 [-0.007, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.137 [-0.200, 0.710], loss: 0.000503, mean_absolute_error: 0.311119, mean_q: 0.271131\n",
      "  8032/50000: episode: 55, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 0.993, mean reward: 0.124 [-0.001, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.048 [-0.102, 0.010], loss: 0.000093, mean_absolute_error: 0.313394, mean_q: 0.265175\n",
      "  8086/50000: episode: 56, duration: 0.290s, episode steps: 54, steps per second: 186, episode reward: 0.781, mean reward: 0.014 [-0.007, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.142 [-0.220, 0.662], loss: 0.000472, mean_absolute_error: 0.309873, mean_q: 0.245220\n",
      "  8386/50000: episode: 57, duration: 1.570s, episode steps: 300, steps per second: 191, episode reward: -2.579, mean reward: -0.009 [-0.010, -0.000], mean action: 1.450 [0.000, 2.000], mean observation: 0.671 [-0.870, 1.000], loss: 0.000395, mean_absolute_error: 0.318339, mean_q: 0.263014\n",
      "  8387/50000: episode: 58, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [-0.010, 0.050], loss: 0.000049, mean_absolute_error: 0.350556, mean_q: 0.392740\n",
      "  8687/50000: episode: 59, duration: 1.579s, episode steps: 300, steps per second: 190, episode reward: -2.565, mean reward: -0.009 [-0.010, -0.000], mean action: 1.273 [0.000, 2.000], mean observation: 0.563 [-0.938, 1.000], loss: 0.000458, mean_absolute_error: 0.325594, mean_q: 0.283518\n",
      "  8704/50000: episode: 60, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 0.978, mean reward: 0.058 [-0.002, 1.000], mean action: 1.529 [0.000, 2.000], mean observation: -0.035 [-0.179, 0.110], loss: 0.000732, mean_absolute_error: 0.312084, mean_q: 0.265040\n",
      "  8747/50000: episode: 61, duration: 0.229s, episode steps: 43, steps per second: 188, episode reward: 0.873, mean reward: 0.020 [-0.005, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.085 [-0.200, 0.543], loss: 0.000696, mean_absolute_error: 0.326127, mean_q: 0.287174\n",
      "  8780/50000: episode: 62, duration: 0.177s, episode steps: 33, steps per second: 187, episode reward: 0.898, mean reward: 0.027 [-0.005, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.097 [-0.483, 0.200], loss: 0.000553, mean_absolute_error: 0.320375, mean_q: 0.285053\n",
      "  8804/50000: episode: 63, duration: 0.130s, episode steps: 24, steps per second: 184, episode reward: 0.962, mean reward: 0.040 [-0.002, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.052 [-0.110, 0.236], loss: 0.000118, mean_absolute_error: 0.325431, mean_q: 0.267307\n",
      "  8919/50000: episode: 64, duration: 0.614s, episode steps: 115, steps per second: 187, episode reward: 0.571, mean reward: 0.005 [-0.008, 1.000], mean action: 0.922 [0.000, 2.000], mean observation: -0.001 [-0.846, 0.416], loss: 0.000703, mean_absolute_error: 0.322936, mean_q: 0.285946\n",
      "  8963/50000: episode: 65, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: 0.867, mean reward: 0.020 [-0.005, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.099 [-0.180, 0.484], loss: 0.000601, mean_absolute_error: 0.319782, mean_q: 0.280481\n",
      "  8991/50000: episode: 66, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 0.934, mean reward: 0.033 [-0.003, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.075 [-0.344, 0.160], loss: 0.000274, mean_absolute_error: 0.326216, mean_q: 0.287524\n",
      "  9003/50000: episode: 67, duration: 0.071s, episode steps: 12, steps per second: 168, episode reward: 0.986, mean reward: 0.082 [-0.002, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.041 [-0.151, 0.070], loss: 0.000149, mean_absolute_error: 0.346868, mean_q: 0.337959\n",
      "  9043/50000: episode: 68, duration: 0.218s, episode steps: 40, steps per second: 184, episode reward: 0.893, mean reward: 0.022 [-0.005, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.074 [-0.170, 0.483], loss: 0.000158, mean_absolute_error: 0.336405, mean_q: 0.311778\n",
      "  9108/50000: episode: 69, duration: 0.344s, episode steps: 65, steps per second: 189, episode reward: 0.660, mean reward: 0.010 [-0.009, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.198 [-0.260, 0.887], loss: 0.001093, mean_absolute_error: 0.328668, mean_q: 0.305959\n",
      "  9143/50000: episode: 70, duration: 0.187s, episode steps: 35, steps per second: 188, episode reward: 0.892, mean reward: 0.025 [-0.005, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.092 [-0.504, 0.210], loss: 0.000847, mean_absolute_error: 0.335570, mean_q: 0.313161\n",
      "  9184/50000: episode: 71, duration: 0.228s, episode steps: 41, steps per second: 180, episode reward: 0.854, mean reward: 0.021 [-0.006, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.109 [-0.613, 0.240], loss: 0.000928, mean_absolute_error: 0.338674, mean_q: 0.334839\n",
      "  9185/50000: episode: 72, duration: 0.011s, episode steps: 1, steps per second: 92, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.024 [0.010, 0.037], loss: 0.010078, mean_absolute_error: 0.285186, mean_q: 0.211571\n",
      "  9302/50000: episode: 73, duration: 0.617s, episode steps: 117, steps per second: 190, episode reward: 0.525, mean reward: 0.004 [-0.010, 1.000], mean action: 0.923 [0.000, 2.000], mean observation: -0.029 [-0.982, 0.376], loss: 0.000529, mean_absolute_error: 0.335998, mean_q: 0.327049\n",
      "  9323/50000: episode: 74, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 0.960, mean reward: 0.046 [-0.003, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.058 [-0.269, 0.120], loss: 0.000231, mean_absolute_error: 0.336620, mean_q: 0.321007\n",
      "  9388/50000: episode: 75, duration: 0.368s, episode steps: 65, steps per second: 177, episode reward: 0.634, mean reward: 0.010 [-0.009, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.216 [-0.260, 0.919], loss: 0.000631, mean_absolute_error: 0.341539, mean_q: 0.338170\n",
      "  9425/50000: episode: 76, duration: 0.210s, episode steps: 37, steps per second: 176, episode reward: 0.885, mean reward: 0.024 [-0.005, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.105 [-0.475, 0.180], loss: 0.000193, mean_absolute_error: 0.351217, mean_q: 0.357355\n",
      "  9621/50000: episode: 77, duration: 0.934s, episode steps: 196, steps per second: 210, episode reward: 0.698, mean reward: 0.004 [-0.004, 1.000], mean action: 0.985 [0.000, 2.000], mean observation: 0.070 [-0.110, 0.399], loss: 0.000580, mean_absolute_error: 0.343216, mean_q: 0.354435\n",
      "  9676/50000: episode: 78, duration: 0.288s, episode steps: 55, steps per second: 191, episode reward: 0.700, mean reward: 0.013 [-0.010, 1.000], mean action: 1.091 [0.000, 2.000], mean observation: -0.196 [-0.955, 0.280], loss: 0.000176, mean_absolute_error: 0.350679, mean_q: 0.361700\n",
      "  9677/50000: episode: 79, duration: 0.009s, episode steps: 1, steps per second: 105, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.047 [-0.085, -0.010], loss: 0.000740, mean_absolute_error: 0.410221, mean_q: 0.491464\n",
      "  9719/50000: episode: 80, duration: 0.249s, episode steps: 42, steps per second: 169, episode reward: 0.848, mean reward: 0.020 [-0.006, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.123 [-0.601, 0.210], loss: 0.000759, mean_absolute_error: 0.355964, mean_q: 0.376310\n",
      "  9781/50000: episode: 81, duration: 0.383s, episode steps: 62, steps per second: 162, episode reward: 0.680, mean reward: 0.011 [-0.010, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.185 [-0.250, 0.955], loss: 0.000309, mean_absolute_error: 0.349007, mean_q: 0.372392\n",
      "  9833/50000: episode: 82, duration: 0.292s, episode steps: 52, steps per second: 178, episode reward: 0.717, mean reward: 0.014 [-0.009, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.188 [-0.941, 0.290], loss: 0.000477, mean_absolute_error: 0.355658, mean_q: 0.380425\n",
      "  9881/50000: episode: 83, duration: 0.263s, episode steps: 48, steps per second: 183, episode reward: 0.921, mean reward: 0.019 [-0.002, 1.000], mean action: 0.896 [0.000, 2.000], mean observation: 0.070 [-0.050, 0.221], loss: 0.000290, mean_absolute_error: 0.357234, mean_q: 0.389948\n",
      "  9975/50000: episode: 84, duration: 0.512s, episode steps: 94, steps per second: 183, episode reward: 0.662, mean reward: 0.007 [-0.009, 1.000], mean action: 1.096 [0.000, 2.000], mean observation: 0.073 [-0.240, 0.914], loss: 0.000478, mean_absolute_error: 0.354484, mean_q: 0.382296\n",
      " 10025/50000: episode: 85, duration: 0.274s, episode steps: 50, steps per second: 182, episode reward: 0.840, mean reward: 0.017 [-0.006, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.095 [-0.170, 0.616], loss: 0.000135, mean_absolute_error: 0.360887, mean_q: 0.401265\n",
      " 10062/50000: episode: 86, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 0.887, mean reward: 0.024 [-0.005, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.100 [-0.484, 0.180], loss: 0.000257, mean_absolute_error: 0.371804, mean_q: 0.417492\n",
      " 10099/50000: episode: 87, duration: 0.196s, episode steps: 37, steps per second: 188, episode reward: 0.885, mean reward: 0.024 [-0.005, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.102 [-0.495, 0.180], loss: 0.000771, mean_absolute_error: 0.362859, mean_q: 0.393175\n",
      " 10179/50000: episode: 88, duration: 0.437s, episode steps: 80, steps per second: 183, episode reward: 0.855, mean reward: 0.011 [-0.003, 1.000], mean action: 0.988 [0.000, 2.000], mean observation: 0.080 [-0.030, 0.274], loss: 0.000423, mean_absolute_error: 0.369678, mean_q: 0.406090\n",
      " 10260/50000: episode: 89, duration: 0.453s, episode steps: 81, steps per second: 179, episode reward: 0.825, mean reward: 0.010 [-0.003, 1.000], mean action: 0.963 [0.000, 2.000], mean observation: 0.095 [-0.040, 0.329], loss: 0.000627, mean_absolute_error: 0.377525, mean_q: 0.433428\n",
      " 10319/50000: episode: 90, duration: 0.322s, episode steps: 59, steps per second: 183, episode reward: 0.723, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.161 [-0.893, 0.270], loss: 0.000534, mean_absolute_error: 0.374878, mean_q: 0.430019\n",
      " 10368/50000: episode: 91, duration: 0.260s, episode steps: 49, steps per second: 189, episode reward: 0.823, mean reward: 0.017 [-0.006, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.131 [-0.120, 0.588], loss: 0.000607, mean_absolute_error: 0.373390, mean_q: 0.434329\n",
      " 10421/50000: episode: 92, duration: 0.282s, episode steps: 53, steps per second: 188, episode reward: 0.836, mean reward: 0.016 [-0.005, 1.000], mean action: 0.868 [0.000, 2.000], mean observation: 0.117 [-0.130, 0.507], loss: 0.000326, mean_absolute_error: 0.374353, mean_q: 0.435148\n",
      " 10675/50000: episode: 93, duration: 1.336s, episode steps: 254, steps per second: 190, episode reward: 0.474, mean reward: 0.002 [-0.004, 1.000], mean action: 0.976 [0.000, 2.000], mean observation: 0.099 [-0.080, 0.351], loss: 0.000258, mean_absolute_error: 0.379783, mean_q: 0.449241\n",
      " 10843/50000: episode: 94, duration: 0.880s, episode steps: 168, steps per second: 191, episode reward: 0.688, mean reward: 0.004 [-0.003, 1.000], mean action: 0.964 [0.000, 2.000], mean observation: 0.092 [-0.080, 0.274], loss: 0.000588, mean_absolute_error: 0.385021, mean_q: 0.454940\n",
      " 11032/50000: episode: 95, duration: 0.988s, episode steps: 189, steps per second: 191, episode reward: 0.550, mean reward: 0.003 [-0.008, 1.000], mean action: 0.995 [0.000, 2.000], mean observation: 0.100 [-0.190, 0.818], loss: 0.000592, mean_absolute_error: 0.385314, mean_q: 0.465095\n",
      " 11077/50000: episode: 96, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 0.886, mean reward: 0.020 [-0.005, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.070 [-0.491, 0.180], loss: 0.000602, mean_absolute_error: 0.393160, mean_q: 0.497776\n",
      " 11115/50000: episode: 97, duration: 0.204s, episode steps: 38, steps per second: 187, episode reward: 0.930, mean reward: 0.024 [-0.004, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.040 [-0.363, 0.150], loss: 0.000218, mean_absolute_error: 0.394893, mean_q: 0.494735\n",
      " 11156/50000: episode: 98, duration: 0.217s, episode steps: 41, steps per second: 189, episode reward: 0.887, mean reward: 0.022 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.079 [-0.499, 0.190], loss: 0.000570, mean_absolute_error: 0.391773, mean_q: 0.486884\n",
      " 11181/50000: episode: 99, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 0.965, mean reward: 0.039 [-0.002, 1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.025 [-0.236, 0.130], loss: 0.000474, mean_absolute_error: 0.392737, mean_q: 0.497411\n",
      " 11182/50000: episode: 100, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.009 [-0.029, 0.010], loss: 0.000395, mean_absolute_error: 0.418533, mean_q: 0.572527\n",
      " 11195/50000: episode: 101, duration: 0.074s, episode steps: 13, steps per second: 177, episode reward: 0.986, mean reward: 0.076 [-0.001, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.043 [-0.140, 0.040], loss: 0.000148, mean_absolute_error: 0.387726, mean_q: 0.492204\n",
      " 11256/50000: episode: 102, duration: 0.323s, episode steps: 61, steps per second: 189, episode reward: 0.758, mean reward: 0.012 [-0.008, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.128 [-0.781, 0.240], loss: 0.000700, mean_absolute_error: 0.396252, mean_q: 0.502278\n",
      " 11268/50000: episode: 103, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 0.987, mean reward: 0.082 [-0.001, 1.000], mean action: 1.583 [0.000, 2.000], mean observation: -0.044 [-0.134, 0.070], loss: 0.000278, mean_absolute_error: 0.393364, mean_q: 0.490182\n",
      " 11354/50000: episode: 104, duration: 0.458s, episode steps: 86, steps per second: 188, episode reward: 0.762, mean reward: 0.009 [-0.006, 1.000], mean action: 0.942 [0.000, 2.000], mean observation: 0.112 [-0.130, 0.565], loss: 0.000363, mean_absolute_error: 0.390697, mean_q: 0.490091\n",
      " 11442/50000: episode: 105, duration: 0.480s, episode steps: 88, steps per second: 183, episode reward: 0.642, mean reward: 0.007 [-0.009, 1.000], mean action: 0.977 [0.000, 2.000], mean observation: 0.159 [-0.200, 0.899], loss: 0.000330, mean_absolute_error: 0.387610, mean_q: 0.484967\n",
      " 11448/50000: episode: 106, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: 0.995, mean reward: 0.166 [-0.001, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.040 [-0.040, 0.111], loss: 0.000121, mean_absolute_error: 0.407592, mean_q: 0.515109\n",
      " 11449/50000: episode: 107, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.045 [-0.079, -0.010], loss: 0.000065, mean_absolute_error: 0.389323, mean_q: 0.518453\n",
      " 11526/50000: episode: 108, duration: 0.409s, episode steps: 77, steps per second: 188, episode reward: 0.661, mean reward: 0.009 [-0.009, 1.000], mean action: 0.948 [0.000, 2.000], mean observation: 0.169 [-0.180, 0.895], loss: 0.000358, mean_absolute_error: 0.388692, mean_q: 0.492646\n",
      " 11593/50000: episode: 109, duration: 0.357s, episode steps: 67, steps per second: 188, episode reward: 0.903, mean reward: 0.013 [-0.002, 1.000], mean action: 0.955 [0.000, 2.000], mean observation: 0.063 [-0.080, 0.232], loss: 0.000451, mean_absolute_error: 0.397979, mean_q: 0.502010\n",
      " 11652/50000: episode: 110, duration: 0.317s, episode steps: 59, steps per second: 186, episode reward: 0.915, mean reward: 0.016 [-0.003, 1.000], mean action: 0.983 [0.000, 2.000], mean observation: 0.060 [-0.080, 0.250], loss: 0.000552, mean_absolute_error: 0.394305, mean_q: 0.492225\n",
      " 11660/50000: episode: 111, duration: 0.046s, episode steps: 8, steps per second: 175, episode reward: 0.991, mean reward: 0.124 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.135, 0.080], loss: 0.000272, mean_absolute_error: 0.405569, mean_q: 0.531784\n",
      " 11688/50000: episode: 112, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 0.961, mean reward: 0.034 [-0.002, 1.000], mean action: 0.929 [0.000, 2.000], mean observation: 0.053 [-0.070, 0.205], loss: 0.000504, mean_absolute_error: 0.408127, mean_q: 0.516136\n",
      " 11737/50000: episode: 113, duration: 0.264s, episode steps: 49, steps per second: 185, episode reward: 0.814, mean reward: 0.017 [-0.007, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.119 [-0.691, 0.230], loss: 0.000509, mean_absolute_error: 0.391441, mean_q: 0.499948\n",
      " 11738/50000: episode: 114, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.029 [-0.069, 0.010], loss: 0.000062, mean_absolute_error: 0.435794, mean_q: 0.591785\n",
      " 11777/50000: episode: 115, duration: 0.209s, episode steps: 39, steps per second: 187, episode reward: 0.922, mean reward: 0.024 [-0.004, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.046 [-0.390, 0.160], loss: 0.000567, mean_absolute_error: 0.400423, mean_q: 0.516377\n",
      " 11841/50000: episode: 116, duration: 0.337s, episode steps: 64, steps per second: 190, episode reward: 0.755, mean reward: 0.012 [-0.007, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.143 [-0.180, 0.721], loss: 0.000463, mean_absolute_error: 0.398377, mean_q: 0.506263\n",
      " 11900/50000: episode: 117, duration: 0.309s, episode steps: 59, steps per second: 191, episode reward: 0.684, mean reward: 0.012 [-0.009, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.194 [-0.931, 0.260], loss: 0.000159, mean_absolute_error: 0.399102, mean_q: 0.505073\n",
      " 11947/50000: episode: 118, duration: 0.253s, episode steps: 47, steps per second: 186, episode reward: 0.838, mean reward: 0.018 [-0.006, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.108 [-0.630, 0.190], loss: 0.000599, mean_absolute_error: 0.397633, mean_q: 0.511786\n",
      " 11980/50000: episode: 119, duration: 0.186s, episode steps: 33, steps per second: 177, episode reward: 0.907, mean reward: 0.027 [-0.004, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.089 [-0.443, 0.170], loss: 0.000172, mean_absolute_error: 0.395220, mean_q: 0.504287\n",
      " 12044/50000: episode: 120, duration: 0.339s, episode steps: 64, steps per second: 189, episode reward: 0.662, mean reward: 0.010 [-0.009, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.199 [-0.190, 0.939], loss: 0.000528, mean_absolute_error: 0.404436, mean_q: 0.528360\n",
      " 12103/50000: episode: 121, duration: 0.309s, episode steps: 59, steps per second: 191, episode reward: 0.836, mean reward: 0.014 [-0.005, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.108 [-0.120, 0.473], loss: 0.000622, mean_absolute_error: 0.396842, mean_q: 0.512019\n",
      " 12182/50000: episode: 122, duration: 0.419s, episode steps: 79, steps per second: 188, episode reward: 0.655, mean reward: 0.008 [-0.009, 1.000], mean action: 0.975 [0.000, 2.000], mean observation: 0.168 [-0.220, 0.907], loss: 0.000578, mean_absolute_error: 0.401038, mean_q: 0.510359\n",
      " 12241/50000: episode: 123, duration: 0.311s, episode steps: 59, steps per second: 190, episode reward: 0.776, mean reward: 0.013 [-0.008, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.122 [-0.775, 0.230], loss: 0.000217, mean_absolute_error: 0.401015, mean_q: 0.519121\n",
      " 12346/50000: episode: 124, duration: 0.552s, episode steps: 105, steps per second: 190, episode reward: 0.782, mean reward: 0.007 [-0.004, 1.000], mean action: 0.952 [0.000, 2.000], mean observation: 0.089 [-0.080, 0.417], loss: 0.000413, mean_absolute_error: 0.402018, mean_q: 0.521734\n",
      " 12385/50000: episode: 125, duration: 0.207s, episode steps: 39, steps per second: 188, episode reward: 0.911, mean reward: 0.023 [-0.004, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.058 [-0.425, 0.160], loss: 0.000610, mean_absolute_error: 0.406403, mean_q: 0.528524\n",
      " 12437/50000: episode: 126, duration: 0.290s, episode steps: 52, steps per second: 179, episode reward: 0.807, mean reward: 0.016 [-0.007, 1.000], mean action: 0.981 [0.000, 2.000], mean observation: 0.133 [-0.170, 0.653], loss: 0.000239, mean_absolute_error: 0.402553, mean_q: 0.527126\n",
      " 12515/50000: episode: 127, duration: 0.416s, episode steps: 78, steps per second: 187, episode reward: 0.847, mean reward: 0.011 [-0.004, 1.000], mean action: 0.923 [0.000, 2.000], mean observation: 0.080 [-0.090, 0.387], loss: 0.000288, mean_absolute_error: 0.405949, mean_q: 0.531571\n",
      " 12601/50000: episode: 128, duration: 0.462s, episode steps: 86, steps per second: 186, episode reward: 0.641, mean reward: 0.007 [-0.008, 1.000], mean action: 0.919 [0.000, 2.000], mean observation: 0.169 [-0.130, 0.793], loss: 0.000615, mean_absolute_error: 0.407807, mean_q: 0.530700\n",
      " 12602/50000: episode: 129, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.015 [-0.040, 0.010], loss: 0.000062, mean_absolute_error: 0.421316, mean_q: 0.639867\n",
      " 12700/50000: episode: 130, duration: 0.509s, episode steps: 98, steps per second: 193, episode reward: 0.619, mean reward: 0.006 [-0.008, 1.000], mean action: 0.949 [0.000, 2.000], mean observation: 0.159 [-0.150, 0.797], loss: 0.000514, mean_absolute_error: 0.409314, mean_q: 0.534203\n",
      " 12717/50000: episode: 131, duration: 0.092s, episode steps: 17, steps per second: 184, episode reward: 0.979, mean reward: 0.058 [-0.001, 1.000], mean action: 1.529 [0.000, 2.000], mean observation: -0.054 [-0.143, 0.090], loss: 0.000232, mean_absolute_error: 0.407194, mean_q: 0.543736\n",
      " 12805/50000: episode: 132, duration: 0.463s, episode steps: 88, steps per second: 190, episode reward: 0.835, mean reward: 0.009 [-0.004, 1.000], mean action: 0.977 [0.000, 2.000], mean observation: 0.079 [-0.080, 0.368], loss: 0.000225, mean_absolute_error: 0.406249, mean_q: 0.530005\n",
      " 12888/50000: episode: 133, duration: 0.439s, episode steps: 83, steps per second: 189, episode reward: 0.761, mean reward: 0.009 [-0.006, 1.000], mean action: 0.964 [0.000, 2.000], mean observation: 0.113 [-0.140, 0.626], loss: 0.000552, mean_absolute_error: 0.407372, mean_q: 0.533441\n",
      " 12936/50000: episode: 134, duration: 0.256s, episode steps: 48, steps per second: 187, episode reward: 0.829, mean reward: 0.017 [-0.006, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.120 [-0.624, 0.170], loss: 0.000374, mean_absolute_error: 0.407005, mean_q: 0.541820\n",
      " 12961/50000: episode: 135, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 0.955, mean reward: 0.038 [-0.003, 1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.051 [-0.263, 0.110], loss: 0.000361, mean_absolute_error: 0.410702, mean_q: 0.548052\n",
      " 13032/50000: episode: 136, duration: 0.371s, episode steps: 71, steps per second: 191, episode reward: 0.653, mean reward: 0.009 [-0.007, 1.000], mean action: 1.113 [0.000, 2.000], mean observation: -0.201 [-0.720, 0.190], loss: 0.000234, mean_absolute_error: 0.407107, mean_q: 0.531747\n",
      " 13100/50000: episode: 137, duration: 0.383s, episode steps: 68, steps per second: 177, episode reward: 0.670, mean reward: 0.010 [-0.008, 1.000], mean action: 1.103 [0.000, 2.000], mean observation: -0.190 [-0.829, 0.170], loss: 0.000239, mean_absolute_error: 0.402720, mean_q: 0.521283\n",
      " 13153/50000: episode: 138, duration: 0.274s, episode steps: 53, steps per second: 193, episode reward: 0.770, mean reward: 0.015 [-0.007, 1.000], mean action: 1.151 [0.000, 2.000], mean observation: -0.162 [-0.692, 0.200], loss: 0.001106, mean_absolute_error: 0.399236, mean_q: 0.518451\n",
      " 13236/50000: episode: 139, duration: 0.432s, episode steps: 83, steps per second: 192, episode reward: 0.835, mean reward: 0.010 [-0.003, 1.000], mean action: 0.976 [0.000, 2.000], mean observation: 0.085 [-0.070, 0.350], loss: 0.000139, mean_absolute_error: 0.404425, mean_q: 0.533500\n",
      " 13276/50000: episode: 140, duration: 0.216s, episode steps: 40, steps per second: 185, episode reward: 0.868, mean reward: 0.022 [-0.005, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.109 [-0.547, 0.200], loss: 0.000452, mean_absolute_error: 0.406479, mean_q: 0.534968\n",
      " 13277/50000: episode: 141, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.012 [0.010, 0.015], loss: 0.000145, mean_absolute_error: 0.439707, mean_q: 0.585688\n",
      " 13322/50000: episode: 142, duration: 0.242s, episode steps: 45, steps per second: 186, episode reward: 0.864, mean reward: 0.019 [-0.005, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.106 [-0.499, 0.160], loss: 0.000360, mean_absolute_error: 0.404549, mean_q: 0.525020\n",
      " 13323/50000: episode: 143, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.047 [0.000, 0.094], loss: 0.000096, mean_absolute_error: 0.439061, mean_q: 0.622190\n",
      " 13382/50000: episode: 144, duration: 0.311s, episode steps: 59, steps per second: 190, episode reward: 0.699, mean reward: 0.012 [-0.008, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.199 [-0.766, 0.230], loss: 0.000192, mean_absolute_error: 0.404563, mean_q: 0.529255\n",
      " 13450/50000: episode: 145, duration: 0.361s, episode steps: 68, steps per second: 188, episode reward: 0.880, mean reward: 0.013 [-0.003, 1.000], mean action: 0.985 [0.000, 2.000], mean observation: 0.073 [-0.080, 0.318], loss: 0.000459, mean_absolute_error: 0.407403, mean_q: 0.538517\n",
      " 13583/50000: episode: 146, duration: 0.719s, episode steps: 133, steps per second: 185, episode reward: 0.663, mean reward: 0.005 [-0.005, 1.000], mean action: 0.947 [0.000, 2.000], mean observation: 0.110 [-0.090, 0.540], loss: 0.000320, mean_absolute_error: 0.405537, mean_q: 0.534436\n",
      " 13621/50000: episode: 147, duration: 0.226s, episode steps: 38, steps per second: 168, episode reward: 0.908, mean reward: 0.024 [-0.004, 1.000], mean action: 1.079 [0.000, 2.000], mean observation: -0.082 [-0.410, 0.150], loss: 0.000278, mean_absolute_error: 0.407301, mean_q: 0.528809\n",
      " 13628/50000: episode: 148, duration: 0.058s, episode steps: 7, steps per second: 122, episode reward: 0.994, mean reward: 0.142 [-0.001, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.040 [-0.115, 0.040], loss: 0.000042, mean_absolute_error: 0.400580, mean_q: 0.511708\n",
      " 13706/50000: episode: 149, duration: 0.416s, episode steps: 78, steps per second: 187, episode reward: 0.832, mean reward: 0.011 [-0.004, 1.000], mean action: 0.974 [0.000, 2.000], mean observation: 0.091 [-0.070, 0.377], loss: 0.000132, mean_absolute_error: 0.405379, mean_q: 0.532253\n",
      " 13779/50000: episode: 150, duration: 0.384s, episode steps: 73, steps per second: 190, episode reward: 0.661, mean reward: 0.009 [-0.007, 1.000], mean action: 1.068 [0.000, 2.000], mean observation: -0.189 [-0.737, 0.160], loss: 0.000221, mean_absolute_error: 0.410200, mean_q: 0.532674\n",
      " 13828/50000: episode: 151, duration: 0.265s, episode steps: 49, steps per second: 185, episode reward: 0.875, mean reward: 0.018 [-0.005, 1.000], mean action: 1.041 [0.000, 2.000], mean observation: -0.090 [-0.482, 0.170], loss: 0.000498, mean_absolute_error: 0.409291, mean_q: 0.545148\n",
      " 14019/50000: episode: 152, duration: 1.039s, episode steps: 191, steps per second: 184, episode reward: 0.400, mean reward: 0.002 [-0.008, 1.000], mean action: 0.984 [0.000, 2.000], mean observation: 0.140 [-0.130, 0.765], loss: 0.000459, mean_absolute_error: 0.404392, mean_q: 0.535527\n",
      " 14059/50000: episode: 153, duration: 0.212s, episode steps: 40, steps per second: 189, episode reward: 0.898, mean reward: 0.022 [-0.004, 1.000], mean action: 1.075 [0.000, 2.000], mean observation: -0.088 [-0.416, 0.140], loss: 0.000285, mean_absolute_error: 0.406335, mean_q: 0.536705\n",
      " 14095/50000: episode: 154, duration: 0.192s, episode steps: 36, steps per second: 187, episode reward: 0.886, mean reward: 0.025 [-0.005, 1.000], mean action: 1.222 [0.000, 2.000], mean observation: -0.106 [-0.477, 0.170], loss: 0.000470, mean_absolute_error: 0.400973, mean_q: 0.528920\n",
      " 14156/50000: episode: 155, duration: 0.321s, episode steps: 61, steps per second: 190, episode reward: 0.854, mean reward: 0.014 [-0.004, 1.000], mean action: 0.918 [0.000, 2.000], mean observation: 0.096 [-0.080, 0.402], loss: 0.000265, mean_absolute_error: 0.399821, mean_q: 0.515791\n",
      " 14204/50000: episode: 156, duration: 0.252s, episode steps: 48, steps per second: 190, episode reward: 0.902, mean reward: 0.019 [-0.003, 1.000], mean action: 0.854 [0.000, 2.000], mean observation: 0.081 [-0.080, 0.308], loss: 0.000446, mean_absolute_error: 0.404656, mean_q: 0.527902\n",
      " 14239/50000: episode: 157, duration: 0.192s, episode steps: 35, steps per second: 183, episode reward: 0.906, mean reward: 0.026 [-0.004, 1.000], mean action: 1.229 [0.000, 2.000], mean observation: -0.087 [-0.435, 0.170], loss: 0.000466, mean_absolute_error: 0.398257, mean_q: 0.513429\n",
      " 14315/50000: episode: 158, duration: 0.403s, episode steps: 76, steps per second: 189, episode reward: 0.610, mean reward: 0.008 [-0.008, 1.000], mean action: 1.066 [0.000, 2.000], mean observation: -0.209 [-0.822, 0.230], loss: 0.000283, mean_absolute_error: 0.399810, mean_q: 0.514632\n",
      " 14355/50000: episode: 159, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 0.899, mean reward: 0.022 [-0.004, 1.000], mean action: 1.050 [0.000, 2.000], mean observation: -0.085 [-0.437, 0.160], loss: 0.000473, mean_absolute_error: 0.401800, mean_q: 0.536737\n",
      " 14476/50000: episode: 160, duration: 0.638s, episode steps: 121, steps per second: 190, episode reward: 0.314, mean reward: 0.003 [-0.008, 1.000], mean action: 1.066 [0.000, 2.000], mean observation: -0.254 [-0.811, 0.200], loss: 0.000466, mean_absolute_error: 0.400309, mean_q: 0.531350\n",
      " 14533/50000: episode: 161, duration: 0.302s, episode steps: 57, steps per second: 189, episode reward: 0.734, mean reward: 0.013 [-0.008, 1.000], mean action: 1.123 [0.000, 2.000], mean observation: -0.174 [-0.787, 0.230], loss: 0.000217, mean_absolute_error: 0.400090, mean_q: 0.519158\n",
      " 14565/50000: episode: 162, duration: 0.171s, episode steps: 32, steps per second: 188, episode reward: 0.959, mean reward: 0.030 [-0.003, 1.000], mean action: 1.281 [0.000, 2.000], mean observation: -0.015 [-0.251, 0.120], loss: 0.000348, mean_absolute_error: 0.400775, mean_q: 0.536431\n",
      " 14566/50000: episode: 163, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.014 [0.010, 0.019], loss: 0.000067, mean_absolute_error: 0.393607, mean_q: 0.537054\n",
      " 14616/50000: episode: 164, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 0.833, mean reward: 0.017 [-0.006, 1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.119 [-0.589, 0.170], loss: 0.000221, mean_absolute_error: 0.397504, mean_q: 0.526772\n",
      " 14703/50000: episode: 165, duration: 0.442s, episode steps: 87, steps per second: 197, episode reward: 0.550, mean reward: 0.006 [-0.008, 1.000], mean action: 1.069 [0.000, 2.000], mean observation: -0.216 [-0.836, 0.210], loss: 0.000292, mean_absolute_error: 0.402227, mean_q: 0.535131\n",
      " 14798/50000: episode: 166, duration: 0.505s, episode steps: 95, steps per second: 188, episode reward: 0.725, mean reward: 0.008 [-0.005, 1.000], mean action: 1.084 [0.000, 2.000], mean observation: 0.078 [-0.200, 0.543], loss: 0.000341, mean_absolute_error: 0.403240, mean_q: 0.538726\n",
      " 15077/50000: episode: 167, duration: 1.471s, episode steps: 279, steps per second: 190, episode reward: 0.434, mean reward: 0.002 [-0.003, 1.000], mean action: 0.968 [0.000, 2.000], mean observation: 0.098 [-0.150, 0.275], loss: 0.000261, mean_absolute_error: 0.401630, mean_q: 0.529546\n",
      " 15218/50000: episode: 168, duration: 0.750s, episode steps: 141, steps per second: 188, episode reward: 0.397, mean reward: 0.003 [-0.009, 1.000], mean action: 0.936 [0.000, 2.000], mean observation: 0.177 [-0.210, 0.914], loss: 0.000385, mean_absolute_error: 0.403053, mean_q: 0.534042\n",
      " 15308/50000: episode: 169, duration: 0.461s, episode steps: 90, steps per second: 195, episode reward: 0.501, mean reward: 0.006 [-0.009, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.232 [-0.875, 0.210], loss: 0.000263, mean_absolute_error: 0.398645, mean_q: 0.527398\n",
      " 15393/50000: episode: 170, duration: 0.462s, episode steps: 85, steps per second: 184, episode reward: 0.537, mean reward: 0.006 [-0.009, 1.000], mean action: 0.894 [0.000, 2.000], mean observation: 0.217 [-0.180, 0.942], loss: 0.000189, mean_absolute_error: 0.397774, mean_q: 0.525364\n",
      " 15464/50000: episode: 171, duration: 0.397s, episode steps: 71, steps per second: 179, episode reward: 0.685, mean reward: 0.010 [-0.009, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.157 [-0.909, 0.230], loss: 0.000194, mean_absolute_error: 0.397746, mean_q: 0.532455\n",
      " 15485/50000: episode: 172, duration: 0.117s, episode steps: 21, steps per second: 179, episode reward: 0.975, mean reward: 0.046 [-0.002, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.016 [-0.140, 0.195], loss: 0.000735, mean_absolute_error: 0.394340, mean_q: 0.514877\n",
      " 15539/50000: episode: 173, duration: 0.295s, episode steps: 54, steps per second: 183, episode reward: 0.838, mean reward: 0.016 [-0.006, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.093 [-0.572, 0.160], loss: 0.000343, mean_absolute_error: 0.401936, mean_q: 0.525093\n",
      " 15610/50000: episode: 174, duration: 0.415s, episode steps: 71, steps per second: 171, episode reward: 0.809, mean reward: 0.011 [-0.005, 1.000], mean action: 0.873 [0.000, 2.000], mean observation: 0.084 [-0.170, 0.537], loss: 0.000680, mean_absolute_error: 0.397060, mean_q: 0.531045\n",
      " 15755/50000: episode: 175, duration: 0.754s, episode steps: 145, steps per second: 192, episode reward: 0.712, mean reward: 0.005 [-0.005, 1.000], mean action: 1.034 [0.000, 2.000], mean observation: 0.049 [-0.180, 0.531], loss: 0.000418, mean_absolute_error: 0.396954, mean_q: 0.527474\n",
      " 15756/50000: episode: 176, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.041 [-0.010, 0.091], loss: 0.000189, mean_absolute_error: 0.414542, mean_q: 0.570568\n",
      " 15820/50000: episode: 177, duration: 0.340s, episode steps: 64, steps per second: 188, episode reward: 0.702, mean reward: 0.011 [-0.009, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.166 [-0.890, 0.240], loss: 0.000239, mean_absolute_error: 0.395692, mean_q: 0.525471\n",
      " 15862/50000: episode: 178, duration: 0.244s, episode steps: 42, steps per second: 172, episode reward: 0.884, mean reward: 0.021 [-0.005, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.082 [-0.486, 0.150], loss: 0.000225, mean_absolute_error: 0.396050, mean_q: 0.526620\n",
      " 15870/50000: episode: 179, duration: 0.046s, episode steps: 8, steps per second: 173, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 1.875 [1.000, 2.000], mean observation: -0.041 [-0.127, 0.070], loss: 0.000201, mean_absolute_error: 0.393029, mean_q: 0.516613\n",
      " 15937/50000: episode: 180, duration: 0.356s, episode steps: 67, steps per second: 188, episode reward: 0.736, mean reward: 0.011 [-0.008, 1.000], mean action: 1.134 [0.000, 2.000], mean observation: -0.126 [-0.812, 0.210], loss: 0.000347, mean_absolute_error: 0.402373, mean_q: 0.539097\n",
      " 15973/50000: episode: 181, duration: 0.194s, episode steps: 36, steps per second: 185, episode reward: 0.934, mean reward: 0.026 [-0.003, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.044 [-0.338, 0.130], loss: 0.000396, mean_absolute_error: 0.405392, mean_q: 0.540053\n",
      " 16005/50000: episode: 182, duration: 0.213s, episode steps: 32, steps per second: 150, episode reward: 0.926, mean reward: 0.029 [-0.004, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.075 [-0.363, 0.140], loss: 0.000152, mean_absolute_error: 0.405978, mean_q: 0.543281\n",
      " 16062/50000: episode: 183, duration: 0.300s, episode steps: 57, steps per second: 190, episode reward: 0.822, mean reward: 0.014 [-0.006, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.100 [-0.606, 0.170], loss: 0.000337, mean_absolute_error: 0.398403, mean_q: 0.529999\n",
      " 16088/50000: episode: 184, duration: 0.145s, episode steps: 26, steps per second: 179, episode reward: 0.949, mean reward: 0.036 [-0.003, 1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.063 [-0.295, 0.120], loss: 0.000589, mean_absolute_error: 0.406037, mean_q: 0.556993\n",
      " 16123/50000: episode: 185, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 0.896, mean reward: 0.026 [-0.005, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.096 [-0.469, 0.160], loss: 0.000299, mean_absolute_error: 0.404014, mean_q: 0.535528\n",
      " 16156/50000: episode: 186, duration: 0.175s, episode steps: 33, steps per second: 188, episode reward: 0.918, mean reward: 0.028 [-0.004, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.082 [-0.130, 0.377], loss: 0.000702, mean_absolute_error: 0.401404, mean_q: 0.533120\n",
      " 16223/50000: episode: 187, duration: 0.356s, episode steps: 67, steps per second: 188, episode reward: 0.639, mean reward: 0.010 [-0.010, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.203 [-0.240, 0.989], loss: 0.000297, mean_absolute_error: 0.396863, mean_q: 0.530762\n",
      " 16265/50000: episode: 188, duration: 0.215s, episode steps: 42, steps per second: 195, episode reward: 0.884, mean reward: 0.021 [-0.004, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.092 [-0.440, 0.140], loss: 0.000405, mean_absolute_error: 0.401340, mean_q: 0.549235\n",
      " 16327/50000: episode: 189, duration: 0.335s, episode steps: 62, steps per second: 185, episode reward: 0.692, mean reward: 0.011 [-0.009, 1.000], mean action: 0.871 [0.000, 2.000], mean observation: 0.185 [-0.220, 0.891], loss: 0.000361, mean_absolute_error: 0.398305, mean_q: 0.535784\n",
      " 16361/50000: episode: 190, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 0.919, mean reward: 0.027 [-0.004, 1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.079 [-0.376, 0.130], loss: 0.000376, mean_absolute_error: 0.397507, mean_q: 0.530426\n",
      " 16362/50000: episode: 191, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.028 [0.010, 0.046], loss: 0.000136, mean_absolute_error: 0.397506, mean_q: 0.549179\n",
      " 16391/50000: episode: 192, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 0.950, mean reward: 0.033 [-0.002, 1.000], mean action: 0.690 [0.000, 2.000], mean observation: 0.074 [-0.110, 0.203], loss: 0.000444, mean_absolute_error: 0.406465, mean_q: 0.546244\n",
      " 16421/50000: episode: 193, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 0.932, mean reward: 0.031 [-0.003, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.073 [-0.120, 0.334], loss: 0.000238, mean_absolute_error: 0.404308, mean_q: 0.527263\n",
      " 16422/50000: episode: 194, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.082, 0.010], loss: 0.000538, mean_absolute_error: 0.426191, mean_q: 0.611587\n",
      " 16435/50000: episode: 195, duration: 0.074s, episode steps: 13, steps per second: 175, episode reward: 0.984, mean reward: 0.076 [-0.001, 1.000], mean action: 0.308 [0.000, 2.000], mean observation: 0.044 [-0.090, 0.148], loss: 0.000453, mean_absolute_error: 0.390573, mean_q: 0.511781\n",
      " 16481/50000: episode: 196, duration: 0.249s, episode steps: 46, steps per second: 185, episode reward: 0.839, mean reward: 0.018 [-0.006, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.121 [-0.180, 0.595], loss: 0.000131, mean_absolute_error: 0.404619, mean_q: 0.555764\n",
      " 16490/50000: episode: 197, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.038 [-0.143, 0.090], loss: 0.000048, mean_absolute_error: 0.382589, mean_q: 0.496143\n",
      " 16491/50000: episode: 198, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [-0.010, 0.048], loss: 0.000040, mean_absolute_error: 0.436758, mean_q: 0.573717\n",
      " 16526/50000: episode: 199, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 0.935, mean reward: 0.027 [-0.004, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.036 [-0.355, 0.150], loss: 0.000744, mean_absolute_error: 0.387746, mean_q: 0.517673\n",
      " 16572/50000: episode: 200, duration: 0.243s, episode steps: 46, steps per second: 189, episode reward: 0.872, mean reward: 0.019 [-0.005, 1.000], mean action: 0.957 [0.000, 2.000], mean observation: 0.096 [-0.170, 0.503], loss: 0.000583, mean_absolute_error: 0.398740, mean_q: 0.530741\n",
      " 16588/50000: episode: 201, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 0.438 [0.000, 2.000], mean observation: 0.050 [-0.110, 0.179], loss: 0.000920, mean_absolute_error: 0.398567, mean_q: 0.524325\n",
      " 16597/50000: episode: 202, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.090, 0.140], loss: 0.000174, mean_absolute_error: 0.389709, mean_q: 0.515059\n",
      " 16619/50000: episode: 203, duration: 0.119s, episode steps: 22, steps per second: 185, episode reward: 0.967, mean reward: 0.044 [-0.002, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.036 [-0.120, 0.226], loss: 0.000122, mean_absolute_error: 0.408071, mean_q: 0.552863\n",
      " 16686/50000: episode: 204, duration: 0.355s, episode steps: 67, steps per second: 189, episode reward: 0.659, mean reward: 0.010 [-0.010, 1.000], mean action: 1.134 [0.000, 2.000], mean observation: -0.183 [-0.984, 0.240], loss: 0.000262, mean_absolute_error: 0.402199, mean_q: 0.545895\n",
      " 16717/50000: episode: 205, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 0.940, mean reward: 0.030 [-0.003, 1.000], mean action: 1.065 [0.000, 2.000], mean observation: -0.064 [-0.316, 0.140], loss: 0.000636, mean_absolute_error: 0.400396, mean_q: 0.531469\n",
      " 16752/50000: episode: 206, duration: 0.186s, episode steps: 35, steps per second: 189, episode reward: 0.923, mean reward: 0.026 [-0.004, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.055 [-0.383, 0.150], loss: 0.000112, mean_absolute_error: 0.395281, mean_q: 0.512795\n",
      " 16818/50000: episode: 207, duration: 0.356s, episode steps: 66, steps per second: 185, episode reward: 0.658, mean reward: 0.010 [-0.010, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.192 [-0.970, 0.210], loss: 0.000349, mean_absolute_error: 0.404951, mean_q: 0.548391\n",
      " 16883/50000: episode: 208, duration: 0.359s, episode steps: 65, steps per second: 181, episode reward: 0.776, mean reward: 0.012 [-0.007, 1.000], mean action: 1.138 [0.000, 2.000], mean observation: -0.111 [-0.711, 0.190], loss: 0.000320, mean_absolute_error: 0.402813, mean_q: 0.545679\n",
      " 16913/50000: episode: 209, duration: 0.162s, episode steps: 30, steps per second: 186, episode reward: 0.943, mean reward: 0.031 [-0.003, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.045 [-0.326, 0.150], loss: 0.000452, mean_absolute_error: 0.408331, mean_q: 0.548841\n",
      " 16959/50000: episode: 210, duration: 0.248s, episode steps: 46, steps per second: 186, episode reward: 0.860, mean reward: 0.019 [-0.005, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.110 [-0.150, 0.497], loss: 0.000376, mean_absolute_error: 0.398769, mean_q: 0.547770\n",
      " 17005/50000: episode: 211, duration: 0.246s, episode steps: 46, steps per second: 187, episode reward: 0.876, mean reward: 0.019 [-0.005, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.082 [-0.490, 0.150], loss: 0.000326, mean_absolute_error: 0.405178, mean_q: 0.548401\n",
      " 17032/50000: episode: 212, duration: 0.146s, episode steps: 27, steps per second: 185, episode reward: 0.941, mean reward: 0.035 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.070 [-0.120, 0.322], loss: 0.000093, mean_absolute_error: 0.404653, mean_q: 0.546726\n",
      " 17088/50000: episode: 213, duration: 0.298s, episode steps: 56, steps per second: 188, episode reward: 0.742, mean reward: 0.013 [-0.008, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.167 [-0.190, 0.812], loss: 0.000414, mean_absolute_error: 0.406163, mean_q: 0.546594\n",
      " 17139/50000: episode: 214, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 0.771, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.158 [-0.210, 0.775], loss: 0.000148, mean_absolute_error: 0.407538, mean_q: 0.562307\n",
      " 17206/50000: episode: 215, duration: 0.356s, episode steps: 67, steps per second: 188, episode reward: 0.712, mean reward: 0.011 [-0.008, 1.000], mean action: 1.134 [0.000, 2.000], mean observation: -0.152 [-0.844, 0.200], loss: 0.000230, mean_absolute_error: 0.411223, mean_q: 0.560595\n",
      " 17271/50000: episode: 216, duration: 0.342s, episode steps: 65, steps per second: 190, episode reward: 0.676, mean reward: 0.010 [-0.009, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.187 [-0.230, 0.899], loss: 0.000484, mean_absolute_error: 0.411610, mean_q: 0.558872\n",
      " 17287/50000: episode: 217, duration: 0.088s, episode steps: 16, steps per second: 181, episode reward: 0.978, mean reward: 0.061 [-0.002, 1.000], mean action: 1.312 [0.000, 2.000], mean observation: -0.045 [-0.184, 0.090], loss: 0.000409, mean_absolute_error: 0.411880, mean_q: 0.552544\n",
      " 17288/50000: episode: 218, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.046 [-0.082, -0.010], loss: 0.000045, mean_absolute_error: 0.353557, mean_q: 0.536027\n",
      " 17343/50000: episode: 219, duration: 0.291s, episode steps: 55, steps per second: 189, episode reward: 0.772, mean reward: 0.014 [-0.007, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.151 [-0.170, 0.723], loss: 0.000492, mean_absolute_error: 0.409092, mean_q: 0.564312\n",
      " 17377/50000: episode: 220, duration: 0.183s, episode steps: 34, steps per second: 185, episode reward: 0.919, mean reward: 0.027 [-0.004, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.078 [-0.120, 0.359], loss: 0.000313, mean_absolute_error: 0.408626, mean_q: 0.561400\n",
      " 17435/50000: episode: 221, duration: 0.315s, episode steps: 58, steps per second: 184, episode reward: 0.757, mean reward: 0.013 [-0.007, 1.000], mean action: 0.897 [0.000, 2.000], mean observation: 0.155 [-0.170, 0.728], loss: 0.000413, mean_absolute_error: 0.415194, mean_q: 0.567614\n",
      " 17499/50000: episode: 222, duration: 0.337s, episode steps: 64, steps per second: 190, episode reward: 0.652, mean reward: 0.010 [-0.010, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.205 [-0.210, 0.961], loss: 0.000215, mean_absolute_error: 0.409947, mean_q: 0.563630\n",
      " 17536/50000: episode: 223, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 0.894, mean reward: 0.024 [-0.005, 1.000], mean action: 1.189 [0.000, 2.000], mean observation: -0.097 [-0.453, 0.130], loss: 0.000265, mean_absolute_error: 0.417259, mean_q: 0.561290\n",
      " 17590/50000: episode: 224, duration: 0.288s, episode steps: 54, steps per second: 188, episode reward: 0.789, mean reward: 0.015 [-0.007, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.141 [-0.150, 0.684], loss: 0.000130, mean_absolute_error: 0.411206, mean_q: 0.569306\n",
      " 17638/50000: episode: 225, duration: 0.255s, episode steps: 48, steps per second: 188, episode reward: 0.806, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.143 [-0.674, 0.170], loss: 0.000190, mean_absolute_error: 0.415256, mean_q: 0.573381\n",
      " 17707/50000: episode: 226, duration: 0.364s, episode steps: 69, steps per second: 190, episode reward: 0.699, mean reward: 0.010 [-0.009, 1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.153 [-0.871, 0.210], loss: 0.000465, mean_absolute_error: 0.417659, mean_q: 0.576873\n",
      " 17749/50000: episode: 227, duration: 0.224s, episode steps: 42, steps per second: 188, episode reward: 0.875, mean reward: 0.021 [-0.005, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.088 [-0.525, 0.170], loss: 0.000445, mean_absolute_error: 0.412896, mean_q: 0.560356\n",
      " 17813/50000: episode: 228, duration: 0.340s, episode steps: 64, steps per second: 188, episode reward: 0.679, mean reward: 0.011 [-0.009, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.189 [-0.898, 0.190], loss: 0.000448, mean_absolute_error: 0.413199, mean_q: 0.559882\n",
      " 17814/50000: episode: 229, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.015 [-0.021, -0.010], loss: 0.000105, mean_absolute_error: 0.419318, mean_q: 0.551255\n",
      " 17852/50000: episode: 230, duration: 0.204s, episode steps: 38, steps per second: 187, episode reward: 0.888, mean reward: 0.023 [-0.005, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.099 [-0.130, 0.473], loss: 0.000485, mean_absolute_error: 0.419890, mean_q: 0.563569\n",
      " 17868/50000: episode: 231, duration: 0.094s, episode steps: 16, steps per second: 171, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.045 [-0.090, 0.191], loss: 0.000567, mean_absolute_error: 0.421075, mean_q: 0.579016\n",
      " 17933/50000: episode: 232, duration: 0.358s, episode steps: 65, steps per second: 182, episode reward: 0.667, mean reward: 0.010 [-0.009, 1.000], mean action: 0.877 [0.000, 2.000], mean observation: 0.192 [-0.200, 0.935], loss: 0.000336, mean_absolute_error: 0.418230, mean_q: 0.569162\n",
      " 17994/50000: episode: 233, duration: 0.306s, episode steps: 61, steps per second: 199, episode reward: 0.718, mean reward: 0.012 [-0.008, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.175 [-0.790, 0.190], loss: 0.000248, mean_absolute_error: 0.424262, mean_q: 0.590145\n",
      " 18024/50000: episode: 234, duration: 0.159s, episode steps: 30, steps per second: 188, episode reward: 0.945, mean reward: 0.031 [-0.003, 1.000], mean action: 0.700 [0.000, 2.000], mean observation: 0.052 [-0.110, 0.295], loss: 0.000191, mean_absolute_error: 0.413156, mean_q: 0.559071\n",
      " 18102/50000: episode: 235, duration: 0.413s, episode steps: 78, steps per second: 189, episode reward: 0.708, mean reward: 0.009 [-0.008, 1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.143 [-0.794, 0.190], loss: 0.000448, mean_absolute_error: 0.419366, mean_q: 0.586920\n",
      " 18155/50000: episode: 236, duration: 0.284s, episode steps: 53, steps per second: 187, episode reward: 0.798, mean reward: 0.015 [-0.007, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.138 [-0.190, 0.669], loss: 0.000147, mean_absolute_error: 0.417404, mean_q: 0.571647\n",
      " 18219/50000: episode: 237, duration: 0.336s, episode steps: 64, steps per second: 190, episode reward: 0.677, mean reward: 0.011 [-0.009, 1.000], mean action: 1.094 [0.000, 2.000], mean observation: -0.190 [-0.902, 0.200], loss: 0.000118, mean_absolute_error: 0.418227, mean_q: 0.574243\n",
      " 18283/50000: episode: 238, duration: 0.337s, episode steps: 64, steps per second: 190, episode reward: 0.675, mean reward: 0.011 [-0.009, 1.000], mean action: 0.859 [0.000, 2.000], mean observation: 0.191 [-0.230, 0.905], loss: 0.000116, mean_absolute_error: 0.418414, mean_q: 0.581306\n",
      " 18338/50000: episode: 239, duration: 0.293s, episode steps: 55, steps per second: 188, episode reward: 0.781, mean reward: 0.014 [-0.007, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.145 [-0.160, 0.689], loss: 0.000078, mean_absolute_error: 0.420603, mean_q: 0.572415\n",
      " 18364/50000: episode: 240, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 0.956, mean reward: 0.037 [-0.003, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.056 [-0.100, 0.256], loss: 0.000809, mean_absolute_error: 0.419613, mean_q: 0.569345\n",
      " 18384/50000: episode: 241, duration: 0.109s, episode steps: 20, steps per second: 183, episode reward: 0.964, mean reward: 0.048 [-0.002, 1.000], mean action: 0.550 [0.000, 2.000], mean observation: 0.056 [-0.110, 0.244], loss: 0.000061, mean_absolute_error: 0.412260, mean_q: 0.578919\n",
      " 18413/50000: episode: 242, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 0.939, mean reward: 0.032 [-0.003, 1.000], mean action: 1.172 [0.000, 2.000], mean observation: -0.071 [-0.305, 0.120], loss: 0.000080, mean_absolute_error: 0.420886, mean_q: 0.579971\n",
      " 18482/50000: episode: 243, duration: 0.365s, episode steps: 69, steps per second: 189, episode reward: 0.700, mean reward: 0.010 [-0.008, 1.000], mean action: 0.913 [0.000, 2.000], mean observation: 0.166 [-0.180, 0.819], loss: 0.000351, mean_absolute_error: 0.421013, mean_q: 0.581179\n",
      " 18509/50000: episode: 244, duration: 0.147s, episode steps: 27, steps per second: 183, episode reward: 0.944, mean reward: 0.035 [-0.003, 1.000], mean action: 0.815 [0.000, 2.000], mean observation: 0.066 [-0.140, 0.307], loss: 0.000063, mean_absolute_error: 0.431550, mean_q: 0.603594\n",
      " 18564/50000: episode: 245, duration: 0.291s, episode steps: 55, steps per second: 189, episode reward: 0.739, mean reward: 0.013 [-0.008, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.174 [-0.210, 0.803], loss: 0.000512, mean_absolute_error: 0.425454, mean_q: 0.584198\n",
      " 18629/50000: episode: 246, duration: 0.343s, episode steps: 65, steps per second: 190, episode reward: 0.673, mean reward: 0.010 [-0.009, 1.000], mean action: 0.877 [0.000, 2.000], mean observation: 0.189 [-0.230, 0.919], loss: 0.000113, mean_absolute_error: 0.427475, mean_q: 0.592898\n",
      " 18669/50000: episode: 247, duration: 0.215s, episode steps: 40, steps per second: 186, episode reward: 0.881, mean reward: 0.022 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.102 [-0.120, 0.478], loss: 0.000272, mean_absolute_error: 0.426639, mean_q: 0.588839\n",
      " 18714/50000: episode: 248, duration: 0.247s, episode steps: 45, steps per second: 182, episode reward: 0.857, mean reward: 0.019 [-0.005, 1.000], mean action: 1.156 [0.000, 2.000], mean observation: -0.111 [-0.531, 0.150], loss: 0.000091, mean_absolute_error: 0.426319, mean_q: 0.592853\n",
      " 18715/50000: episode: 249, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.022 [-0.035, -0.010], loss: 0.000042, mean_absolute_error: 0.474095, mean_q: 0.663028\n",
      " 18793/50000: episode: 250, duration: 0.419s, episode steps: 78, steps per second: 186, episode reward: 0.846, mean reward: 0.011 [-0.005, 1.000], mean action: 1.064 [0.000, 2.000], mean observation: 0.012 [-0.167, 0.473], loss: 0.000147, mean_absolute_error: 0.423651, mean_q: 0.589180\n",
      " 18854/50000: episode: 251, duration: 0.330s, episode steps: 61, steps per second: 185, episode reward: 0.711, mean reward: 0.012 [-0.008, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.177 [-0.180, 0.838], loss: 0.000464, mean_absolute_error: 0.429102, mean_q: 0.591074\n",
      " 18901/50000: episode: 252, duration: 0.253s, episode steps: 47, steps per second: 186, episode reward: 0.822, mean reward: 0.017 [-0.006, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.130 [-0.639, 0.170], loss: 0.000051, mean_absolute_error: 0.424355, mean_q: 0.589755\n",
      " 18918/50000: episode: 253, duration: 0.097s, episode steps: 17, steps per second: 175, episode reward: 0.975, mean reward: 0.057 [-0.002, 1.000], mean action: 1.294 [0.000, 2.000], mean observation: -0.046 [-0.202, 0.100], loss: 0.000250, mean_absolute_error: 0.426573, mean_q: 0.586098\n",
      " 18919/50000: episode: 254, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [-0.010, 0.015], loss: 0.000194, mean_absolute_error: 0.423032, mean_q: 0.584402\n",
      " 18955/50000: episode: 255, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 0.895, mean reward: 0.025 [-0.005, 1.000], mean action: 1.194 [0.000, 2.000], mean observation: -0.098 [-0.450, 0.150], loss: 0.000604, mean_absolute_error: 0.429854, mean_q: 0.597322\n",
      " 18978/50000: episode: 256, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 0.963, mean reward: 0.042 [-0.002, 1.000], mean action: 1.217 [0.000, 2.000], mean observation: -0.053 [-0.227, 0.090], loss: 0.000095, mean_absolute_error: 0.424038, mean_q: 0.574155\n",
      " 19015/50000: episode: 257, duration: 0.203s, episode steps: 37, steps per second: 182, episode reward: 0.903, mean reward: 0.024 [-0.004, 1.000], mean action: 0.784 [0.000, 2.000], mean observation: 0.090 [-0.110, 0.412], loss: 0.000766, mean_absolute_error: 0.424046, mean_q: 0.576582\n",
      " 19080/50000: episode: 258, duration: 0.347s, episode steps: 65, steps per second: 187, episode reward: 0.658, mean reward: 0.010 [-0.009, 1.000], mean action: 1.138 [0.000, 2.000], mean observation: -0.200 [-0.922, 0.170], loss: 0.000539, mean_absolute_error: 0.433932, mean_q: 0.598611\n",
      " 19096/50000: episode: 259, duration: 0.087s, episode steps: 16, steps per second: 184, episode reward: 0.976, mean reward: 0.061 [-0.002, 1.000], mean action: 0.438 [0.000, 2.000], mean observation: 0.047 [-0.090, 0.192], loss: 0.000041, mean_absolute_error: 0.432286, mean_q: 0.609211\n",
      " 19135/50000: episode: 260, duration: 0.216s, episode steps: 39, steps per second: 181, episode reward: 0.887, mean reward: 0.023 [-0.005, 1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.098 [-0.473, 0.140], loss: 0.000053, mean_absolute_error: 0.427223, mean_q: 0.586619\n",
      " 19192/50000: episode: 261, duration: 0.302s, episode steps: 57, steps per second: 189, episode reward: 0.770, mean reward: 0.014 [-0.007, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.147 [-0.210, 0.725], loss: 0.000056, mean_absolute_error: 0.428306, mean_q: 0.587604\n",
      " 19258/50000: episode: 262, duration: 0.350s, episode steps: 66, steps per second: 189, episode reward: 0.722, mean reward: 0.011 [-0.008, 1.000], mean action: 1.061 [0.000, 2.000], mean observation: -0.160 [-0.771, 0.150], loss: 0.000215, mean_absolute_error: 0.434482, mean_q: 0.595520\n",
      " 19331/50000: episode: 263, duration: 0.389s, episode steps: 73, steps per second: 188, episode reward: 0.618, mean reward: 0.008 [-0.009, 1.000], mean action: 1.082 [0.000, 2.000], mean observation: -0.207 [-0.900, 0.180], loss: 0.000278, mean_absolute_error: 0.434260, mean_q: 0.598519\n",
      " 19392/50000: episode: 264, duration: 0.308s, episode steps: 61, steps per second: 198, episode reward: 0.713, mean reward: 0.012 [-0.009, 1.000], mean action: 0.869 [0.000, 2.000], mean observation: 0.174 [-0.180, 0.855], loss: 0.000073, mean_absolute_error: 0.431701, mean_q: 0.592074\n",
      " 19442/50000: episode: 265, duration: 0.268s, episode steps: 50, steps per second: 187, episode reward: 0.811, mean reward: 0.016 [-0.007, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.133 [-0.180, 0.660], loss: 0.000340, mean_absolute_error: 0.433336, mean_q: 0.594195\n",
      " 19497/50000: episode: 266, duration: 0.292s, episode steps: 55, steps per second: 188, episode reward: 0.745, mean reward: 0.014 [-0.008, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.169 [-0.788, 0.160], loss: 0.000161, mean_absolute_error: 0.426703, mean_q: 0.589686\n",
      " 19498/50000: episode: 267, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.044 [-0.098, 0.010], loss: 0.000042, mean_absolute_error: 0.406862, mean_q: 0.550057\n",
      " 19522/50000: episode: 268, duration: 0.133s, episode steps: 24, steps per second: 180, episode reward: 0.955, mean reward: 0.040 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.061 [-0.110, 0.269], loss: 0.000061, mean_absolute_error: 0.437492, mean_q: 0.591431\n",
      " 19593/50000: episode: 269, duration: 0.357s, episode steps: 71, steps per second: 199, episode reward: 0.645, mean reward: 0.009 [-0.009, 1.000], mean action: 0.901 [0.000, 2.000], mean observation: 0.191 [-0.240, 0.936], loss: 0.000244, mean_absolute_error: 0.433537, mean_q: 0.597110\n",
      " 19662/50000: episode: 270, duration: 0.388s, episode steps: 69, steps per second: 178, episode reward: 0.641, mean reward: 0.009 [-0.009, 1.000], mean action: 1.116 [0.000, 2.000], mean observation: -0.201 [-0.926, 0.170], loss: 0.000120, mean_absolute_error: 0.436388, mean_q: 0.602925\n",
      " 19702/50000: episode: 271, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 0.896, mean reward: 0.022 [-0.004, 1.000], mean action: 1.075 [0.000, 2.000], mean observation: -0.087 [-0.445, 0.140], loss: 0.000224, mean_absolute_error: 0.438991, mean_q: 0.609838\n",
      " 19703/50000: episode: 272, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.014 [-0.017, -0.010], loss: 0.000054, mean_absolute_error: 0.382909, mean_q: 0.450534\n",
      " 19713/50000: episode: 273, duration: 0.059s, episode steps: 10, steps per second: 168, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.038 [-0.139, 0.070], loss: 0.000047, mean_absolute_error: 0.437564, mean_q: 0.603288\n",
      " 19726/50000: episode: 274, duration: 0.072s, episode steps: 13, steps per second: 182, episode reward: 0.986, mean reward: 0.076 [-0.001, 1.000], mean action: 1.077 [0.000, 2.000], mean observation: -0.043 [-0.138, 0.060], loss: 0.000033, mean_absolute_error: 0.430605, mean_q: 0.610672\n",
      " 19757/50000: episode: 275, duration: 0.170s, episode steps: 31, steps per second: 183, episode reward: 0.926, mean reward: 0.030 [-0.004, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.076 [-0.378, 0.160], loss: 0.000381, mean_absolute_error: 0.441770, mean_q: 0.605414\n",
      " 19812/50000: episode: 276, duration: 0.295s, episode steps: 55, steps per second: 186, episode reward: 0.796, mean reward: 0.014 [-0.006, 1.000], mean action: 0.891 [0.000, 2.000], mean observation: 0.137 [-0.180, 0.642], loss: 0.000046, mean_absolute_error: 0.435851, mean_q: 0.604908\n",
      " 19858/50000: episode: 277, duration: 0.247s, episode steps: 46, steps per second: 187, episode reward: 0.862, mean reward: 0.019 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.091 [-0.551, 0.170], loss: 0.000548, mean_absolute_error: 0.428701, mean_q: 0.592875\n",
      " 19903/50000: episode: 278, duration: 0.243s, episode steps: 45, steps per second: 185, episode reward: 0.860, mean reward: 0.019 [-0.005, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.107 [-0.537, 0.170], loss: 0.000438, mean_absolute_error: 0.436577, mean_q: 0.597951\n",
      " 19954/50000: episode: 279, duration: 0.273s, episode steps: 51, steps per second: 187, episode reward: 0.796, mean reward: 0.016 [-0.007, 1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.143 [-0.680, 0.160], loss: 0.000293, mean_absolute_error: 0.435785, mean_q: 0.607683\n",
      " 20054/50000: episode: 280, duration: 0.528s, episode steps: 100, steps per second: 189, episode reward: 0.780, mean reward: 0.008 [-0.005, 1.000], mean action: 1.050 [0.000, 2.000], mean observation: 0.016 [-0.191, 0.548], loss: 0.000432, mean_absolute_error: 0.439393, mean_q: 0.607575\n",
      " 20103/50000: episode: 281, duration: 0.259s, episode steps: 49, steps per second: 189, episode reward: 0.833, mean reward: 0.017 [-0.006, 1.000], mean action: 1.082 [0.000, 2.000], mean observation: -0.121 [-0.591, 0.150], loss: 0.000047, mean_absolute_error: 0.436854, mean_q: 0.614868\n",
      " 20153/50000: episode: 282, duration: 0.270s, episode steps: 50, steps per second: 185, episode reward: 0.813, mean reward: 0.016 [-0.006, 1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.134 [-0.634, 0.170], loss: 0.000074, mean_absolute_error: 0.440039, mean_q: 0.602769\n",
      " 20200/50000: episode: 283, duration: 0.253s, episode steps: 47, steps per second: 186, episode reward: 0.825, mean reward: 0.018 [-0.006, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.136 [-0.572, 0.150], loss: 0.000253, mean_absolute_error: 0.426613, mean_q: 0.582503\n",
      " 20256/50000: episode: 284, duration: 0.321s, episode steps: 56, steps per second: 174, episode reward: 0.797, mean reward: 0.014 [-0.007, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.125 [-0.656, 0.180], loss: 0.000517, mean_absolute_error: 0.436643, mean_q: 0.602785\n",
      " 20306/50000: episode: 285, duration: 0.267s, episode steps: 50, steps per second: 187, episode reward: 0.799, mean reward: 0.016 [-0.007, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.141 [-0.220, 0.713], loss: 0.000146, mean_absolute_error: 0.435864, mean_q: 0.602864\n",
      " 20339/50000: episode: 286, duration: 0.181s, episode steps: 33, steps per second: 182, episode reward: 0.922, mean reward: 0.028 [-0.004, 1.000], mean action: 1.182 [0.000, 2.000], mean observation: -0.078 [-0.375, 0.130], loss: 0.000386, mean_absolute_error: 0.439496, mean_q: 0.598804\n",
      " 20371/50000: episode: 287, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 0.931, mean reward: 0.029 [-0.003, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.071 [-0.342, 0.140], loss: 0.000072, mean_absolute_error: 0.439515, mean_q: 0.600805\n",
      " 20415/50000: episode: 288, duration: 0.237s, episode steps: 44, steps per second: 185, episode reward: 0.862, mean reward: 0.020 [-0.005, 1.000], mean action: 0.818 [0.000, 2.000], mean observation: 0.110 [-0.160, 0.513], loss: 0.000081, mean_absolute_error: 0.440177, mean_q: 0.600975\n",
      " 20466/50000: episode: 289, duration: 0.264s, episode steps: 51, steps per second: 193, episode reward: 0.804, mean reward: 0.016 [-0.006, 1.000], mean action: 0.882 [0.000, 2.000], mean observation: 0.139 [-0.190, 0.644], loss: 0.000306, mean_absolute_error: 0.440628, mean_q: 0.594982\n",
      " 20515/50000: episode: 290, duration: 0.262s, episode steps: 49, steps per second: 187, episode reward: 0.949, mean reward: 0.019 [-0.002, 1.000], mean action: 1.102 [0.000, 2.000], mean observation: -0.043 [-0.154, 0.160], loss: 0.000405, mean_absolute_error: 0.436113, mean_q: 0.597368\n",
      " 20516/50000: episode: 291, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [-0.010, 0.074], loss: 0.000034, mean_absolute_error: 0.453187, mean_q: 0.687522\n",
      " 20531/50000: episode: 292, duration: 0.086s, episode steps: 15, steps per second: 175, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.045 [-0.070, 0.182], loss: 0.000032, mean_absolute_error: 0.441750, mean_q: 0.598012\n",
      " 20587/50000: episode: 293, duration: 0.326s, episode steps: 56, steps per second: 172, episode reward: 0.796, mean reward: 0.014 [-0.007, 1.000], mean action: 0.911 [0.000, 2.000], mean observation: 0.132 [-0.180, 0.660], loss: 0.000131, mean_absolute_error: 0.433631, mean_q: 0.603557\n",
      " 20646/50000: episode: 294, duration: 0.314s, episode steps: 59, steps per second: 188, episode reward: 0.712, mean reward: 0.012 [-0.009, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.176 [-0.270, 0.909], loss: 0.000251, mean_absolute_error: 0.433965, mean_q: 0.600692\n",
      " 20700/50000: episode: 295, duration: 0.291s, episode steps: 54, steps per second: 185, episode reward: 0.781, mean reward: 0.014 [-0.007, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.147 [-0.200, 0.707], loss: 0.000134, mean_absolute_error: 0.439148, mean_q: 0.607958\n",
      " 20734/50000: episode: 296, duration: 0.195s, episode steps: 34, steps per second: 174, episode reward: 0.908, mean reward: 0.027 [-0.004, 1.000], mean action: 1.147 [0.000, 2.000], mean observation: -0.087 [-0.432, 0.160], loss: 0.000051, mean_absolute_error: 0.435798, mean_q: 0.603433\n",
      " 20779/50000: episode: 297, duration: 0.240s, episode steps: 45, steps per second: 187, episode reward: 0.845, mean reward: 0.019 [-0.006, 1.000], mean action: 1.156 [0.000, 2.000], mean observation: -0.121 [-0.567, 0.170], loss: 0.000305, mean_absolute_error: 0.440510, mean_q: 0.608231\n",
      " 20849/50000: episode: 298, duration: 0.345s, episode steps: 70, steps per second: 203, episode reward: 0.624, mean reward: 0.009 [-0.010, 1.000], mean action: 1.071 [0.000, 2.000], mean observation: -0.206 [-0.984, 0.190], loss: 0.000058, mean_absolute_error: 0.442263, mean_q: 0.616701\n",
      " 20916/50000: episode: 299, duration: 0.361s, episode steps: 67, steps per second: 186, episode reward: 0.663, mean reward: 0.010 [-0.009, 1.000], mean action: 0.866 [0.000, 2.000], mean observation: 0.188 [-0.240, 0.949], loss: 0.000272, mean_absolute_error: 0.438630, mean_q: 0.602782\n",
      " 20958/50000: episode: 300, duration: 0.195s, episode steps: 42, steps per second: 215, episode reward: 0.892, mean reward: 0.021 [-0.004, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.093 [-0.110, 0.402], loss: 0.000225, mean_absolute_error: 0.433574, mean_q: 0.591074\n",
      " 21014/50000: episode: 301, duration: 0.314s, episode steps: 56, steps per second: 178, episode reward: 0.739, mean reward: 0.013 [-0.008, 1.000], mean action: 1.107 [0.000, 2.000], mean observation: -0.169 [-0.818, 0.180], loss: 0.000198, mean_absolute_error: 0.437073, mean_q: 0.598433\n",
      " 21042/50000: episode: 302, duration: 0.156s, episode steps: 28, steps per second: 180, episode reward: 0.930, mean reward: 0.033 [-0.004, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.076 [-0.377, 0.150], loss: 0.000387, mean_absolute_error: 0.441951, mean_q: 0.614340\n",
      " 21098/50000: episode: 303, duration: 0.321s, episode steps: 56, steps per second: 174, episode reward: 0.744, mean reward: 0.013 [-0.008, 1.000], mean action: 1.107 [0.000, 2.000], mean observation: -0.166 [-0.806, 0.190], loss: 0.000148, mean_absolute_error: 0.439003, mean_q: 0.604503\n",
      " 21113/50000: episode: 304, duration: 0.079s, episode steps: 15, steps per second: 191, episode reward: 0.981, mean reward: 0.065 [-0.002, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.043 [-0.170, 0.070], loss: 0.000087, mean_absolute_error: 0.439991, mean_q: 0.618243\n",
      " 21158/50000: episode: 305, duration: 0.252s, episode steps: 45, steps per second: 179, episode reward: 0.831, mean reward: 0.018 [-0.006, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.128 [-0.190, 0.640], loss: 0.000313, mean_absolute_error: 0.449857, mean_q: 0.626268\n",
      " 21200/50000: episode: 306, duration: 0.225s, episode steps: 42, steps per second: 187, episode reward: 0.870, mean reward: 0.021 [-0.005, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.095 [-0.537, 0.170], loss: 0.000325, mean_absolute_error: 0.437603, mean_q: 0.605150\n",
      " 21222/50000: episode: 307, duration: 0.133s, episode steps: 22, steps per second: 165, episode reward: 0.962, mean reward: 0.044 [-0.002, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.053 [-0.246, 0.110], loss: 0.000046, mean_absolute_error: 0.444802, mean_q: 0.627809\n",
      " 21253/50000: episode: 308, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 0.940, mean reward: 0.030 [-0.003, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.052 [-0.310, 0.130], loss: 0.000926, mean_absolute_error: 0.453609, mean_q: 0.623430\n",
      " 21317/50000: episode: 309, duration: 0.361s, episode steps: 64, steps per second: 177, episode reward: 0.657, mean reward: 0.010 [-0.010, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.201 [-0.962, 0.210], loss: 0.000288, mean_absolute_error: 0.436913, mean_q: 0.603619\n",
      " 21334/50000: episode: 310, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 0.978, mean reward: 0.058 [-0.002, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: 0.046 [-0.060, 0.171], loss: 0.000297, mean_absolute_error: 0.443543, mean_q: 0.626692\n",
      " 21386/50000: episode: 311, duration: 0.273s, episode steps: 52, steps per second: 190, episode reward: 0.760, mean reward: 0.015 [-0.008, 1.000], mean action: 0.885 [0.000, 2.000], mean observation: 0.162 [-0.230, 0.812], loss: 0.000047, mean_absolute_error: 0.438134, mean_q: 0.606004\n",
      " 21420/50000: episode: 312, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 0.916, mean reward: 0.027 [-0.004, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.080 [-0.393, 0.140], loss: 0.000342, mean_absolute_error: 0.443177, mean_q: 0.625354\n",
      " 21430/50000: episode: 313, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: -0.039 [-0.135, 0.050], loss: 0.000034, mean_absolute_error: 0.452669, mean_q: 0.634193\n",
      " 21431/50000: episode: 314, duration: 0.013s, episode steps: 1, steps per second: 79, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.012 [-0.035, 0.010], loss: 0.000067, mean_absolute_error: 0.420769, mean_q: 0.514987\n",
      " 21476/50000: episode: 315, duration: 0.254s, episode steps: 45, steps per second: 177, episode reward: 0.878, mean reward: 0.020 [-0.005, 1.000], mean action: 0.844 [0.000, 2.000], mean observation: 0.097 [-0.140, 0.450], loss: 0.000170, mean_absolute_error: 0.431357, mean_q: 0.595937\n",
      " 21486/50000: episode: 316, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 0.988, mean reward: 0.099 [-0.001, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.038 [-0.070, 0.145], loss: 0.000043, mean_absolute_error: 0.448182, mean_q: 0.642240\n",
      " 21555/50000: episode: 317, duration: 0.360s, episode steps: 69, steps per second: 192, episode reward: 0.643, mean reward: 0.009 [-0.010, 1.000], mean action: 1.072 [0.000, 2.000], mean observation: -0.196 [-0.977, 0.190], loss: 0.000208, mean_absolute_error: 0.442121, mean_q: 0.615456\n",
      " 21591/50000: episode: 318, duration: 0.193s, episode steps: 36, steps per second: 186, episode reward: 0.903, mean reward: 0.025 [-0.004, 1.000], mean action: 1.194 [0.000, 2.000], mean observation: -0.090 [-0.430, 0.140], loss: 0.000278, mean_absolute_error: 0.437594, mean_q: 0.613063\n",
      " 21642/50000: episode: 319, duration: 0.269s, episode steps: 51, steps per second: 189, episode reward: 0.789, mean reward: 0.015 [-0.007, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.142 [-0.726, 0.180], loss: 0.000228, mean_absolute_error: 0.437172, mean_q: 0.599766\n",
      " 21660/50000: episode: 320, duration: 0.102s, episode steps: 18, steps per second: 177, episode reward: 0.971, mean reward: 0.054 [-0.002, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.051 [-0.110, 0.217], loss: 0.000052, mean_absolute_error: 0.449922, mean_q: 0.615886\n",
      " 21693/50000: episode: 321, duration: 0.179s, episode steps: 33, steps per second: 184, episode reward: 0.913, mean reward: 0.028 [-0.004, 1.000], mean action: 1.242 [0.000, 2.000], mean observation: -0.088 [-0.399, 0.140], loss: 0.000041, mean_absolute_error: 0.449433, mean_q: 0.631002\n",
      " 21730/50000: episode: 322, duration: 0.202s, episode steps: 37, steps per second: 183, episode reward: 0.894, mean reward: 0.024 [-0.005, 1.000], mean action: 1.189 [0.000, 2.000], mean observation: -0.094 [-0.468, 0.160], loss: 0.000036, mean_absolute_error: 0.434908, mean_q: 0.596659\n",
      " 21752/50000: episode: 323, duration: 0.121s, episode steps: 22, steps per second: 182, episode reward: 0.962, mean reward: 0.044 [-0.002, 1.000], mean action: 0.636 [0.000, 2.000], mean observation: 0.053 [-0.100, 0.246], loss: 0.000039, mean_absolute_error: 0.447500, mean_q: 0.623454\n",
      " 21799/50000: episode: 324, duration: 0.251s, episode steps: 47, steps per second: 187, episode reward: 0.817, mean reward: 0.017 [-0.007, 1.000], mean action: 1.149 [0.000, 2.000], mean observation: -0.137 [-0.651, 0.190], loss: 0.000303, mean_absolute_error: 0.444828, mean_q: 0.624179\n",
      " 21821/50000: episode: 325, duration: 0.120s, episode steps: 22, steps per second: 183, episode reward: 0.959, mean reward: 0.044 [-0.003, 1.000], mean action: 1.318 [0.000, 2.000], mean observation: -0.057 [-0.263, 0.120], loss: 0.000045, mean_absolute_error: 0.445643, mean_q: 0.625945\n",
      " 21883/50000: episode: 326, duration: 0.333s, episode steps: 62, steps per second: 186, episode reward: 0.694, mean reward: 0.011 [-0.009, 1.000], mean action: 1.113 [0.000, 2.000], mean observation: -0.182 [-0.903, 0.200], loss: 0.000092, mean_absolute_error: 0.441593, mean_q: 0.619292\n",
      " 21890/50000: episode: 327, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.037 [-0.060, 0.123], loss: 0.000082, mean_absolute_error: 0.431280, mean_q: 0.610658\n",
      " 21952/50000: episode: 328, duration: 0.326s, episode steps: 62, steps per second: 190, episode reward: 0.855, mean reward: 0.014 [-0.005, 1.000], mean action: 0.871 [0.000, 2.000], mean observation: 0.088 [-0.130, 0.462], loss: 0.000364, mean_absolute_error: 0.440100, mean_q: 0.616353\n",
      " 21953/50000: episode: 329, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.040 [-0.071, -0.010], loss: 0.000064, mean_absolute_error: 0.464243, mean_q: 0.664943\n",
      " 22014/50000: episode: 330, duration: 0.325s, episode steps: 61, steps per second: 188, episode reward: 0.692, mean reward: 0.011 [-0.009, 1.000], mean action: 1.131 [0.000, 2.000], mean observation: -0.187 [-0.903, 0.180], loss: 0.000251, mean_absolute_error: 0.442604, mean_q: 0.616230\n",
      " 22020/50000: episode: 331, duration: 0.036s, episode steps: 6, steps per second: 165, episode reward: 0.994, mean reward: 0.166 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.116, 0.060], loss: 0.000106, mean_absolute_error: 0.441203, mean_q: 0.599265\n",
      " 22065/50000: episode: 332, duration: 0.242s, episode steps: 45, steps per second: 186, episode reward: 0.855, mean reward: 0.019 [-0.005, 1.000], mean action: 0.844 [0.000, 2.000], mean observation: 0.114 [-0.160, 0.533], loss: 0.000347, mean_absolute_error: 0.444022, mean_q: 0.611626\n",
      " 22066/50000: episode: 333, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.024 [-0.048, 0.000], loss: 0.000014, mean_absolute_error: 0.436116, mean_q: 0.570208\n",
      " 22115/50000: episode: 334, duration: 0.261s, episode steps: 49, steps per second: 188, episode reward: 0.779, mean reward: 0.016 [-0.008, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.156 [-0.240, 0.780], loss: 0.000043, mean_absolute_error: 0.440395, mean_q: 0.618983\n",
      " 22141/50000: episode: 335, duration: 0.143s, episode steps: 26, steps per second: 182, episode reward: 0.966, mean reward: 0.037 [-0.002, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.049 [-0.080, 0.192], loss: 0.000329, mean_absolute_error: 0.441732, mean_q: 0.628512\n",
      " 22156/50000: episode: 336, duration: 0.083s, episode steps: 15, steps per second: 180, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 1.467 [0.000, 2.000], mean observation: -0.042 [-0.181, 0.080], loss: 0.000042, mean_absolute_error: 0.450110, mean_q: 0.624743\n",
      " 22193/50000: episode: 337, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 0.897, mean reward: 0.024 [-0.005, 1.000], mean action: 0.811 [0.000, 2.000], mean observation: 0.092 [-0.160, 0.451], loss: 0.000398, mean_absolute_error: 0.446364, mean_q: 0.614801\n",
      " 22250/50000: episode: 338, duration: 0.389s, episode steps: 57, steps per second: 147, episode reward: 0.703, mean reward: 0.012 [-0.009, 1.000], mean action: 0.895 [0.000, 2.000], mean observation: 0.187 [-0.260, 0.939], loss: 0.000459, mean_absolute_error: 0.443370, mean_q: 0.618422\n",
      " 22297/50000: episode: 339, duration: 0.265s, episode steps: 47, steps per second: 178, episode reward: 0.811, mean reward: 0.017 [-0.007, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.140 [-0.683, 0.200], loss: 0.000371, mean_absolute_error: 0.442853, mean_q: 0.623008\n",
      " 22320/50000: episode: 340, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 0.955, mean reward: 0.042 [-0.003, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.062 [-0.130, 0.272], loss: 0.000054, mean_absolute_error: 0.443395, mean_q: 0.607066\n",
      " 22351/50000: episode: 341, duration: 0.165s, episode steps: 31, steps per second: 188, episode reward: 0.933, mean reward: 0.030 [-0.003, 1.000], mean action: 0.742 [0.000, 2.000], mean observation: 0.074 [-0.100, 0.316], loss: 0.000042, mean_absolute_error: 0.449386, mean_q: 0.640346\n",
      " 22417/50000: episode: 342, duration: 0.361s, episode steps: 66, steps per second: 183, episode reward: 0.642, mean reward: 0.010 [-0.010, 1.000], mean action: 1.121 [0.000, 2.000], mean observation: -0.204 [-0.984, 0.220], loss: 0.000168, mean_absolute_error: 0.442244, mean_q: 0.621835\n",
      " 22481/50000: episode: 343, duration: 0.347s, episode steps: 64, steps per second: 185, episode reward: 0.662, mean reward: 0.010 [-0.009, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.199 [-0.938, 0.200], loss: 0.000142, mean_absolute_error: 0.439567, mean_q: 0.622586\n",
      " 22533/50000: episode: 344, duration: 0.391s, episode steps: 52, steps per second: 133, episode reward: 0.733, mean reward: 0.014 [-0.009, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.182 [-0.240, 0.886], loss: 0.000304, mean_absolute_error: 0.447066, mean_q: 0.629500\n",
      " 22570/50000: episode: 345, duration: 0.238s, episode steps: 37, steps per second: 155, episode reward: 0.908, mean reward: 0.025 [-0.004, 1.000], mean action: 0.865 [0.000, 2.000], mean observation: 0.084 [-0.140, 0.407], loss: 0.000069, mean_absolute_error: 0.444042, mean_q: 0.613169\n",
      " 22635/50000: episode: 346, duration: 0.362s, episode steps: 65, steps per second: 180, episode reward: 0.674, mean reward: 0.010 [-0.009, 1.000], mean action: 1.123 [0.000, 2.000], mean observation: -0.187 [-0.930, 0.210], loss: 0.000181, mean_absolute_error: 0.443217, mean_q: 0.620237\n",
      " 22675/50000: episode: 347, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 0.880, mean reward: 0.022 [-0.005, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.100 [-0.190, 0.511], loss: 0.000055, mean_absolute_error: 0.445422, mean_q: 0.616349\n",
      " 22725/50000: episode: 348, duration: 0.282s, episode steps: 50, steps per second: 177, episode reward: 0.802, mean reward: 0.016 [-0.007, 1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.140 [-0.684, 0.190], loss: 0.000147, mean_absolute_error: 0.445594, mean_q: 0.634452\n",
      " 22770/50000: episode: 349, duration: 0.247s, episode steps: 45, steps per second: 182, episode reward: 0.837, mean reward: 0.019 [-0.006, 1.000], mean action: 0.822 [0.000, 2.000], mean observation: 0.126 [-0.160, 0.599], loss: 0.000060, mean_absolute_error: 0.451187, mean_q: 0.636581\n",
      " 22811/50000: episode: 350, duration: 0.250s, episode steps: 41, steps per second: 164, episode reward: 0.864, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.112 [-0.530, 0.180], loss: 0.000240, mean_absolute_error: 0.441128, mean_q: 0.622985\n",
      " 22864/50000: episode: 351, duration: 0.297s, episode steps: 53, steps per second: 179, episode reward: 0.753, mean reward: 0.014 [-0.008, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.163 [-0.220, 0.825], loss: 0.000210, mean_absolute_error: 0.445370, mean_q: 0.624720\n",
      " 22917/50000: episode: 352, duration: 0.292s, episode steps: 53, steps per second: 181, episode reward: 0.785, mean reward: 0.015 [-0.007, 1.000], mean action: 1.094 [0.000, 2.000], mean observation: -0.145 [-0.720, 0.200], loss: 0.000165, mean_absolute_error: 0.442704, mean_q: 0.617505\n",
      " 22953/50000: episode: 353, duration: 0.195s, episode steps: 36, steps per second: 184, episode reward: 0.917, mean reward: 0.025 [-0.003, 1.000], mean action: 0.778 [0.000, 2.000], mean observation: 0.082 [-0.090, 0.347], loss: 0.000050, mean_absolute_error: 0.449957, mean_q: 0.641740\n",
      " 22954/50000: episode: 354, duration: 0.012s, episode steps: 1, steps per second: 86, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.041 [-0.092, 0.010], loss: 0.000022, mean_absolute_error: 0.474838, mean_q: 0.671282\n",
      " 22996/50000: episode: 355, duration: 0.229s, episode steps: 42, steps per second: 183, episode reward: 0.862, mean reward: 0.021 [-0.006, 1.000], mean action: 1.048 [0.000, 2.000], mean observation: -0.110 [-0.565, 0.180], loss: 0.000657, mean_absolute_error: 0.446573, mean_q: 0.620313\n",
      " 23023/50000: episode: 356, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 0.945, mean reward: 0.035 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.067 [-0.100, 0.291], loss: 0.000614, mean_absolute_error: 0.452042, mean_q: 0.645143\n",
      " 23078/50000: episode: 357, duration: 0.293s, episode steps: 55, steps per second: 188, episode reward: 0.739, mean reward: 0.013 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.174 [-0.807, 0.210], loss: 0.000062, mean_absolute_error: 0.453228, mean_q: 0.638478\n",
      " 23129/50000: episode: 358, duration: 0.265s, episode steps: 51, steps per second: 192, episode reward: 0.840, mean reward: 0.016 [-0.006, 1.000], mean action: 0.863 [0.000, 2.000], mean observation: 0.110 [-0.200, 0.579], loss: 0.000602, mean_absolute_error: 0.450277, mean_q: 0.635460\n",
      " 23167/50000: episode: 359, duration: 0.200s, episode steps: 38, steps per second: 190, episode reward: 0.892, mean reward: 0.023 [-0.004, 1.000], mean action: 0.789 [0.000, 2.000], mean observation: 0.097 [-0.120, 0.450], loss: 0.000195, mean_absolute_error: 0.439420, mean_q: 0.623811\n",
      " 23198/50000: episode: 360, duration: 0.164s, episode steps: 31, steps per second: 189, episode reward: 0.929, mean reward: 0.030 [-0.004, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.074 [-0.351, 0.110], loss: 0.000053, mean_absolute_error: 0.450288, mean_q: 0.630689\n",
      " 23259/50000: episode: 361, duration: 0.328s, episode steps: 61, steps per second: 186, episode reward: 0.687, mean reward: 0.011 [-0.009, 1.000], mean action: 1.131 [0.000, 2.000], mean observation: -0.190 [-0.921, 0.210], loss: 0.000033, mean_absolute_error: 0.438208, mean_q: 0.618968\n",
      " 23260/50000: episode: 362, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [-0.010, 0.075], loss: 0.000020, mean_absolute_error: 0.430405, mean_q: 0.584558\n",
      " 23317/50000: episode: 363, duration: 0.301s, episode steps: 57, steps per second: 190, episode reward: 0.723, mean reward: 0.013 [-0.009, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.175 [-0.260, 0.878], loss: 0.000354, mean_absolute_error: 0.445792, mean_q: 0.633121\n",
      " 23363/50000: episode: 364, duration: 0.243s, episode steps: 46, steps per second: 189, episode reward: 0.813, mean reward: 0.018 [-0.007, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.139 [-0.190, 0.687], loss: 0.000144, mean_absolute_error: 0.439393, mean_q: 0.620447\n",
      " 23377/50000: episode: 365, duration: 0.079s, episode steps: 14, steps per second: 176, episode reward: 0.980, mean reward: 0.070 [-0.002, 1.000], mean action: 1.643 [0.000, 2.000], mean observation: -0.044 [-0.181, 0.110], loss: 0.000069, mean_absolute_error: 0.436549, mean_q: 0.629477\n",
      " 23405/50000: episode: 366, duration: 0.152s, episode steps: 28, steps per second: 184, episode reward: 0.940, mean reward: 0.034 [-0.003, 1.000], mean action: 0.714 [0.000, 2.000], mean observation: 0.068 [-0.120, 0.323], loss: 0.000094, mean_absolute_error: 0.444362, mean_q: 0.630549\n",
      " 23464/50000: episode: 367, duration: 0.317s, episode steps: 59, steps per second: 186, episode reward: 0.680, mean reward: 0.012 [-0.010, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.196 [-0.260, 0.982], loss: 0.000099, mean_absolute_error: 0.441401, mean_q: 0.624055\n",
      " 23469/50000: episode: 368, duration: 0.034s, episode steps: 5, steps per second: 148, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.039 [-0.050, 0.114], loss: 0.000024, mean_absolute_error: 0.458852, mean_q: 0.673293\n",
      " 23495/50000: episode: 369, duration: 0.145s, episode steps: 26, steps per second: 180, episode reward: 0.948, mean reward: 0.036 [-0.003, 1.000], mean action: 0.731 [0.000, 2.000], mean observation: 0.062 [-0.120, 0.299], loss: 0.000084, mean_absolute_error: 0.447684, mean_q: 0.639777\n",
      " 23524/50000: episode: 370, duration: 0.154s, episode steps: 29, steps per second: 188, episode reward: 0.928, mean reward: 0.032 [-0.004, 1.000], mean action: 1.276 [0.000, 2.000], mean observation: -0.077 [-0.375, 0.140], loss: 0.000142, mean_absolute_error: 0.447638, mean_q: 0.638914\n",
      " 23525/50000: episode: 371, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.025 [-0.060, 0.010], loss: 0.000068, mean_absolute_error: 0.439365, mean_q: 0.605469\n",
      " 23585/50000: episode: 372, duration: 0.312s, episode steps: 60, steps per second: 192, episode reward: 0.750, mean reward: 0.012 [-0.008, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.144 [-0.797, 0.210], loss: 0.000228, mean_absolute_error: 0.446021, mean_q: 0.634985\n",
      " 23639/50000: episode: 373, duration: 0.295s, episode steps: 54, steps per second: 183, episode reward: 0.738, mean reward: 0.014 [-0.008, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.174 [-0.220, 0.840], loss: 0.000105, mean_absolute_error: 0.447167, mean_q: 0.638833\n",
      " 23659/50000: episode: 374, duration: 0.111s, episode steps: 20, steps per second: 180, episode reward: 0.968, mean reward: 0.048 [-0.002, 1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.046 [-0.220, 0.110], loss: 0.000131, mean_absolute_error: 0.442199, mean_q: 0.636925\n",
      " 23703/50000: episode: 375, duration: 0.239s, episode steps: 44, steps per second: 184, episode reward: 0.864, mean reward: 0.020 [-0.005, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.108 [-0.140, 0.522], loss: 0.000246, mean_absolute_error: 0.439235, mean_q: 0.623613\n",
      " 23747/50000: episode: 376, duration: 0.235s, episode steps: 44, steps per second: 187, episode reward: 0.857, mean reward: 0.019 [-0.005, 1.000], mean action: 1.159 [0.000, 2.000], mean observation: -0.112 [-0.547, 0.160], loss: 0.000150, mean_absolute_error: 0.446909, mean_q: 0.634463\n",
      " 23787/50000: episode: 377, duration: 0.214s, episode steps: 40, steps per second: 187, episode reward: 0.880, mean reward: 0.022 [-0.005, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.100 [-0.190, 0.504], loss: 0.000052, mean_absolute_error: 0.436942, mean_q: 0.613353\n",
      " 23804/50000: episode: 378, duration: 0.095s, episode steps: 17, steps per second: 180, episode reward: 0.976, mean reward: 0.057 [-0.002, 1.000], mean action: 0.647 [0.000, 2.000], mean observation: 0.046 [-0.090, 0.191], loss: 0.000481, mean_absolute_error: 0.442664, mean_q: 0.615521\n",
      " 23845/50000: episode: 379, duration: 0.216s, episode steps: 41, steps per second: 189, episode reward: 0.901, mean reward: 0.022 [-0.004, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.073 [-0.419, 0.150], loss: 0.000307, mean_absolute_error: 0.448766, mean_q: 0.643400\n",
      " 23897/50000: episode: 380, duration: 0.275s, episode steps: 52, steps per second: 189, episode reward: 0.737, mean reward: 0.014 [-0.009, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.180 [-0.270, 0.867], loss: 0.000044, mean_absolute_error: 0.443172, mean_q: 0.632168\n",
      " 23936/50000: episode: 381, duration: 0.209s, episode steps: 39, steps per second: 187, episode reward: 0.897, mean reward: 0.023 [-0.004, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.097 [-0.140, 0.382], loss: 0.000371, mean_absolute_error: 0.453025, mean_q: 0.651575\n",
      " 23988/50000: episode: 382, duration: 0.278s, episode steps: 52, steps per second: 187, episode reward: 0.756, mean reward: 0.015 [-0.008, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.167 [-0.240, 0.797], loss: 0.000057, mean_absolute_error: 0.454631, mean_q: 0.651956\n",
      " 24029/50000: episode: 383, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 0.876, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.105 [-0.485, 0.160], loss: 0.000104, mean_absolute_error: 0.453551, mean_q: 0.654030\n",
      " 24052/50000: episode: 384, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 0.956, mean reward: 0.042 [-0.003, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.059 [-0.100, 0.275], loss: 0.000039, mean_absolute_error: 0.450284, mean_q: 0.644507\n",
      " 24090/50000: episode: 385, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 0.901, mean reward: 0.024 [-0.004, 1.000], mean action: 0.895 [0.000, 2.000], mean observation: 0.088 [-0.170, 0.431], loss: 0.000280, mean_absolute_error: 0.448925, mean_q: 0.643037\n",
      " 24111/50000: episode: 386, duration: 0.113s, episode steps: 21, steps per second: 186, episode reward: 0.968, mean reward: 0.046 [-0.002, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.051 [-0.200, 0.090], loss: 0.000589, mean_absolute_error: 0.450214, mean_q: 0.648008\n",
      " 24144/50000: episode: 387, duration: 0.177s, episode steps: 33, steps per second: 186, episode reward: 0.922, mean reward: 0.028 [-0.004, 1.000], mean action: 1.242 [0.000, 2.000], mean observation: -0.080 [-0.352, 0.110], loss: 0.000452, mean_absolute_error: 0.442640, mean_q: 0.636445\n",
      " 24185/50000: episode: 388, duration: 0.236s, episode steps: 41, steps per second: 174, episode reward: 0.880, mean reward: 0.021 [-0.005, 1.000], mean action: 1.195 [0.000, 2.000], mean observation: -0.100 [-0.487, 0.160], loss: 0.000042, mean_absolute_error: 0.450896, mean_q: 0.640104\n",
      " 24191/50000: episode: 389, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: 0.994, mean reward: 0.166 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [-0.060, 0.118], loss: 0.000146, mean_absolute_error: 0.442262, mean_q: 0.628051\n",
      " 24226/50000: episode: 390, duration: 0.195s, episode steps: 35, steps per second: 179, episode reward: 0.908, mean reward: 0.026 [-0.004, 1.000], mean action: 0.771 [0.000, 2.000], mean observation: 0.088 [-0.120, 0.410], loss: 0.000040, mean_absolute_error: 0.449481, mean_q: 0.648447\n",
      " 24238/50000: episode: 391, duration: 0.067s, episode steps: 12, steps per second: 178, episode reward: 0.985, mean reward: 0.082 [-0.002, 1.000], mean action: 1.667 [0.000, 2.000], mean observation: -0.039 [-0.164, 0.090], loss: 0.000193, mean_absolute_error: 0.443579, mean_q: 0.639649\n",
      " 24260/50000: episode: 392, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 0.963, mean reward: 0.044 [-0.002, 1.000], mean action: 0.727 [0.000, 2.000], mean observation: 0.054 [-0.100, 0.240], loss: 0.000047, mean_absolute_error: 0.439639, mean_q: 0.637534\n",
      " 24311/50000: episode: 393, duration: 0.271s, episode steps: 51, steps per second: 188, episode reward: 0.793, mean reward: 0.016 [-0.007, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.144 [-0.711, 0.200], loss: 0.000306, mean_absolute_error: 0.452468, mean_q: 0.649406\n",
      " 24321/50000: episode: 394, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 0.300 [0.000, 2.000], mean observation: 0.045 [-0.070, 0.125], loss: 0.000078, mean_absolute_error: 0.451434, mean_q: 0.639201\n",
      " 24365/50000: episode: 395, duration: 0.270s, episode steps: 44, steps per second: 163, episode reward: 0.818, mean reward: 0.019 [-0.007, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.143 [-0.670, 0.200], loss: 0.000565, mean_absolute_error: 0.450675, mean_q: 0.643831\n",
      " 24366/50000: episode: 396, duration: 0.015s, episode steps: 1, steps per second: 65, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.008 [-0.010, -0.006], loss: 0.000062, mean_absolute_error: 0.406074, mean_q: 0.563352\n",
      " 24405/50000: episode: 397, duration: 0.239s, episode steps: 39, steps per second: 163, episode reward: 0.892, mean reward: 0.023 [-0.004, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.098 [-0.110, 0.419], loss: 0.000373, mean_absolute_error: 0.445156, mean_q: 0.638964\n",
      " 24458/50000: episode: 398, duration: 0.280s, episode steps: 53, steps per second: 189, episode reward: 0.704, mean reward: 0.013 [-0.010, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.198 [-0.280, 0.961], loss: 0.000254, mean_absolute_error: 0.445355, mean_q: 0.636509\n",
      " 24513/50000: episode: 399, duration: 0.297s, episode steps: 55, steps per second: 185, episode reward: 0.779, mean reward: 0.014 [-0.007, 1.000], mean action: 0.873 [0.000, 2.000], mean observation: 0.148 [-0.160, 0.685], loss: 0.000179, mean_absolute_error: 0.447874, mean_q: 0.640722\n",
      " 24550/50000: episode: 400, duration: 0.202s, episode steps: 37, steps per second: 183, episode reward: 0.885, mean reward: 0.024 [-0.005, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.105 [-0.180, 0.483], loss: 0.000061, mean_absolute_error: 0.448528, mean_q: 0.647969\n",
      " 24600/50000: episode: 401, duration: 0.271s, episode steps: 50, steps per second: 184, episode reward: 0.790, mean reward: 0.016 [-0.007, 1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.148 [-0.716, 0.190], loss: 0.000050, mean_absolute_error: 0.449192, mean_q: 0.645051\n",
      " 24628/50000: episode: 402, duration: 0.141s, episode steps: 28, steps per second: 198, episode reward: 0.941, mean reward: 0.034 [-0.003, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: -0.068 [-0.318, 0.120], loss: 0.000518, mean_absolute_error: 0.449466, mean_q: 0.641688\n",
      " 24679/50000: episode: 403, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 0.850, mean reward: 0.017 [-0.006, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.078 [-0.579, 0.170], loss: 0.000326, mean_absolute_error: 0.449239, mean_q: 0.642794\n",
      " 24722/50000: episode: 404, duration: 0.232s, episode steps: 43, steps per second: 185, episode reward: 0.858, mean reward: 0.020 [-0.006, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.111 [-0.565, 0.170], loss: 0.000445, mean_absolute_error: 0.448676, mean_q: 0.644649\n",
      " 24770/50000: episode: 405, duration: 0.256s, episode steps: 48, steps per second: 187, episode reward: 0.833, mean reward: 0.017 [-0.006, 1.000], mean action: 0.854 [0.000, 2.000], mean observation: 0.125 [-0.190, 0.569], loss: 0.000341, mean_absolute_error: 0.446795, mean_q: 0.645211\n",
      " 24830/50000: episode: 406, duration: 0.317s, episode steps: 60, steps per second: 189, episode reward: 0.749, mean reward: 0.012 [-0.007, 1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.156 [-0.743, 0.190], loss: 0.000082, mean_absolute_error: 0.445290, mean_q: 0.641729\n",
      " 24863/50000: episode: 407, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 0.916, mean reward: 0.028 [-0.004, 1.000], mean action: 1.242 [0.000, 2.000], mean observation: -0.084 [-0.396, 0.140], loss: 0.000419, mean_absolute_error: 0.440736, mean_q: 0.632012\n",
      " 24904/50000: episode: 408, duration: 0.226s, episode steps: 41, steps per second: 181, episode reward: 0.878, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.102 [-0.486, 0.150], loss: 0.000352, mean_absolute_error: 0.453472, mean_q: 0.655773\n",
      " 24905/50000: episode: 409, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.031 [-0.051, -0.010], loss: 0.000032, mean_absolute_error: 0.463118, mean_q: 0.702707\n",
      " 24948/50000: episode: 410, duration: 0.230s, episode steps: 43, steps per second: 187, episode reward: 0.864, mean reward: 0.020 [-0.005, 1.000], mean action: 1.163 [0.000, 2.000], mean observation: -0.110 [-0.528, 0.150], loss: 0.000109, mean_absolute_error: 0.448709, mean_q: 0.641165\n",
      " 24949/50000: episode: 411, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.035 [-0.081, 0.010], loss: 0.000017, mean_absolute_error: 0.450415, mean_q: 0.641677\n",
      " 25009/50000: episode: 412, duration: 0.315s, episode steps: 60, steps per second: 191, episode reward: 0.734, mean reward: 0.012 [-0.008, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.166 [-0.757, 0.190], loss: 0.000098, mean_absolute_error: 0.443698, mean_q: 0.643071\n",
      " 25010/50000: episode: 413, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.037 [-0.085, 0.010], loss: 0.000064, mean_absolute_error: 0.469697, mean_q: 0.704844\n",
      " 25067/50000: episode: 414, duration: 0.301s, episode steps: 57, steps per second: 189, episode reward: 0.755, mean reward: 0.013 [-0.008, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.156 [-0.768, 0.190], loss: 0.000086, mean_absolute_error: 0.440275, mean_q: 0.636278\n",
      " 25083/50000: episode: 415, duration: 0.088s, episode steps: 16, steps per second: 182, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 1.375 [0.000, 2.000], mean observation: -0.047 [-0.184, 0.080], loss: 0.000077, mean_absolute_error: 0.445225, mean_q: 0.648775\n",
      " 25112/50000: episode: 416, duration: 0.156s, episode steps: 29, steps per second: 186, episode reward: 0.934, mean reward: 0.032 [-0.003, 1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.070 [-0.347, 0.130], loss: 0.000056, mean_absolute_error: 0.446799, mean_q: 0.637688\n",
      " 25145/50000: episode: 417, duration: 0.176s, episode steps: 33, steps per second: 187, episode reward: 0.914, mean reward: 0.028 [-0.004, 1.000], mean action: 0.788 [0.000, 2.000], mean observation: 0.083 [-0.160, 0.414], loss: 0.000131, mean_absolute_error: 0.452564, mean_q: 0.651947\n",
      " 25157/50000: episode: 418, duration: 0.068s, episode steps: 12, steps per second: 176, episode reward: 0.984, mean reward: 0.082 [-0.002, 1.000], mean action: 0.333 [0.000, 2.000], mean observation: 0.041 [-0.090, 0.166], loss: 0.000120, mean_absolute_error: 0.454581, mean_q: 0.657167\n",
      " 25172/50000: episode: 419, duration: 0.083s, episode steps: 15, steps per second: 182, episode reward: 0.978, mean reward: 0.065 [-0.002, 1.000], mean action: 0.533 [0.000, 2.000], mean observation: 0.042 [-0.100, 0.191], loss: 0.000055, mean_absolute_error: 0.437161, mean_q: 0.632468\n",
      " 25218/50000: episode: 420, duration: 0.255s, episode steps: 46, steps per second: 181, episode reward: 0.847, mean reward: 0.018 [-0.005, 1.000], mean action: 1.174 [0.000, 2.000], mean observation: -0.120 [-0.538, 0.160], loss: 0.000040, mean_absolute_error: 0.450541, mean_q: 0.643072\n",
      " 25285/50000: episode: 421, duration: 0.368s, episode steps: 67, steps per second: 182, episode reward: 0.666, mean reward: 0.010 [-0.009, 1.000], mean action: 1.134 [0.000, 2.000], mean observation: -0.188 [-0.923, 0.200], loss: 0.000041, mean_absolute_error: 0.445515, mean_q: 0.644482\n",
      " 25356/50000: episode: 422, duration: 0.373s, episode steps: 71, steps per second: 191, episode reward: 0.617, mean reward: 0.009 [-0.009, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.208 [-0.938, 0.240], loss: 0.000233, mean_absolute_error: 0.453724, mean_q: 0.654738\n",
      " 25370/50000: episode: 423, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 0.980, mean reward: 0.070 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.044 [-0.182, 0.090], loss: 0.000029, mean_absolute_error: 0.437458, mean_q: 0.619627\n",
      " 25421/50000: episode: 424, duration: 0.272s, episode steps: 51, steps per second: 187, episode reward: 0.790, mean reward: 0.015 [-0.007, 1.000], mean action: 0.863 [0.000, 2.000], mean observation: 0.145 [-0.220, 0.726], loss: 0.000103, mean_absolute_error: 0.445719, mean_q: 0.639830\n",
      " 25473/50000: episode: 425, duration: 0.279s, episode steps: 52, steps per second: 187, episode reward: 0.771, mean reward: 0.015 [-0.007, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.153 [-0.749, 0.190], loss: 0.000582, mean_absolute_error: 0.449392, mean_q: 0.646686\n",
      " 25501/50000: episode: 426, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 0.941, mean reward: 0.034 [-0.003, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.068 [-0.312, 0.100], loss: 0.000176, mean_absolute_error: 0.450166, mean_q: 0.641870\n",
      " 25570/50000: episode: 427, duration: 0.366s, episode steps: 69, steps per second: 188, episode reward: 0.624, mean reward: 0.009 [-0.010, 1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.209 [-0.967, 0.190], loss: 0.000271, mean_absolute_error: 0.451142, mean_q: 0.649135\n",
      " 25595/50000: episode: 428, duration: 0.138s, episode steps: 25, steps per second: 182, episode reward: 0.946, mean reward: 0.038 [-0.003, 1.000], mean action: 0.640 [0.000, 2.000], mean observation: 0.067 [-0.130, 0.313], loss: 0.000052, mean_absolute_error: 0.453701, mean_q: 0.645859\n",
      " 25630/50000: episode: 429, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 0.906, mean reward: 0.026 [-0.004, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.088 [-0.150, 0.426], loss: 0.000043, mean_absolute_error: 0.447155, mean_q: 0.647184\n",
      " 25678/50000: episode: 430, duration: 0.256s, episode steps: 48, steps per second: 188, episode reward: 0.804, mean reward: 0.017 [-0.007, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.145 [-0.200, 0.680], loss: 0.000120, mean_absolute_error: 0.455676, mean_q: 0.664142\n",
      " 25719/50000: episode: 431, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 0.861, mean reward: 0.021 [-0.006, 1.000], mean action: 0.854 [0.000, 2.000], mean observation: 0.113 [-0.210, 0.572], loss: 0.000103, mean_absolute_error: 0.448541, mean_q: 0.646810\n",
      " 25768/50000: episode: 432, duration: 0.259s, episode steps: 49, steps per second: 189, episode reward: 0.824, mean reward: 0.017 [-0.007, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.112 [-0.210, 0.665], loss: 0.000039, mean_absolute_error: 0.446212, mean_q: 0.640843\n",
      " 25806/50000: episode: 433, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 0.898, mean reward: 0.024 [-0.004, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.092 [-0.427, 0.140], loss: 0.000124, mean_absolute_error: 0.452259, mean_q: 0.653756\n",
      " 25831/50000: episode: 434, duration: 0.137s, episode steps: 25, steps per second: 183, episode reward: 0.952, mean reward: 0.038 [-0.003, 1.000], mean action: 1.280 [0.000, 2.000], mean observation: -0.061 [-0.278, 0.110], loss: 0.000147, mean_absolute_error: 0.446017, mean_q: 0.639254\n",
      " 25872/50000: episode: 435, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 0.862, mean reward: 0.021 [-0.006, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.114 [-0.150, 0.550], loss: 0.000277, mean_absolute_error: 0.450712, mean_q: 0.642975\n",
      " 25924/50000: episode: 436, duration: 0.274s, episode steps: 52, steps per second: 190, episode reward: 0.728, mean reward: 0.014 [-0.009, 1.000], mean action: 0.865 [0.000, 2.000], mean observation: 0.188 [-0.280, 0.864], loss: 0.000297, mean_absolute_error: 0.447943, mean_q: 0.644820\n",
      " 25949/50000: episode: 437, duration: 0.135s, episode steps: 25, steps per second: 186, episode reward: 0.949, mean reward: 0.038 [-0.003, 1.000], mean action: 0.680 [0.000, 2.000], mean observation: 0.062 [-0.130, 0.300], loss: 0.000045, mean_absolute_error: 0.453748, mean_q: 0.656414\n",
      " 26002/50000: episode: 438, duration: 0.281s, episode steps: 53, steps per second: 189, episode reward: 0.736, mean reward: 0.014 [-0.008, 1.000], mean action: 0.887 [0.000, 2.000], mean observation: 0.180 [-0.260, 0.834], loss: 0.000295, mean_absolute_error: 0.450368, mean_q: 0.649179\n",
      " 26056/50000: episode: 439, duration: 0.295s, episode steps: 54, steps per second: 183, episode reward: 0.711, mean reward: 0.013 [-0.009, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.194 [-0.280, 0.900], loss: 0.000090, mean_absolute_error: 0.450632, mean_q: 0.651705\n",
      " 26057/50000: episode: 440, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.034 [-0.010, 0.078], loss: 0.000212, mean_absolute_error: 0.456556, mean_q: 0.607290\n",
      " 26104/50000: episode: 441, duration: 0.250s, episode steps: 47, steps per second: 188, episode reward: 0.814, mean reward: 0.017 [-0.007, 1.000], mean action: 0.851 [0.000, 2.000], mean observation: 0.138 [-0.220, 0.668], loss: 0.000091, mean_absolute_error: 0.453587, mean_q: 0.654398\n",
      " 26112/50000: episode: 442, duration: 0.047s, episode steps: 8, steps per second: 170, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.038 [-0.060, 0.130], loss: 0.001533, mean_absolute_error: 0.437045, mean_q: 0.640965\n",
      " 26161/50000: episode: 443, duration: 0.262s, episode steps: 49, steps per second: 187, episode reward: 0.807, mean reward: 0.016 [-0.007, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.140 [-0.663, 0.160], loss: 0.000293, mean_absolute_error: 0.453041, mean_q: 0.650138\n",
      " 26217/50000: episode: 444, duration: 0.303s, episode steps: 56, steps per second: 185, episode reward: 0.785, mean reward: 0.014 [-0.006, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.145 [-0.637, 0.150], loss: 0.000314, mean_absolute_error: 0.455166, mean_q: 0.655015\n",
      " 26280/50000: episode: 445, duration: 0.333s, episode steps: 63, steps per second: 189, episode reward: 0.715, mean reward: 0.011 [-0.008, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.173 [-0.774, 0.170], loss: 0.000229, mean_absolute_error: 0.453459, mean_q: 0.657384\n",
      " 26296/50000: episode: 446, duration: 0.082s, episode steps: 16, steps per second: 194, episode reward: 0.976, mean reward: 0.061 [-0.002, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.046 [-0.100, 0.198], loss: 0.000615, mean_absolute_error: 0.460903, mean_q: 0.666600\n",
      " 26341/50000: episode: 447, duration: 0.235s, episode steps: 45, steps per second: 192, episode reward: 0.825, mean reward: 0.018 [-0.007, 1.000], mean action: 0.822 [0.000, 2.000], mean observation: 0.133 [-0.220, 0.654], loss: 0.000064, mean_absolute_error: 0.450674, mean_q: 0.651926\n",
      " 26381/50000: episode: 448, duration: 0.217s, episode steps: 40, steps per second: 184, episode reward: 0.893, mean reward: 0.022 [-0.004, 1.000], mean action: 1.175 [0.000, 2.000], mean observation: -0.091 [-0.445, 0.150], loss: 0.000027, mean_absolute_error: 0.457895, mean_q: 0.653793\n",
      " 26446/50000: episode: 449, duration: 0.353s, episode steps: 65, steps per second: 184, episode reward: 0.739, mean reward: 0.011 [-0.008, 1.000], mean action: 1.123 [0.000, 2.000], mean observation: -0.151 [-0.758, 0.160], loss: 0.000084, mean_absolute_error: 0.454806, mean_q: 0.657131\n",
      " 26467/50000: episode: 450, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 0.962, mean reward: 0.046 [-0.003, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.056 [-0.251, 0.100], loss: 0.000052, mean_absolute_error: 0.460286, mean_q: 0.652515\n",
      " 26515/50000: episode: 451, duration: 0.284s, episode steps: 48, steps per second: 169, episode reward: 0.782, mean reward: 0.016 [-0.008, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.157 [-0.240, 0.775], loss: 0.000371, mean_absolute_error: 0.459521, mean_q: 0.669311\n",
      " 26579/50000: episode: 452, duration: 0.408s, episode steps: 64, steps per second: 157, episode reward: 0.653, mean reward: 0.010 [-0.009, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.205 [-0.946, 0.200], loss: 0.000383, mean_absolute_error: 0.457106, mean_q: 0.658916\n",
      " 26626/50000: episode: 453, duration: 0.276s, episode steps: 47, steps per second: 170, episode reward: 0.816, mean reward: 0.017 [-0.007, 1.000], mean action: 0.851 [0.000, 2.000], mean observation: 0.137 [-0.210, 0.658], loss: 0.000108, mean_absolute_error: 0.452596, mean_q: 0.652280\n",
      " 26627/50000: episode: 454, duration: 0.012s, episode steps: 1, steps per second: 83, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.034 [-0.057, -0.010], loss: 0.000047, mean_absolute_error: 0.418607, mean_q: 0.611834\n",
      " 26660/50000: episode: 455, duration: 0.185s, episode steps: 33, steps per second: 178, episode reward: 0.917, mean reward: 0.028 [-0.004, 1.000], mean action: 0.758 [0.000, 2.000], mean observation: 0.088 [-0.140, 0.357], loss: 0.000026, mean_absolute_error: 0.455423, mean_q: 0.657699\n",
      " 26661/50000: episode: 456, duration: 0.012s, episode steps: 1, steps per second: 80, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.006 [-0.010, -0.002], loss: 0.000019, mean_absolute_error: 0.439044, mean_q: 0.663955\n",
      " 26669/50000: episode: 457, duration: 0.050s, episode steps: 8, steps per second: 159, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.080, 0.130], loss: 0.000039, mean_absolute_error: 0.459088, mean_q: 0.666720\n",
      " 26712/50000: episode: 458, duration: 0.246s, episode steps: 43, steps per second: 174, episode reward: 0.841, mean reward: 0.020 [-0.006, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.124 [-0.200, 0.619], loss: 0.000315, mean_absolute_error: 0.454680, mean_q: 0.658756\n",
      " 26737/50000: episode: 459, duration: 0.138s, episode steps: 25, steps per second: 181, episode reward: 0.954, mean reward: 0.038 [-0.003, 1.000], mean action: 0.640 [0.000, 2.000], mean observation: 0.059 [-0.110, 0.258], loss: 0.000502, mean_absolute_error: 0.451308, mean_q: 0.654928\n",
      " 26738/50000: episode: 460, duration: 0.010s, episode steps: 1, steps per second: 103, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [-0.010, 0.033], loss: 0.000015, mean_absolute_error: 0.444152, mean_q: 0.641828\n",
      " 26772/50000: episode: 461, duration: 0.178s, episode steps: 34, steps per second: 191, episode reward: 0.912, mean reward: 0.027 [-0.004, 1.000], mean action: 1.147 [0.000, 2.000], mean observation: -0.084 [-0.410, 0.140], loss: 0.000059, mean_absolute_error: 0.456146, mean_q: 0.658554\n",
      " 26773/50000: episode: 462, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.018 [-0.010, 0.046], loss: 0.000039, mean_absolute_error: 0.436453, mean_q: 0.615627\n",
      " 26798/50000: episode: 463, duration: 0.134s, episode steps: 25, steps per second: 187, episode reward: 0.954, mean reward: 0.038 [-0.003, 1.000], mean action: 1.320 [0.000, 2.000], mean observation: -0.063 [-0.260, 0.090], loss: 0.000456, mean_absolute_error: 0.443844, mean_q: 0.633412\n",
      " 26848/50000: episode: 464, duration: 0.265s, episode steps: 50, steps per second: 189, episode reward: 0.803, mean reward: 0.016 [-0.007, 1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.140 [-0.681, 0.180], loss: 0.000053, mean_absolute_error: 0.458123, mean_q: 0.664031\n",
      " 26888/50000: episode: 465, duration: 0.220s, episode steps: 40, steps per second: 182, episode reward: 0.916, mean reward: 0.023 [-0.004, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.063 [-0.357, 0.130], loss: 0.000538, mean_absolute_error: 0.459456, mean_q: 0.666419\n",
      " 26943/50000: episode: 466, duration: 0.292s, episode steps: 55, steps per second: 188, episode reward: 0.763, mean reward: 0.014 [-0.008, 1.000], mean action: 1.109 [0.000, 2.000], mean observation: -0.155 [-0.770, 0.190], loss: 0.000101, mean_absolute_error: 0.447590, mean_q: 0.648925\n",
      " 27000/50000: episode: 467, duration: 0.310s, episode steps: 57, steps per second: 184, episode reward: 0.693, mean reward: 0.012 [-0.009, 1.000], mean action: 0.877 [0.000, 2.000], mean observation: 0.196 [-0.260, 0.947], loss: 0.000115, mean_absolute_error: 0.456058, mean_q: 0.667258\n",
      " 27042/50000: episode: 468, duration: 0.224s, episode steps: 42, steps per second: 188, episode reward: 0.848, mean reward: 0.020 [-0.006, 1.000], mean action: 0.810 [0.000, 2.000], mean observation: 0.122 [-0.180, 0.601], loss: 0.000036, mean_absolute_error: 0.451981, mean_q: 0.649555\n",
      " 27065/50000: episode: 469, duration: 0.125s, episode steps: 23, steps per second: 184, episode reward: 0.956, mean reward: 0.042 [-0.003, 1.000], mean action: 1.304 [0.000, 2.000], mean observation: -0.059 [-0.278, 0.130], loss: 0.000037, mean_absolute_error: 0.452825, mean_q: 0.662864\n",
      " 27066/50000: episode: 470, duration: 0.012s, episode steps: 1, steps per second: 85, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [-0.010, 0.098], loss: 0.000041, mean_absolute_error: 0.414781, mean_q: 0.631724\n",
      " 27109/50000: episode: 471, duration: 0.236s, episode steps: 43, steps per second: 182, episode reward: 0.849, mean reward: 0.020 [-0.006, 1.000], mean action: 0.814 [0.000, 2.000], mean observation: 0.123 [-0.190, 0.560], loss: 0.000147, mean_absolute_error: 0.456359, mean_q: 0.655189\n",
      " 27130/50000: episode: 472, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 0.969, mean reward: 0.046 [-0.002, 1.000], mean action: 1.238 [0.000, 2.000], mean observation: -0.049 [-0.211, 0.090], loss: 0.000071, mean_absolute_error: 0.458250, mean_q: 0.656730\n",
      " 27182/50000: episode: 473, duration: 0.287s, episode steps: 52, steps per second: 181, episode reward: 0.799, mean reward: 0.015 [-0.007, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.133 [-0.684, 0.170], loss: 0.000328, mean_absolute_error: 0.452009, mean_q: 0.657857\n",
      " 27235/50000: episode: 474, duration: 0.283s, episode steps: 53, steps per second: 188, episode reward: 0.800, mean reward: 0.015 [-0.007, 1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.136 [-0.666, 0.190], loss: 0.000048, mean_absolute_error: 0.450080, mean_q: 0.656678\n",
      " 27268/50000: episode: 475, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 0.924, mean reward: 0.028 [-0.004, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.077 [-0.358, 0.130], loss: 0.000071, mean_absolute_error: 0.461577, mean_q: 0.671388\n",
      " 27306/50000: episode: 476, duration: 0.208s, episode steps: 38, steps per second: 183, episode reward: 0.887, mean reward: 0.023 [-0.005, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.100 [-0.170, 0.478], loss: 0.000079, mean_absolute_error: 0.452743, mean_q: 0.656221\n",
      " 27378/50000: episode: 477, duration: 0.382s, episode steps: 72, steps per second: 189, episode reward: 0.607, mean reward: 0.008 [-0.010, 1.000], mean action: 1.083 [0.000, 2.000], mean observation: -0.212 [-0.980, 0.210], loss: 0.000040, mean_absolute_error: 0.456036, mean_q: 0.662448\n",
      " 27406/50000: episode: 478, duration: 0.151s, episode steps: 28, steps per second: 186, episode reward: 0.937, mean reward: 0.033 [-0.003, 1.000], mean action: 0.714 [0.000, 2.000], mean observation: 0.072 [-0.150, 0.333], loss: 0.000036, mean_absolute_error: 0.463772, mean_q: 0.672276\n",
      " 27456/50000: episode: 479, duration: 0.267s, episode steps: 50, steps per second: 187, episode reward: 0.831, mean reward: 0.017 [-0.006, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.111 [-0.620, 0.170], loss: 0.000563, mean_absolute_error: 0.461286, mean_q: 0.669163\n",
      " 27514/50000: episode: 480, duration: 0.306s, episode steps: 58, steps per second: 190, episode reward: 0.752, mean reward: 0.013 [-0.008, 1.000], mean action: 1.138 [0.000, 2.000], mean observation: -0.156 [-0.777, 0.190], loss: 0.000134, mean_absolute_error: 0.456063, mean_q: 0.663636\n",
      " 27571/50000: episode: 481, duration: 0.303s, episode steps: 57, steps per second: 188, episode reward: 0.678, mean reward: 0.012 [-0.010, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.201 [-0.300, 0.977], loss: 0.000078, mean_absolute_error: 0.449863, mean_q: 0.647939\n",
      " 27590/50000: episode: 482, duration: 0.105s, episode steps: 19, steps per second: 182, episode reward: 0.970, mean reward: 0.051 [-0.002, 1.000], mean action: 1.368 [0.000, 2.000], mean observation: -0.049 [-0.218, 0.100], loss: 0.000058, mean_absolute_error: 0.457916, mean_q: 0.665000\n",
      " 27635/50000: episode: 483, duration: 0.236s, episode steps: 45, steps per second: 191, episode reward: 0.826, mean reward: 0.018 [-0.007, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.131 [-0.230, 0.662], loss: 0.000101, mean_absolute_error: 0.455635, mean_q: 0.662101\n",
      " 27690/50000: episode: 484, duration: 0.289s, episode steps: 55, steps per second: 191, episode reward: 0.771, mean reward: 0.014 [-0.007, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.151 [-0.733, 0.190], loss: 0.000060, mean_absolute_error: 0.459141, mean_q: 0.668885\n",
      " 27753/50000: episode: 485, duration: 0.332s, episode steps: 63, steps per second: 190, episode reward: 0.679, mean reward: 0.011 [-0.009, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.186 [-0.916, 0.190], loss: 0.000354, mean_absolute_error: 0.457108, mean_q: 0.660178\n",
      " 27794/50000: episode: 486, duration: 0.219s, episode steps: 41, steps per second: 187, episode reward: 0.864, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.114 [-0.531, 0.160], loss: 0.000041, mean_absolute_error: 0.453090, mean_q: 0.656096\n",
      " 27829/50000: episode: 487, duration: 0.186s, episode steps: 35, steps per second: 188, episode reward: 0.901, mean reward: 0.026 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.091 [-0.170, 0.456], loss: 0.000034, mean_absolute_error: 0.461472, mean_q: 0.674931\n",
      " 27830/50000: episode: 488, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.019 [-0.010, 0.048], loss: 0.000034, mean_absolute_error: 0.417252, mean_q: 0.636997\n",
      " 27869/50000: episode: 489, duration: 0.226s, episode steps: 39, steps per second: 173, episode reward: 0.881, mean reward: 0.023 [-0.005, 1.000], mean action: 0.821 [0.000, 2.000], mean observation: 0.102 [-0.180, 0.506], loss: 0.000048, mean_absolute_error: 0.463111, mean_q: 0.672167\n",
      " 27939/50000: episode: 490, duration: 0.354s, episode steps: 70, steps per second: 198, episode reward: 0.635, mean reward: 0.009 [-0.010, 1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.200 [-0.950, 0.190], loss: 0.000200, mean_absolute_error: 0.458845, mean_q: 0.666264\n",
      " 27995/50000: episode: 491, duration: 0.297s, episode steps: 56, steps per second: 188, episode reward: 0.735, mean reward: 0.013 [-0.008, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.175 [-0.799, 0.200], loss: 0.000246, mean_absolute_error: 0.465181, mean_q: 0.671976\n",
      " 28041/50000: episode: 492, duration: 0.246s, episode steps: 46, steps per second: 187, episode reward: 0.823, mean reward: 0.018 [-0.007, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.131 [-0.210, 0.659], loss: 0.000092, mean_absolute_error: 0.458828, mean_q: 0.670757\n",
      " 28042/50000: episode: 493, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.013 [0.010, 0.016], loss: 0.000033, mean_absolute_error: 0.426554, mean_q: 0.568609\n",
      " 28076/50000: episode: 494, duration: 0.184s, episode steps: 34, steps per second: 185, episode reward: 0.915, mean reward: 0.027 [-0.004, 1.000], mean action: 1.235 [0.000, 2.000], mean observation: -0.084 [-0.381, 0.120], loss: 0.000039, mean_absolute_error: 0.463826, mean_q: 0.682615\n",
      " 28107/50000: episode: 495, duration: 0.167s, episode steps: 31, steps per second: 185, episode reward: 0.922, mean reward: 0.030 [-0.004, 1.000], mean action: 0.774 [0.000, 2.000], mean observation: 0.082 [-0.150, 0.378], loss: 0.000049, mean_absolute_error: 0.462130, mean_q: 0.667129\n",
      " 28170/50000: episode: 496, duration: 0.332s, episode steps: 63, steps per second: 190, episode reward: 0.721, mean reward: 0.011 [-0.008, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.161 [-0.807, 0.180], loss: 0.000093, mean_absolute_error: 0.460809, mean_q: 0.670906\n",
      " 28171/50000: episode: 497, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.000 [-0.011, 0.010], loss: 0.000074, mean_absolute_error: 0.421962, mean_q: 0.599395\n",
      " 28239/50000: episode: 498, duration: 0.362s, episode steps: 68, steps per second: 188, episode reward: 0.637, mean reward: 0.009 [-0.010, 1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.202 [-0.977, 0.210], loss: 0.000043, mean_absolute_error: 0.459201, mean_q: 0.669682\n",
      " 28292/50000: episode: 499, duration: 0.280s, episode steps: 53, steps per second: 190, episode reward: 0.750, mean reward: 0.014 [-0.008, 1.000], mean action: 0.868 [0.000, 2.000], mean observation: 0.167 [-0.250, 0.833], loss: 0.000248, mean_absolute_error: 0.453397, mean_q: 0.654503\n",
      " 28348/50000: episode: 500, duration: 0.298s, episode steps: 56, steps per second: 188, episode reward: 0.730, mean reward: 0.013 [-0.008, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.175 [-0.838, 0.200], loss: 0.000163, mean_absolute_error: 0.463407, mean_q: 0.671577\n",
      " 28349/50000: episode: 501, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.001 [-0.010, 0.008], loss: 0.000496, mean_absolute_error: 0.430983, mean_q: 0.614087\n",
      " 28389/50000: episode: 502, duration: 0.217s, episode steps: 40, steps per second: 184, episode reward: 0.875, mean reward: 0.022 [-0.005, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.106 [-0.509, 0.140], loss: 0.000037, mean_absolute_error: 0.449597, mean_q: 0.642038\n",
      " 28410/50000: episode: 503, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 0.962, mean reward: 0.046 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.055 [-0.120, 0.258], loss: 0.000043, mean_absolute_error: 0.462412, mean_q: 0.675303\n",
      " 28433/50000: episode: 504, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 0.956, mean reward: 0.042 [-0.003, 1.000], mean action: 0.609 [0.000, 2.000], mean observation: 0.061 [-0.110, 0.265], loss: 0.000044, mean_absolute_error: 0.464962, mean_q: 0.667018\n",
      " 28491/50000: episode: 505, duration: 0.305s, episode steps: 58, steps per second: 190, episode reward: 0.695, mean reward: 0.012 [-0.009, 1.000], mean action: 0.879 [0.000, 2.000], mean observation: 0.194 [-0.240, 0.906], loss: 0.000048, mean_absolute_error: 0.460878, mean_q: 0.667778\n",
      " 28528/50000: episode: 506, duration: 0.198s, episode steps: 37, steps per second: 187, episode reward: 0.901, mean reward: 0.024 [-0.004, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.087 [-0.435, 0.140], loss: 0.000043, mean_absolute_error: 0.461557, mean_q: 0.668291\n",
      " 28529/50000: episode: 507, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.000 [-0.010, 0.009], loss: 0.000013, mean_absolute_error: 0.466815, mean_q: 0.657267\n",
      " 28561/50000: episode: 508, duration: 0.164s, episode steps: 32, steps per second: 195, episode reward: 0.919, mean reward: 0.029 [-0.004, 1.000], mean action: 0.781 [0.000, 2.000], mean observation: 0.081 [-0.160, 0.397], loss: 0.000047, mean_absolute_error: 0.463961, mean_q: 0.679831\n",
      " 28613/50000: episode: 509, duration: 0.272s, episode steps: 52, steps per second: 191, episode reward: 0.757, mean reward: 0.015 [-0.008, 1.000], mean action: 0.865 [0.000, 2.000], mean observation: 0.166 [-0.220, 0.814], loss: 0.000065, mean_absolute_error: 0.462230, mean_q: 0.671351\n",
      " 28647/50000: episode: 510, duration: 0.183s, episode steps: 34, steps per second: 186, episode reward: 0.909, mean reward: 0.027 [-0.004, 1.000], mean action: 1.235 [0.000, 2.000], mean observation: -0.088 [-0.422, 0.140], loss: 0.000132, mean_absolute_error: 0.459684, mean_q: 0.665960\n",
      " 28692/50000: episode: 511, duration: 0.244s, episode steps: 45, steps per second: 184, episode reward: 0.837, mean reward: 0.019 [-0.006, 1.000], mean action: 1.178 [0.000, 2.000], mean observation: -0.127 [-0.602, 0.160], loss: 0.000047, mean_absolute_error: 0.460226, mean_q: 0.662528\n",
      " 28722/50000: episode: 512, duration: 0.163s, episode steps: 30, steps per second: 184, episode reward: 0.924, mean reward: 0.031 [-0.004, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.081 [-0.374, 0.140], loss: 0.000490, mean_absolute_error: 0.464913, mean_q: 0.671723\n",
      " 28762/50000: episode: 513, duration: 0.218s, episode steps: 40, steps per second: 184, episode reward: 0.873, mean reward: 0.022 [-0.005, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.107 [-0.170, 0.515], loss: 0.000217, mean_absolute_error: 0.460762, mean_q: 0.664097\n",
      " 28805/50000: episode: 514, duration: 0.232s, episode steps: 43, steps per second: 185, episode reward: 0.862, mean reward: 0.020 [-0.005, 1.000], mean action: 1.186 [0.000, 2.000], mean observation: -0.112 [-0.523, 0.160], loss: 0.000447, mean_absolute_error: 0.462117, mean_q: 0.660742\n",
      " 28857/50000: episode: 515, duration: 0.277s, episode steps: 52, steps per second: 188, episode reward: 0.775, mean reward: 0.015 [-0.007, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.159 [-0.220, 0.706], loss: 0.000051, mean_absolute_error: 0.462081, mean_q: 0.668866\n",
      " 28923/50000: episode: 516, duration: 0.353s, episode steps: 66, steps per second: 187, episode reward: 0.658, mean reward: 0.010 [-0.009, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.192 [-0.942, 0.230], loss: 0.000203, mean_absolute_error: 0.465922, mean_q: 0.678541\n",
      " 28958/50000: episode: 517, duration: 0.190s, episode steps: 35, steps per second: 184, episode reward: 0.900, mean reward: 0.026 [-0.005, 1.000], mean action: 0.771 [0.000, 2.000], mean observation: 0.092 [-0.170, 0.460], loss: 0.000044, mean_absolute_error: 0.455752, mean_q: 0.662413\n",
      " 28979/50000: episode: 518, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 0.962, mean reward: 0.046 [-0.002, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.056 [-0.248, 0.110], loss: 0.000119, mean_absolute_error: 0.462065, mean_q: 0.674486\n",
      " 29041/50000: episode: 519, duration: 0.327s, episode steps: 62, steps per second: 190, episode reward: 0.684, mean reward: 0.011 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.186 [-0.924, 0.210], loss: 0.000229, mean_absolute_error: 0.458991, mean_q: 0.668109\n",
      " 29101/50000: episode: 520, duration: 0.317s, episode steps: 60, steps per second: 190, episode reward: 0.694, mean reward: 0.012 [-0.009, 1.000], mean action: 0.900 [0.000, 2.000], mean observation: 0.187 [-0.250, 0.916], loss: 0.000096, mean_absolute_error: 0.462400, mean_q: 0.672839\n",
      " 29149/50000: episode: 521, duration: 0.259s, episode steps: 48, steps per second: 185, episode reward: 0.813, mean reward: 0.017 [-0.007, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.137 [-0.190, 0.664], loss: 0.000154, mean_absolute_error: 0.463318, mean_q: 0.673199\n",
      " 29166/50000: episode: 522, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 0.979, mean reward: 0.058 [-0.002, 1.000], mean action: 1.529 [0.000, 2.000], mean observation: -0.027 [-0.181, 0.120], loss: 0.000647, mean_absolute_error: 0.456864, mean_q: 0.653854\n",
      " 29211/50000: episode: 523, duration: 0.244s, episode steps: 45, steps per second: 185, episode reward: 0.833, mean reward: 0.019 [-0.006, 1.000], mean action: 0.844 [0.000, 2.000], mean observation: 0.129 [-0.180, 0.611], loss: 0.000054, mean_absolute_error: 0.455692, mean_q: 0.666022\n",
      " 29239/50000: episode: 524, duration: 0.154s, episode steps: 28, steps per second: 182, episode reward: 0.940, mean reward: 0.034 [-0.003, 1.000], mean action: 0.714 [0.000, 2.000], mean observation: 0.072 [-0.120, 0.306], loss: 0.000094, mean_absolute_error: 0.464400, mean_q: 0.675138\n",
      " 29270/50000: episode: 525, duration: 0.166s, episode steps: 31, steps per second: 187, episode reward: 0.925, mean reward: 0.030 [-0.003, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.079 [-0.346, 0.160], loss: 0.000328, mean_absolute_error: 0.458842, mean_q: 0.667016\n",
      " 29294/50000: episode: 526, duration: 0.132s, episode steps: 24, steps per second: 182, episode reward: 0.953, mean reward: 0.040 [-0.003, 1.000], mean action: 0.708 [0.000, 2.000], mean observation: 0.060 [-0.130, 0.288], loss: 0.000246, mean_absolute_error: 0.452765, mean_q: 0.654860\n",
      " 29300/50000: episode: 527, duration: 0.036s, episode steps: 6, steps per second: 166, episode reward: 0.995, mean reward: 0.166 [-0.001, 1.000], mean action: 1.667 [1.000, 2.000], mean observation: -0.040 [-0.113, 0.040], loss: 0.000041, mean_absolute_error: 0.467617, mean_q: 0.683810\n",
      " 29326/50000: episode: 528, duration: 0.141s, episode steps: 26, steps per second: 184, episode reward: 0.945, mean reward: 0.036 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.062 [-0.312, 0.160], loss: 0.000478, mean_absolute_error: 0.460303, mean_q: 0.666156\n",
      " 29374/50000: episode: 529, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 0.814, mean reward: 0.017 [-0.006, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.139 [-0.170, 0.633], loss: 0.000354, mean_absolute_error: 0.455502, mean_q: 0.661607\n",
      " 29422/50000: episode: 530, duration: 0.255s, episode steps: 48, steps per second: 188, episode reward: 0.832, mean reward: 0.017 [-0.006, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.115 [-0.627, 0.180], loss: 0.000058, mean_absolute_error: 0.463340, mean_q: 0.677299\n",
      " 29462/50000: episode: 531, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 0.875, mean reward: 0.022 [-0.005, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.104 [-0.501, 0.150], loss: 0.000318, mean_absolute_error: 0.453203, mean_q: 0.657109\n",
      " 29502/50000: episode: 532, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 0.876, mean reward: 0.022 [-0.005, 1.000], mean action: 0.825 [0.000, 2.000], mean observation: 0.105 [-0.170, 0.507], loss: 0.000319, mean_absolute_error: 0.460214, mean_q: 0.671278\n",
      " 29562/50000: episode: 533, duration: 0.323s, episode steps: 60, steps per second: 186, episode reward: 0.688, mean reward: 0.011 [-0.009, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.194 [-0.210, 0.906], loss: 0.000073, mean_absolute_error: 0.461153, mean_q: 0.670327\n",
      " 29585/50000: episode: 534, duration: 0.123s, episode steps: 23, steps per second: 187, episode reward: 0.955, mean reward: 0.042 [-0.003, 1.000], mean action: 1.391 [0.000, 2.000], mean observation: -0.061 [-0.268, 0.120], loss: 0.000044, mean_absolute_error: 0.465605, mean_q: 0.684372\n",
      " 29631/50000: episode: 535, duration: 0.240s, episode steps: 46, steps per second: 191, episode reward: 0.839, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.117 [-0.591, 0.190], loss: 0.000040, mean_absolute_error: 0.462127, mean_q: 0.668231\n",
      " 29664/50000: episode: 536, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 0.911, mean reward: 0.028 [-0.004, 1.000], mean action: 0.758 [0.000, 2.000], mean observation: 0.086 [-0.160, 0.422], loss: 0.000033, mean_absolute_error: 0.456003, mean_q: 0.662242\n",
      " 29665/50000: episode: 537, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.005 [-0.020, 0.010], loss: 0.000069, mean_absolute_error: 0.485253, mean_q: 0.687381\n",
      " 29703/50000: episode: 538, duration: 0.204s, episode steps: 38, steps per second: 186, episode reward: 0.884, mean reward: 0.023 [-0.005, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.104 [-0.476, 0.150], loss: 0.000098, mean_absolute_error: 0.460022, mean_q: 0.667209\n",
      " 29752/50000: episode: 539, duration: 0.293s, episode steps: 49, steps per second: 167, episode reward: 0.876, mean reward: 0.018 [-0.005, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.071 [-0.509, 0.150], loss: 0.000126, mean_absolute_error: 0.465741, mean_q: 0.685559\n",
      " 29797/50000: episode: 540, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 0.841, mean reward: 0.019 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.119 [-0.605, 0.180], loss: 0.000081, mean_absolute_error: 0.454073, mean_q: 0.661646\n",
      " 29841/50000: episode: 541, duration: 0.245s, episode steps: 44, steps per second: 179, episode reward: 0.853, mean reward: 0.019 [-0.006, 1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.112 [-0.564, 0.160], loss: 0.000068, mean_absolute_error: 0.461096, mean_q: 0.671406\n",
      " 29842/50000: episode: 542, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.014 [-0.038, 0.010], loss: 0.000042, mean_absolute_error: 0.465920, mean_q: 0.685091\n",
      " 29864/50000: episode: 543, duration: 0.122s, episode steps: 22, steps per second: 181, episode reward: 0.961, mean reward: 0.044 [-0.003, 1.000], mean action: 0.682 [0.000, 2.000], mean observation: 0.056 [-0.100, 0.250], loss: 0.000049, mean_absolute_error: 0.458607, mean_q: 0.672715\n",
      " 29889/50000: episode: 544, duration: 0.140s, episode steps: 25, steps per second: 179, episode reward: 0.953, mean reward: 0.038 [-0.003, 1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.050 [-0.289, 0.140], loss: 0.000217, mean_absolute_error: 0.473124, mean_q: 0.688048\n",
      " 29929/50000: episode: 545, duration: 0.213s, episode steps: 40, steps per second: 188, episode reward: 0.859, mean reward: 0.021 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.116 [-0.578, 0.190], loss: 0.000058, mean_absolute_error: 0.455607, mean_q: 0.665664\n",
      " 29955/50000: episode: 546, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 0.944, mean reward: 0.036 [-0.003, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.067 [-0.140, 0.321], loss: 0.000397, mean_absolute_error: 0.473292, mean_q: 0.694944\n",
      " 30023/50000: episode: 547, duration: 0.360s, episode steps: 68, steps per second: 189, episode reward: 0.704, mean reward: 0.010 [-0.009, 1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.150 [-0.885, 0.220], loss: 0.000128, mean_absolute_error: 0.461979, mean_q: 0.673599\n",
      " 30088/50000: episode: 548, duration: 0.349s, episode steps: 65, steps per second: 186, episode reward: 0.647, mean reward: 0.010 [-0.010, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.206 [-0.190, 0.954], loss: 0.000197, mean_absolute_error: 0.455898, mean_q: 0.663550\n",
      " 30151/50000: episode: 549, duration: 0.337s, episode steps: 63, steps per second: 187, episode reward: 0.654, mean reward: 0.010 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.208 [-0.240, 0.948], loss: 0.000192, mean_absolute_error: 0.456570, mean_q: 0.667242\n",
      " 30206/50000: episode: 550, duration: 0.291s, episode steps: 55, steps per second: 189, episode reward: 0.762, mean reward: 0.014 [-0.008, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.157 [-0.170, 0.759], loss: 0.000060, mean_absolute_error: 0.464136, mean_q: 0.678872\n",
      " 30260/50000: episode: 551, duration: 0.303s, episode steps: 54, steps per second: 178, episode reward: 0.770, mean reward: 0.014 [-0.007, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.154 [-0.170, 0.747], loss: 0.000052, mean_absolute_error: 0.460298, mean_q: 0.663909\n",
      " 30290/50000: episode: 552, duration: 0.160s, episode steps: 30, steps per second: 187, episode reward: 0.928, mean reward: 0.031 [-0.004, 1.000], mean action: 0.733 [0.000, 2.000], mean observation: 0.077 [-0.140, 0.370], loss: 0.000107, mean_absolute_error: 0.450027, mean_q: 0.657791\n",
      " 30307/50000: episode: 553, duration: 0.094s, episode steps: 17, steps per second: 181, episode reward: 0.972, mean reward: 0.057 [-0.002, 1.000], mean action: 1.529 [0.000, 2.000], mean observation: -0.048 [-0.222, 0.120], loss: 0.000174, mean_absolute_error: 0.465913, mean_q: 0.675114\n",
      " 30352/50000: episode: 554, duration: 0.237s, episode steps: 45, steps per second: 190, episode reward: 0.841, mean reward: 0.019 [-0.006, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.119 [-0.594, 0.180], loss: 0.000073, mean_absolute_error: 0.461700, mean_q: 0.673699\n",
      " 30411/50000: episode: 555, duration: 0.314s, episode steps: 59, steps per second: 188, episode reward: 0.710, mean reward: 0.012 [-0.009, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.179 [-0.892, 0.220], loss: 0.000079, mean_absolute_error: 0.465367, mean_q: 0.672771\n",
      " 30439/50000: episode: 556, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 0.971, mean reward: 0.035 [-0.002, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: 0.007 [-0.203, 0.150], loss: 0.000424, mean_absolute_error: 0.459202, mean_q: 0.673973\n",
      " 30463/50000: episode: 557, duration: 0.128s, episode steps: 24, steps per second: 187, episode reward: 0.956, mean reward: 0.040 [-0.003, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.059 [-0.110, 0.259], loss: 0.000046, mean_absolute_error: 0.453298, mean_q: 0.660223\n",
      " 30518/50000: episode: 558, duration: 0.292s, episode steps: 55, steps per second: 188, episode reward: 0.748, mean reward: 0.014 [-0.008, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.162 [-0.820, 0.230], loss: 0.000042, mean_absolute_error: 0.461154, mean_q: 0.673761\n",
      " 30581/50000: episode: 559, duration: 0.335s, episode steps: 63, steps per second: 188, episode reward: 0.707, mean reward: 0.011 [-0.009, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.156 [-0.916, 0.230], loss: 0.000227, mean_absolute_error: 0.470318, mean_q: 0.687331\n",
      " 30602/50000: episode: 560, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 0.962, mean reward: 0.046 [-0.003, 1.000], mean action: 0.619 [0.000, 2.000], mean observation: 0.056 [-0.110, 0.257], loss: 0.000064, mean_absolute_error: 0.465927, mean_q: 0.678733\n",
      " 30630/50000: episode: 561, duration: 0.150s, episode steps: 28, steps per second: 187, episode reward: 0.952, mean reward: 0.034 [-0.003, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.045 [-0.269, 0.110], loss: 0.000049, mean_absolute_error: 0.457663, mean_q: 0.666300\n",
      " 30684/50000: episode: 562, duration: 0.289s, episode steps: 54, steps per second: 187, episode reward: 0.743, mean reward: 0.014 [-0.008, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.172 [-0.220, 0.820], loss: 0.000077, mean_absolute_error: 0.461803, mean_q: 0.673768\n",
      " 30695/50000: episode: 563, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 0.987, mean reward: 0.090 [-0.001, 1.000], mean action: 1.818 [0.000, 2.000], mean observation: -0.042 [-0.144, 0.090], loss: 0.000052, mean_absolute_error: 0.467154, mean_q: 0.688340\n",
      " 30737/50000: episode: 564, duration: 0.223s, episode steps: 42, steps per second: 188, episode reward: 0.843, mean reward: 0.020 [-0.006, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.118 [-0.220, 0.634], loss: 0.000081, mean_absolute_error: 0.457875, mean_q: 0.669621\n",
      " 30778/50000: episode: 565, duration: 0.224s, episode steps: 41, steps per second: 183, episode reward: 0.872, mean reward: 0.021 [-0.005, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.105 [-0.504, 0.160], loss: 0.000422, mean_absolute_error: 0.449551, mean_q: 0.651904\n",
      " 30825/50000: episode: 566, duration: 0.272s, episode steps: 47, steps per second: 173, episode reward: 0.825, mean reward: 0.018 [-0.006, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.131 [-0.180, 0.622], loss: 0.000284, mean_absolute_error: 0.464752, mean_q: 0.674176\n",
      " 30867/50000: episode: 567, duration: 0.223s, episode steps: 42, steps per second: 188, episode reward: 0.878, mean reward: 0.021 [-0.005, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.086 [-0.500, 0.190], loss: 0.000398, mean_absolute_error: 0.455033, mean_q: 0.656907\n",
      " 30923/50000: episode: 568, duration: 0.295s, episode steps: 56, steps per second: 190, episode reward: 0.748, mean reward: 0.013 [-0.009, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.144 [-0.854, 0.290], loss: 0.000235, mean_absolute_error: 0.462868, mean_q: 0.677333\n",
      " 30972/50000: episode: 569, duration: 0.261s, episode steps: 49, steps per second: 188, episode reward: 0.801, mean reward: 0.016 [-0.007, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.143 [-0.695, 0.190], loss: 0.000112, mean_absolute_error: 0.462488, mean_q: 0.675367\n",
      " 30979/50000: episode: 570, duration: 0.044s, episode steps: 7, steps per second: 161, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.122, 0.070], loss: 0.000064, mean_absolute_error: 0.467878, mean_q: 0.679741\n",
      " 31027/50000: episode: 571, duration: 0.256s, episode steps: 48, steps per second: 188, episode reward: 0.796, mean reward: 0.017 [-0.007, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.151 [-0.200, 0.704], loss: 0.000361, mean_absolute_error: 0.461245, mean_q: 0.674130\n",
      " 31078/50000: episode: 572, duration: 0.274s, episode steps: 51, steps per second: 186, episode reward: 0.772, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.157 [-0.220, 0.780], loss: 0.000059, mean_absolute_error: 0.454845, mean_q: 0.662396\n",
      " 31115/50000: episode: 573, duration: 0.195s, episode steps: 37, steps per second: 189, episode reward: 0.887, mean reward: 0.024 [-0.005, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.100 [-0.180, 0.494], loss: 0.000055, mean_absolute_error: 0.461186, mean_q: 0.675583\n",
      " 31151/50000: episode: 574, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 0.894, mean reward: 0.025 [-0.005, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.096 [-0.170, 0.470], loss: 0.000060, mean_absolute_error: 0.459667, mean_q: 0.669223\n",
      " 31203/50000: episode: 575, duration: 0.279s, episode steps: 52, steps per second: 186, episode reward: 0.755, mean reward: 0.015 [-0.008, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.167 [-0.220, 0.814], loss: 0.000212, mean_absolute_error: 0.455383, mean_q: 0.663559\n",
      " 31204/50000: episode: 576, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.015 [-0.040, 0.010], loss: 0.000260, mean_absolute_error: 0.475582, mean_q: 0.697429\n",
      " 31236/50000: episode: 577, duration: 0.169s, episode steps: 32, steps per second: 190, episode reward: 0.920, mean reward: 0.029 [-0.004, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.079 [-0.150, 0.393], loss: 0.000072, mean_absolute_error: 0.458386, mean_q: 0.665912\n",
      " 31291/50000: episode: 578, duration: 0.289s, episode steps: 55, steps per second: 190, episode reward: 0.689, mean reward: 0.013 [-0.010, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.201 [-0.974, 0.290], loss: 0.000061, mean_absolute_error: 0.458138, mean_q: 0.663285\n",
      " 31355/50000: episode: 579, duration: 0.346s, episode steps: 64, steps per second: 185, episode reward: 0.652, mean reward: 0.010 [-0.009, 1.000], mean action: 0.891 [0.000, 2.000], mean observation: 0.208 [-0.220, 0.926], loss: 0.000274, mean_absolute_error: 0.459005, mean_q: 0.667755\n",
      " 31401/50000: episode: 580, duration: 0.245s, episode steps: 46, steps per second: 188, episode reward: 0.821, mean reward: 0.018 [-0.007, 1.000], mean action: 0.848 [0.000, 2.000], mean observation: 0.134 [-0.200, 0.656], loss: 0.000089, mean_absolute_error: 0.457475, mean_q: 0.664628\n",
      " 31440/50000: episode: 581, duration: 0.206s, episode steps: 39, steps per second: 189, episode reward: 0.871, mean reward: 0.022 [-0.005, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.110 [-0.190, 0.542], loss: 0.000058, mean_absolute_error: 0.460635, mean_q: 0.672907\n",
      " 31494/50000: episode: 582, duration: 0.287s, episode steps: 54, steps per second: 188, episode reward: 0.797, mean reward: 0.015 [-0.007, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.130 [-0.683, 0.180], loss: 0.000050, mean_absolute_error: 0.458986, mean_q: 0.670132\n",
      " 31521/50000: episode: 583, duration: 0.148s, episode steps: 27, steps per second: 183, episode reward: 0.942, mean reward: 0.035 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.067 [-0.110, 0.319], loss: 0.000044, mean_absolute_error: 0.451451, mean_q: 0.652504\n",
      " 31540/50000: episode: 584, duration: 0.103s, episode steps: 19, steps per second: 184, episode reward: 0.967, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 2.000], mean observation: 0.051 [-0.110, 0.238], loss: 0.000034, mean_absolute_error: 0.460602, mean_q: 0.663466\n",
      " 31601/50000: episode: 585, duration: 0.337s, episode steps: 61, steps per second: 181, episode reward: 0.695, mean reward: 0.011 [-0.009, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.183 [-0.912, 0.220], loss: 0.000282, mean_absolute_error: 0.454923, mean_q: 0.664572\n",
      " 31659/50000: episode: 586, duration: 0.314s, episode steps: 58, steps per second: 185, episode reward: 0.693, mean reward: 0.012 [-0.009, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.193 [-0.250, 0.934], loss: 0.000264, mean_absolute_error: 0.457758, mean_q: 0.667711\n",
      " 31711/50000: episode: 587, duration: 0.288s, episode steps: 52, steps per second: 181, episode reward: 0.773, mean reward: 0.015 [-0.008, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.155 [-0.220, 0.757], loss: 0.000284, mean_absolute_error: 0.457651, mean_q: 0.667052\n",
      " 31712/50000: episode: 588, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.029 [-0.068, 0.010], loss: 0.000096, mean_absolute_error: 0.522599, mean_q: 0.788198\n",
      " 31756/50000: episode: 589, duration: 0.247s, episode steps: 44, steps per second: 178, episode reward: 0.843, mean reward: 0.019 [-0.006, 1.000], mean action: 1.182 [0.000, 2.000], mean observation: -0.124 [-0.579, 0.170], loss: 0.000139, mean_absolute_error: 0.465406, mean_q: 0.685513\n",
      " 31757/50000: episode: 590, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.015 [-0.040, 0.010], loss: 0.000032, mean_absolute_error: 0.444192, mean_q: 0.673614\n",
      " 31758/50000: episode: 591, duration: 0.010s, episode steps: 1, steps per second: 98, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [-0.010, 0.019], loss: 0.000179, mean_absolute_error: 0.412021, mean_q: 0.601460\n",
      " 31804/50000: episode: 592, duration: 0.260s, episode steps: 46, steps per second: 177, episode reward: 0.843, mean reward: 0.018 [-0.006, 1.000], mean action: 1.174 [0.000, 2.000], mean observation: -0.118 [-0.593, 0.190], loss: 0.000046, mean_absolute_error: 0.462927, mean_q: 0.679877\n",
      " 31849/50000: episode: 593, duration: 0.225s, episode steps: 45, steps per second: 200, episode reward: 0.837, mean reward: 0.019 [-0.006, 1.000], mean action: 1.178 [0.000, 2.000], mean observation: -0.130 [-0.566, 0.180], loss: 0.000086, mean_absolute_error: 0.459976, mean_q: 0.669617\n",
      " 31893/50000: episode: 594, duration: 0.230s, episode steps: 44, steps per second: 192, episode reward: 0.854, mean reward: 0.019 [-0.006, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.103 [-0.210, 0.589], loss: 0.000304, mean_absolute_error: 0.458557, mean_q: 0.670498\n",
      " 31947/50000: episode: 595, duration: 0.286s, episode steps: 54, steps per second: 189, episode reward: 0.780, mean reward: 0.014 [-0.007, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.145 [-0.720, 0.200], loss: 0.000079, mean_absolute_error: 0.459762, mean_q: 0.676239\n",
      " 31989/50000: episode: 596, duration: 0.210s, episode steps: 42, steps per second: 200, episode reward: 0.855, mean reward: 0.020 [-0.006, 1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.106 [-0.597, 0.210], loss: 0.000325, mean_absolute_error: 0.463354, mean_q: 0.681649\n",
      " 32045/50000: episode: 597, duration: 0.262s, episode steps: 56, steps per second: 213, episode reward: 0.791, mean reward: 0.014 [-0.007, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.121 [-0.700, 0.180], loss: 0.000486, mean_absolute_error: 0.462381, mean_q: 0.673565\n",
      " 32061/50000: episode: 598, duration: 0.090s, episode steps: 16, steps per second: 178, episode reward: 0.978, mean reward: 0.061 [-0.002, 1.000], mean action: 0.438 [0.000, 2.000], mean observation: 0.043 [-0.100, 0.179], loss: 0.000067, mean_absolute_error: 0.465061, mean_q: 0.683263\n",
      " 32118/50000: episode: 599, duration: 0.381s, episode steps: 57, steps per second: 150, episode reward: 0.714, mean reward: 0.013 [-0.009, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.183 [-0.240, 0.873], loss: 0.000086, mean_absolute_error: 0.466343, mean_q: 0.682568\n",
      " 32132/50000: episode: 600, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 0.980, mean reward: 0.070 [-0.002, 1.000], mean action: 1.643 [0.000, 2.000], mean observation: -0.042 [-0.188, 0.110], loss: 0.000083, mean_absolute_error: 0.467450, mean_q: 0.685934\n",
      " 32133/50000: episode: 601, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.004 [-0.018, 0.010], loss: 0.000015, mean_absolute_error: 0.500048, mean_q: 0.738374\n",
      " 32178/50000: episode: 602, duration: 0.250s, episode steps: 45, steps per second: 180, episode reward: 0.827, mean reward: 0.018 [-0.006, 1.000], mean action: 0.822 [0.000, 2.000], mean observation: 0.134 [-0.200, 0.631], loss: 0.000331, mean_absolute_error: 0.456606, mean_q: 0.665836\n",
      " 32238/50000: episode: 603, duration: 0.431s, episode steps: 60, steps per second: 139, episode reward: 0.681, mean reward: 0.011 [-0.010, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.188 [-0.952, 0.220], loss: 0.000261, mean_absolute_error: 0.458663, mean_q: 0.674956\n",
      " 32258/50000: episode: 604, duration: 0.124s, episode steps: 20, steps per second: 161, episode reward: 0.967, mean reward: 0.048 [-0.002, 1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.051 [-0.230, 0.090], loss: 0.000074, mean_absolute_error: 0.468821, mean_q: 0.688224\n",
      " 32325/50000: episode: 605, duration: 0.446s, episode steps: 67, steps per second: 150, episode reward: 0.681, mean reward: 0.010 [-0.009, 1.000], mean action: 1.134 [0.000, 2.000], mean observation: -0.167 [-0.938, 0.240], loss: 0.000211, mean_absolute_error: 0.463612, mean_q: 0.679523\n",
      " 32350/50000: episode: 606, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 0.945, mean reward: 0.038 [-0.003, 1.000], mean action: 0.680 [0.000, 2.000], mean observation: 0.066 [-0.150, 0.325], loss: 0.000080, mean_absolute_error: 0.464718, mean_q: 0.688218\n",
      " 32406/50000: episode: 607, duration: 0.301s, episode steps: 56, steps per second: 186, episode reward: 0.716, mean reward: 0.013 [-0.009, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.186 [-0.230, 0.863], loss: 0.000074, mean_absolute_error: 0.460629, mean_q: 0.677250\n",
      " 32435/50000: episode: 608, duration: 0.226s, episode steps: 29, steps per second: 129, episode reward: 0.930, mean reward: 0.032 [-0.004, 1.000], mean action: 0.724 [0.000, 2.000], mean observation: 0.075 [-0.150, 0.369], loss: 0.000051, mean_absolute_error: 0.458440, mean_q: 0.672183\n",
      " 32461/50000: episode: 609, duration: 0.144s, episode steps: 26, steps per second: 180, episode reward: 0.942, mean reward: 0.036 [-0.003, 1.000], mean action: 1.346 [0.000, 2.000], mean observation: -0.068 [-0.328, 0.150], loss: 0.000064, mean_absolute_error: 0.455564, mean_q: 0.665901\n",
      " 32477/50000: episode: 610, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 0.976, mean reward: 0.061 [-0.002, 1.000], mean action: 0.562 [0.000, 2.000], mean observation: 0.045 [-0.110, 0.203], loss: 0.000069, mean_absolute_error: 0.456448, mean_q: 0.670171\n",
      " 32488/50000: episode: 611, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 0.987, mean reward: 0.090 [-0.002, 1.000], mean action: 0.182 [0.000, 2.000], mean observation: 0.037 [-0.090, 0.155], loss: 0.000073, mean_absolute_error: 0.471657, mean_q: 0.671196\n",
      " 32530/50000: episode: 612, duration: 0.224s, episode steps: 42, steps per second: 188, episode reward: 0.839, mean reward: 0.020 [-0.006, 1.000], mean action: 0.786 [0.000, 2.000], mean observation: 0.129 [-0.200, 0.636], loss: 0.000083, mean_absolute_error: 0.462039, mean_q: 0.679105\n",
      " 32531/50000: episode: 613, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.038 [-0.010, 0.086], loss: 0.000019, mean_absolute_error: 0.437438, mean_q: 0.657560\n",
      " 32565/50000: episode: 614, duration: 0.184s, episode steps: 34, steps per second: 184, episode reward: 0.904, mean reward: 0.027 [-0.004, 1.000], mean action: 1.235 [0.000, 2.000], mean observation: -0.090 [-0.450, 0.180], loss: 0.000051, mean_absolute_error: 0.456241, mean_q: 0.663437\n",
      " 32613/50000: episode: 615, duration: 0.261s, episode steps: 48, steps per second: 184, episode reward: 0.799, mean reward: 0.017 [-0.007, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.145 [-0.220, 0.722], loss: 0.000256, mean_absolute_error: 0.461002, mean_q: 0.677944\n",
      " 32614/50000: episode: 616, duration: 0.010s, episode steps: 1, steps per second: 97, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.033 [-0.010, 0.076], loss: 0.011727, mean_absolute_error: 0.477201, mean_q: 0.678668\n",
      " 32675/50000: episode: 617, duration: 0.320s, episode steps: 61, steps per second: 191, episode reward: 0.694, mean reward: 0.011 [-0.008, 1.000], mean action: 0.869 [0.000, 2.000], mean observation: 0.190 [-0.230, 0.847], loss: 0.000068, mean_absolute_error: 0.457544, mean_q: 0.670875\n",
      " 32710/50000: episode: 618, duration: 0.218s, episode steps: 35, steps per second: 161, episode reward: 0.907, mean reward: 0.026 [-0.004, 1.000], mean action: 0.771 [0.000, 2.000], mean observation: 0.087 [-0.150, 0.428], loss: 0.000075, mean_absolute_error: 0.458450, mean_q: 0.674253\n",
      " 32711/50000: episode: 619, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [-0.010, 0.050], loss: 0.000135, mean_absolute_error: 0.501447, mean_q: 0.754519\n",
      " 32769/50000: episode: 620, duration: 0.313s, episode steps: 58, steps per second: 185, episode reward: 0.720, mean reward: 0.012 [-0.008, 1.000], mean action: 0.879 [0.000, 2.000], mean observation: 0.178 [-0.230, 0.838], loss: 0.000139, mean_absolute_error: 0.463201, mean_q: 0.679281\n",
      " 32790/50000: episode: 621, duration: 0.114s, episode steps: 21, steps per second: 185, episode reward: 0.961, mean reward: 0.046 [-0.003, 1.000], mean action: 1.381 [0.000, 2.000], mean observation: -0.055 [-0.262, 0.120], loss: 0.000058, mean_absolute_error: 0.461700, mean_q: 0.677511\n",
      " 32791/50000: episode: 622, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.034 [-0.079, 0.010], loss: 0.000040, mean_absolute_error: 0.449896, mean_q: 0.681288\n",
      " 32829/50000: episode: 623, duration: 0.202s, episode steps: 38, steps per second: 188, episode reward: 0.888, mean reward: 0.023 [-0.005, 1.000], mean action: 0.816 [0.000, 2.000], mean observation: 0.098 [-0.160, 0.473], loss: 0.000368, mean_absolute_error: 0.470204, mean_q: 0.684200\n",
      " 32857/50000: episode: 624, duration: 0.151s, episode steps: 28, steps per second: 185, episode reward: 0.933, mean reward: 0.033 [-0.004, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.074 [-0.140, 0.358], loss: 0.000206, mean_absolute_error: 0.476493, mean_q: 0.700320\n",
      " 32892/50000: episode: 625, duration: 0.183s, episode steps: 35, steps per second: 192, episode reward: 0.931, mean reward: 0.027 [-0.003, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.053 [-0.340, 0.140], loss: 0.000384, mean_absolute_error: 0.463225, mean_q: 0.675591\n",
      " 32928/50000: episode: 626, duration: 0.197s, episode steps: 36, steps per second: 183, episode reward: 0.924, mean reward: 0.026 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.056 [-0.359, 0.160], loss: 0.000381, mean_absolute_error: 0.466200, mean_q: 0.682628\n",
      " 32992/50000: episode: 627, duration: 0.339s, episode steps: 64, steps per second: 189, episode reward: 0.664, mean reward: 0.010 [-0.009, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.197 [-0.937, 0.190], loss: 0.000079, mean_absolute_error: 0.467939, mean_q: 0.684317\n",
      " 33002/50000: episode: 628, duration: 0.058s, episode steps: 10, steps per second: 174, episode reward: 0.988, mean reward: 0.099 [-0.001, 1.000], mean action: 0.200 [0.000, 2.000], mean observation: 0.037 [-0.080, 0.145], loss: 0.000071, mean_absolute_error: 0.465567, mean_q: 0.674022\n",
      " 33009/50000: episode: 629, duration: 0.041s, episode steps: 7, steps per second: 170, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.286 [0.000, 2.000], mean observation: 0.039 [-0.050, 0.123], loss: 0.000033, mean_absolute_error: 0.444727, mean_q: 0.662319\n",
      " 33038/50000: episode: 630, duration: 0.161s, episode steps: 29, steps per second: 180, episode reward: 0.930, mean reward: 0.032 [-0.004, 1.000], mean action: 0.759 [0.000, 2.000], mean observation: 0.076 [-0.150, 0.369], loss: 0.000183, mean_absolute_error: 0.470712, mean_q: 0.696379\n",
      " 33067/50000: episode: 631, duration: 0.155s, episode steps: 29, steps per second: 187, episode reward: 0.933, mean reward: 0.032 [-0.004, 1.000], mean action: 0.759 [0.000, 2.000], mean observation: 0.072 [-0.140, 0.353], loss: 0.000070, mean_absolute_error: 0.457661, mean_q: 0.671100\n",
      " 33131/50000: episode: 632, duration: 0.453s, episode steps: 64, steps per second: 141, episode reward: 0.686, mean reward: 0.011 [-0.009, 1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.184 [-0.877, 0.190], loss: 0.000222, mean_absolute_error: 0.462361, mean_q: 0.672400\n",
      " 33190/50000: episode: 633, duration: 0.310s, episode steps: 59, steps per second: 190, episode reward: 0.703, mean reward: 0.012 [-0.009, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.185 [-0.230, 0.896], loss: 0.000120, mean_absolute_error: 0.460804, mean_q: 0.669644\n",
      " 33207/50000: episode: 634, duration: 0.094s, episode steps: 17, steps per second: 182, episode reward: 0.975, mean reward: 0.057 [-0.002, 1.000], mean action: 0.588 [0.000, 2.000], mean observation: 0.046 [-0.080, 0.195], loss: 0.000065, mean_absolute_error: 0.467377, mean_q: 0.688500\n",
      " 33242/50000: episode: 635, duration: 0.188s, episode steps: 35, steps per second: 186, episode reward: 0.898, mean reward: 0.026 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.094 [-0.180, 0.467], loss: 0.000048, mean_absolute_error: 0.458904, mean_q: 0.674622\n",
      " 33303/50000: episode: 636, duration: 0.485s, episode steps: 61, steps per second: 126, episode reward: 0.680, mean reward: 0.011 [-0.010, 1.000], mean action: 0.869 [0.000, 2.000], mean observation: 0.192 [-0.240, 0.956], loss: 0.000122, mean_absolute_error: 0.458126, mean_q: 0.674862\n",
      " 33347/50000: episode: 637, duration: 0.231s, episode steps: 44, steps per second: 191, episode reward: 0.850, mean reward: 0.019 [-0.006, 1.000], mean action: 0.886 [0.000, 2.000], mean observation: 0.116 [-0.190, 0.584], loss: 0.000075, mean_absolute_error: 0.464706, mean_q: 0.685258\n",
      " 33407/50000: episode: 638, duration: 0.317s, episode steps: 60, steps per second: 189, episode reward: 0.744, mean reward: 0.012 [-0.007, 1.000], mean action: 1.150 [0.000, 2.000], mean observation: -0.161 [-0.738, 0.180], loss: 0.000071, mean_absolute_error: 0.469506, mean_q: 0.689352\n",
      " 33462/50000: episode: 639, duration: 0.293s, episode steps: 55, steps per second: 188, episode reward: 0.764, mean reward: 0.014 [-0.008, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.155 [-0.760, 0.190], loss: 0.000066, mean_absolute_error: 0.468930, mean_q: 0.687371\n",
      " 33508/50000: episode: 640, duration: 0.244s, episode steps: 46, steps per second: 189, episode reward: 0.826, mean reward: 0.018 [-0.007, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.130 [-0.200, 0.651], loss: 0.000300, mean_absolute_error: 0.470337, mean_q: 0.686985\n",
      " 33571/50000: episode: 641, duration: 0.329s, episode steps: 63, steps per second: 191, episode reward: 0.666, mean reward: 0.011 [-0.010, 1.000], mean action: 0.873 [0.000, 2.000], mean observation: 0.196 [-0.230, 0.974], loss: 0.000053, mean_absolute_error: 0.466448, mean_q: 0.685443\n",
      " 33628/50000: episode: 642, duration: 0.305s, episode steps: 57, steps per second: 187, episode reward: 0.723, mean reward: 0.013 [-0.008, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.181 [-0.806, 0.210], loss: 0.000049, mean_absolute_error: 0.462731, mean_q: 0.676977\n",
      " 33660/50000: episode: 643, duration: 0.175s, episode steps: 32, steps per second: 183, episode reward: 0.905, mean reward: 0.028 [-0.005, 1.000], mean action: 1.281 [0.000, 2.000], mean observation: -0.093 [-0.453, 0.170], loss: 0.000401, mean_absolute_error: 0.456936, mean_q: 0.664477\n",
      " 33705/50000: episode: 644, duration: 0.235s, episode steps: 45, steps per second: 191, episode reward: 0.842, mean reward: 0.019 [-0.006, 1.000], mean action: 0.844 [0.000, 2.000], mean observation: 0.124 [-0.170, 0.572], loss: 0.000127, mean_absolute_error: 0.463281, mean_q: 0.674234\n",
      " 33765/50000: episode: 645, duration: 0.327s, episode steps: 60, steps per second: 184, episode reward: 0.687, mean reward: 0.011 [-0.009, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.192 [-0.220, 0.930], loss: 0.000197, mean_absolute_error: 0.465403, mean_q: 0.680525\n",
      " 33819/50000: episode: 646, duration: 0.288s, episode steps: 54, steps per second: 188, episode reward: 0.780, mean reward: 0.014 [-0.007, 1.000], mean action: 0.889 [0.000, 2.000], mean observation: 0.147 [-0.190, 0.714], loss: 0.000053, mean_absolute_error: 0.470048, mean_q: 0.692723\n",
      " 33826/50000: episode: 647, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.035 [-0.122, 0.070], loss: 0.000063, mean_absolute_error: 0.470123, mean_q: 0.677826\n",
      " 33887/50000: episode: 648, duration: 0.323s, episode steps: 61, steps per second: 189, episode reward: 0.702, mean reward: 0.012 [-0.009, 1.000], mean action: 0.885 [0.000, 2.000], mean observation: 0.183 [-0.210, 0.854], loss: 0.000087, mean_absolute_error: 0.469205, mean_q: 0.687169\n",
      " 33914/50000: episode: 649, duration: 0.144s, episode steps: 27, steps per second: 187, episode reward: 0.937, mean reward: 0.035 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.072 [-0.339, 0.130], loss: 0.000131, mean_absolute_error: 0.474381, mean_q: 0.698589\n",
      " 33953/50000: episode: 650, duration: 0.208s, episode steps: 39, steps per second: 187, episode reward: 0.900, mean reward: 0.023 [-0.004, 1.000], mean action: 0.872 [0.000, 2.000], mean observation: 0.092 [-0.130, 0.390], loss: 0.000157, mean_absolute_error: 0.465164, mean_q: 0.678895\n",
      " 33954/50000: episode: 651, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.013 [-0.010, 0.036], loss: 0.000042, mean_absolute_error: 0.463825, mean_q: 0.680767\n",
      " 33977/50000: episode: 652, duration: 0.128s, episode steps: 23, steps per second: 180, episode reward: 0.959, mean reward: 0.042 [-0.002, 1.000], mean action: 1.348 [0.000, 2.000], mean observation: -0.058 [-0.242, 0.100], loss: 0.000057, mean_absolute_error: 0.464086, mean_q: 0.678537\n",
      " 33978/50000: episode: 653, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.015 [-0.010, 0.040], loss: 0.000039, mean_absolute_error: 0.468697, mean_q: 0.694865\n",
      " 34019/50000: episode: 654, duration: 0.220s, episode steps: 41, steps per second: 186, episode reward: 0.853, mean reward: 0.021 [-0.006, 1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.121 [-0.574, 0.160], loss: 0.000061, mean_absolute_error: 0.468986, mean_q: 0.685660\n",
      " 34072/50000: episode: 655, duration: 0.284s, episode steps: 53, steps per second: 187, episode reward: 0.789, mean reward: 0.015 [-0.007, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.141 [-0.697, 0.190], loss: 0.000448, mean_absolute_error: 0.468127, mean_q: 0.682715\n",
      " 34120/50000: episode: 656, duration: 0.257s, episode steps: 48, steps per second: 187, episode reward: 0.796, mean reward: 0.017 [-0.007, 1.000], mean action: 1.146 [0.000, 2.000], mean observation: -0.148 [-0.718, 0.200], loss: 0.000052, mean_absolute_error: 0.459194, mean_q: 0.673311\n",
      " 34176/50000: episode: 657, duration: 0.303s, episode steps: 56, steps per second: 185, episode reward: 0.734, mean reward: 0.013 [-0.008, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.169 [-0.827, 0.190], loss: 0.000224, mean_absolute_error: 0.467782, mean_q: 0.680747\n",
      " 34224/50000: episode: 658, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 0.810, mean reward: 0.017 [-0.006, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.141 [-0.647, 0.190], loss: 0.000313, mean_absolute_error: 0.468496, mean_q: 0.678083\n",
      " 34267/50000: episode: 659, duration: 0.230s, episode steps: 43, steps per second: 187, episode reward: 0.847, mean reward: 0.020 [-0.006, 1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.122 [-0.580, 0.160], loss: 0.000089, mean_absolute_error: 0.464054, mean_q: 0.672273\n",
      " 34297/50000: episode: 660, duration: 0.159s, episode steps: 30, steps per second: 189, episode reward: 0.924, mean reward: 0.031 [-0.004, 1.000], mean action: 1.267 [0.000, 2.000], mean observation: -0.079 [-0.390, 0.160], loss: 0.000093, mean_absolute_error: 0.471132, mean_q: 0.687021\n",
      " 34366/50000: episode: 661, duration: 0.366s, episode steps: 69, steps per second: 188, episode reward: 0.637, mean reward: 0.009 [-0.010, 1.000], mean action: 0.899 [0.000, 2.000], mean observation: 0.199 [-0.240, 0.990], loss: 0.000381, mean_absolute_error: 0.469185, mean_q: 0.683698\n",
      " 34398/50000: episode: 662, duration: 0.180s, episode steps: 32, steps per second: 178, episode reward: 0.922, mean reward: 0.029 [-0.004, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.080 [-0.381, 0.140], loss: 0.000054, mean_absolute_error: 0.465075, mean_q: 0.670022\n",
      " 34413/50000: episode: 663, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 0.533 [0.000, 2.000], mean observation: 0.047 [-0.080, 0.175], loss: 0.000033, mean_absolute_error: 0.459249, mean_q: 0.667335\n",
      " 34414/50000: episode: 664, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [-0.010, 0.017], loss: 0.002307, mean_absolute_error: 0.452576, mean_q: 0.642624\n",
      " 34433/50000: episode: 665, duration: 0.104s, episode steps: 19, steps per second: 182, episode reward: 0.975, mean reward: 0.051 [-0.002, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.046 [-0.090, 0.185], loss: 0.000338, mean_absolute_error: 0.466047, mean_q: 0.676094\n",
      " 34483/50000: episode: 666, duration: 0.266s, episode steps: 50, steps per second: 188, episode reward: 0.798, mean reward: 0.016 [-0.007, 1.000], mean action: 0.860 [0.000, 2.000], mean observation: 0.144 [-0.190, 0.680], loss: 0.000055, mean_absolute_error: 0.465531, mean_q: 0.680176\n",
      " 34545/50000: episode: 667, duration: 0.332s, episode steps: 62, steps per second: 187, episode reward: 0.707, mean reward: 0.011 [-0.009, 1.000], mean action: 0.919 [0.000, 2.000], mean observation: 0.175 [-0.200, 0.862], loss: 0.000095, mean_absolute_error: 0.465994, mean_q: 0.682057\n",
      " 34603/50000: episode: 668, duration: 0.309s, episode steps: 58, steps per second: 188, episode reward: 0.750, mean reward: 0.013 [-0.008, 1.000], mean action: 0.897 [0.000, 2.000], mean observation: 0.157 [-0.190, 0.789], loss: 0.000058, mean_absolute_error: 0.470016, mean_q: 0.685357\n",
      " 34638/50000: episode: 669, duration: 0.180s, episode steps: 35, steps per second: 195, episode reward: 0.903, mean reward: 0.026 [-0.004, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.095 [-0.411, 0.160], loss: 0.000054, mean_absolute_error: 0.460268, mean_q: 0.669838\n",
      " 34653/50000: episode: 670, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 1.467 [0.000, 2.000], mean observation: -0.046 [-0.175, 0.070], loss: 0.000058, mean_absolute_error: 0.476253, mean_q: 0.696136\n",
      " 34712/50000: episode: 671, duration: 0.314s, episode steps: 59, steps per second: 188, episode reward: 0.686, mean reward: 0.012 [-0.010, 1.000], mean action: 1.119 [0.000, 2.000], mean observation: -0.195 [-0.953, 0.240], loss: 0.000043, mean_absolute_error: 0.464759, mean_q: 0.675503\n",
      " 34748/50000: episode: 672, duration: 0.193s, episode steps: 36, steps per second: 187, episode reward: 0.899, mean reward: 0.025 [-0.004, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.094 [-0.439, 0.160], loss: 0.000129, mean_absolute_error: 0.471678, mean_q: 0.692634\n",
      " 34784/50000: episode: 673, duration: 0.194s, episode steps: 36, steps per second: 185, episode reward: 0.906, mean reward: 0.025 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.081 [-0.431, 0.160], loss: 0.000354, mean_absolute_error: 0.473115, mean_q: 0.688655\n",
      " 34841/50000: episode: 674, duration: 0.297s, episode steps: 57, steps per second: 192, episode reward: 0.768, mean reward: 0.013 [-0.007, 1.000], mean action: 0.912 [0.000, 2.000], mean observation: 0.149 [-0.180, 0.732], loss: 0.000124, mean_absolute_error: 0.468398, mean_q: 0.685575\n",
      " 34882/50000: episode: 675, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 0.868, mean reward: 0.021 [-0.005, 1.000], mean action: 1.195 [0.000, 2.000], mean observation: -0.108 [-0.534, 0.160], loss: 0.000043, mean_absolute_error: 0.471922, mean_q: 0.691837\n",
      " 34921/50000: episode: 676, duration: 0.209s, episode steps: 39, steps per second: 186, episode reward: 0.895, mean reward: 0.023 [-0.004, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.093 [-0.150, 0.432], loss: 0.000133, mean_absolute_error: 0.477948, mean_q: 0.701197\n",
      " 34964/50000: episode: 677, duration: 0.230s, episode steps: 43, steps per second: 187, episode reward: 0.860, mean reward: 0.020 [-0.006, 1.000], mean action: 0.884 [0.000, 2.000], mean observation: 0.111 [-0.170, 0.553], loss: 0.000044, mean_absolute_error: 0.460666, mean_q: 0.668914\n",
      " 35022/50000: episode: 678, duration: 0.306s, episode steps: 58, steps per second: 190, episode reward: 0.753, mean reward: 0.013 [-0.008, 1.000], mean action: 1.034 [0.000, 2.000], mean observation: -0.156 [-0.776, 0.180], loss: 0.000228, mean_absolute_error: 0.463049, mean_q: 0.672669\n",
      " 35033/50000: episode: 679, duration: 0.062s, episode steps: 11, steps per second: 177, episode reward: 0.987, mean reward: 0.090 [-0.001, 1.000], mean action: 0.545 [0.000, 2.000], mean observation: 0.045 [-0.070, 0.139], loss: 0.000055, mean_absolute_error: 0.473803, mean_q: 0.696719\n",
      " 35077/50000: episode: 680, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: 0.873, mean reward: 0.020 [-0.005, 1.000], mean action: 0.818 [0.000, 2.000], mean observation: 0.100 [-0.160, 0.494], loss: 0.000048, mean_absolute_error: 0.476277, mean_q: 0.697606\n",
      " 35111/50000: episode: 681, duration: 0.182s, episode steps: 34, steps per second: 187, episode reward: 0.913, mean reward: 0.027 [-0.004, 1.000], mean action: 0.765 [0.000, 2.000], mean observation: 0.085 [-0.130, 0.398], loss: 0.000048, mean_absolute_error: 0.467444, mean_q: 0.683702\n",
      " 35160/50000: episode: 682, duration: 0.258s, episode steps: 49, steps per second: 190, episode reward: 0.819, mean reward: 0.017 [-0.007, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: -0.122 [-0.656, 0.190], loss: 0.000306, mean_absolute_error: 0.470090, mean_q: 0.690605\n",
      " 35231/50000: episode: 683, duration: 0.434s, episode steps: 71, steps per second: 164, episode reward: 0.624, mean reward: 0.009 [-0.010, 1.000], mean action: 0.915 [0.000, 2.000], mean observation: 0.203 [-0.210, 0.987], loss: 0.000087, mean_absolute_error: 0.471125, mean_q: 0.686963\n",
      " 35289/50000: episode: 684, duration: 0.321s, episode steps: 58, steps per second: 181, episode reward: 0.713, mean reward: 0.012 [-0.008, 1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.183 [-0.845, 0.200], loss: 0.000107, mean_absolute_error: 0.468615, mean_q: 0.689207\n",
      " 35293/50000: episode: 685, duration: 0.032s, episode steps: 4, steps per second: 126, episode reward: 0.997, mean reward: 0.249 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.039 [-0.108, 0.040], loss: 0.000049, mean_absolute_error: 0.463620, mean_q: 0.681283\n",
      " 35308/50000: episode: 686, duration: 0.095s, episode steps: 15, steps per second: 157, episode reward: 0.978, mean reward: 0.065 [-0.002, 1.000], mean action: 0.533 [0.000, 2.000], mean observation: 0.044 [-0.090, 0.191], loss: 0.000051, mean_absolute_error: 0.472833, mean_q: 0.679105\n",
      " 35365/50000: episode: 687, duration: 0.320s, episode steps: 57, steps per second: 178, episode reward: 0.746, mean reward: 0.013 [-0.008, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.164 [-0.190, 0.776], loss: 0.000053, mean_absolute_error: 0.466838, mean_q: 0.675913\n",
      " 35421/50000: episode: 688, duration: 0.298s, episode steps: 56, steps per second: 188, episode reward: 0.773, mean reward: 0.014 [-0.007, 1.000], mean action: 1.089 [0.000, 2.000], mean observation: -0.149 [-0.715, 0.170], loss: 0.000148, mean_absolute_error: 0.466157, mean_q: 0.677692\n",
      " 35422/50000: episode: 689, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.041 [-0.010, 0.092], loss: 0.000046, mean_absolute_error: 0.469546, mean_q: 0.699443\n",
      " 35423/50000: episode: 690, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.000 [0.000, 0.001], loss: 0.000024, mean_absolute_error: 0.464280, mean_q: 0.699056\n",
      " 35450/50000: episode: 691, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 0.946, mean reward: 0.035 [-0.003, 1.000], mean action: 1.222 [0.000, 2.000], mean observation: -0.066 [-0.296, 0.130], loss: 0.000056, mean_absolute_error: 0.463345, mean_q: 0.664658\n",
      " 35518/50000: episode: 692, duration: 0.389s, episode steps: 68, steps per second: 175, episode reward: 0.645, mean reward: 0.009 [-0.009, 1.000], mean action: 0.882 [0.000, 2.000], mean observation: 0.200 [-0.210, 0.925], loss: 0.000252, mean_absolute_error: 0.470285, mean_q: 0.688938\n",
      " 35543/50000: episode: 693, duration: 0.135s, episode steps: 25, steps per second: 185, episode reward: 0.950, mean reward: 0.038 [-0.003, 1.000], mean action: 1.240 [0.000, 2.000], mean observation: -0.061 [-0.296, 0.130], loss: 0.000057, mean_absolute_error: 0.467860, mean_q: 0.682197\n",
      " 35544/50000: episode: 694, duration: 0.011s, episode steps: 1, steps per second: 94, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.009 [-0.028, 0.010], loss: 0.000059, mean_absolute_error: 0.514205, mean_q: 0.778799\n",
      " 35591/50000: episode: 695, duration: 0.249s, episode steps: 47, steps per second: 189, episode reward: 0.809, mean reward: 0.017 [-0.007, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.139 [-0.170, 0.685], loss: 0.000237, mean_absolute_error: 0.470390, mean_q: 0.683300\n",
      " 35637/50000: episode: 696, duration: 0.246s, episode steps: 46, steps per second: 187, episode reward: 0.820, mean reward: 0.018 [-0.006, 1.000], mean action: 0.826 [0.000, 2.000], mean observation: 0.137 [-0.180, 0.647], loss: 0.000350, mean_absolute_error: 0.470651, mean_q: 0.692698\n",
      " 35659/50000: episode: 697, duration: 0.120s, episode steps: 22, steps per second: 184, episode reward: 0.963, mean reward: 0.044 [-0.002, 1.000], mean action: 1.318 [0.000, 2.000], mean observation: -0.055 [-0.236, 0.090], loss: 0.000393, mean_absolute_error: 0.476320, mean_q: 0.699828\n",
      " 35684/50000: episode: 698, duration: 0.136s, episode steps: 25, steps per second: 184, episode reward: 0.950, mean reward: 0.038 [-0.003, 1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.060 [-0.288, 0.120], loss: 0.000060, mean_absolute_error: 0.467043, mean_q: 0.685489\n",
      " 35747/50000: episode: 699, duration: 0.335s, episode steps: 63, steps per second: 188, episode reward: 0.705, mean reward: 0.011 [-0.008, 1.000], mean action: 0.905 [0.000, 2.000], mean observation: 0.176 [-0.190, 0.837], loss: 0.000161, mean_absolute_error: 0.463947, mean_q: 0.677619\n",
      " 35815/50000: episode: 700, duration: 0.363s, episode steps: 68, steps per second: 187, episode reward: 0.664, mean reward: 0.010 [-0.009, 1.000], mean action: 0.897 [0.000, 2.000], mean observation: 0.191 [-0.160, 0.871], loss: 0.000127, mean_absolute_error: 0.470529, mean_q: 0.683736\n",
      " 35871/50000: episode: 701, duration: 0.334s, episode steps: 56, steps per second: 168, episode reward: 0.773, mean reward: 0.014 [-0.007, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.150 [-0.170, 0.683], loss: 0.000056, mean_absolute_error: 0.467502, mean_q: 0.683515\n",
      " 35911/50000: episode: 702, duration: 0.253s, episode steps: 40, steps per second: 158, episode reward: 0.880, mean reward: 0.022 [-0.005, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.107 [-0.456, 0.150], loss: 0.000102, mean_absolute_error: 0.466345, mean_q: 0.688638\n",
      " 35978/50000: episode: 703, duration: 0.398s, episode steps: 67, steps per second: 168, episode reward: 0.637, mean reward: 0.010 [-0.010, 1.000], mean action: 0.881 [0.000, 2.000], mean observation: 0.205 [-0.180, 0.980], loss: 0.000044, mean_absolute_error: 0.469511, mean_q: 0.691622\n",
      " 36042/50000: episode: 704, duration: 0.338s, episode steps: 64, steps per second: 189, episode reward: 0.669, mean reward: 0.010 [-0.009, 1.000], mean action: 0.922 [0.000, 2.000], mean observation: 0.194 [-0.230, 0.938], loss: 0.000067, mean_absolute_error: 0.477874, mean_q: 0.696756\n",
      " 36073/50000: episode: 705, duration: 0.213s, episode steps: 31, steps per second: 145, episode reward: 0.930, mean reward: 0.030 [-0.004, 1.000], mean action: 0.774 [0.000, 2.000], mean observation: 0.074 [-0.130, 0.350], loss: 0.000386, mean_absolute_error: 0.471821, mean_q: 0.690543\n",
      " 36111/50000: episode: 706, duration: 0.334s, episode steps: 38, steps per second: 114, episode reward: 0.888, mean reward: 0.023 [-0.005, 1.000], mean action: 1.211 [0.000, 2.000], mean observation: -0.098 [-0.474, 0.170], loss: 0.000056, mean_absolute_error: 0.471723, mean_q: 0.692323\n",
      " 36112/50000: episode: 707, duration: 0.047s, episode steps: 1, steps per second: 21, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.002 [0.000, 0.004], loss: 0.000025, mean_absolute_error: 0.463480, mean_q: 0.660211\n",
      " 36160/50000: episode: 708, duration: 0.378s, episode steps: 48, steps per second: 127, episode reward: 0.832, mean reward: 0.017 [-0.006, 1.000], mean action: 0.875 [0.000, 2.000], mean observation: 0.123 [-0.160, 0.601], loss: 0.000057, mean_absolute_error: 0.471080, mean_q: 0.683514\n",
      " 36211/50000: episode: 709, duration: 0.307s, episode steps: 51, steps per second: 166, episode reward: 0.808, mean reward: 0.016 [-0.006, 1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.136 [-0.640, 0.170], loss: 0.000143, mean_absolute_error: 0.475681, mean_q: 0.690500\n",
      " 36212/50000: episode: 710, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.034 [-0.078, 0.010], loss: 0.000021, mean_absolute_error: 0.437872, mean_q: 0.613801\n",
      " 36249/50000: episode: 711, duration: 0.200s, episode steps: 37, steps per second: 185, episode reward: 0.891, mean reward: 0.024 [-0.005, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.097 [-0.150, 0.474], loss: 0.000489, mean_absolute_error: 0.464636, mean_q: 0.678516\n",
      " 36288/50000: episode: 712, duration: 0.225s, episode steps: 39, steps per second: 174, episode reward: 0.894, mean reward: 0.023 [-0.005, 1.000], mean action: 1.128 [0.000, 2.000], mean observation: -0.091 [-0.455, 0.170], loss: 0.000387, mean_absolute_error: 0.471062, mean_q: 0.686858\n",
      " 36308/50000: episode: 713, duration: 0.130s, episode steps: 20, steps per second: 153, episode reward: 0.966, mean reward: 0.048 [-0.002, 1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.056 [-0.221, 0.100], loss: 0.000197, mean_absolute_error: 0.469907, mean_q: 0.687563\n",
      " 36342/50000: episode: 714, duration: 0.194s, episode steps: 34, steps per second: 175, episode reward: 0.908, mean reward: 0.027 [-0.004, 1.000], mean action: 0.765 [0.000, 2.000], mean observation: 0.089 [-0.140, 0.421], loss: 0.000077, mean_absolute_error: 0.475648, mean_q: 0.694899\n",
      " 36386/50000: episode: 715, duration: 0.244s, episode steps: 44, steps per second: 180, episode reward: 0.851, mean reward: 0.019 [-0.005, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.119 [-0.200, 0.549], loss: 0.000057, mean_absolute_error: 0.467127, mean_q: 0.677910\n",
      " 36400/50000: episode: 716, duration: 0.078s, episode steps: 14, steps per second: 180, episode reward: 0.980, mean reward: 0.070 [-0.002, 1.000], mean action: 0.429 [0.000, 2.000], mean observation: 0.043 [-0.090, 0.184], loss: 0.000036, mean_absolute_error: 0.463090, mean_q: 0.668308\n",
      " 36422/50000: episode: 717, duration: 0.118s, episode steps: 22, steps per second: 186, episode reward: 0.963, mean reward: 0.044 [-0.002, 1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.055 [-0.232, 0.100], loss: 0.000067, mean_absolute_error: 0.467073, mean_q: 0.676984\n",
      " 36473/50000: episode: 718, duration: 0.314s, episode steps: 51, steps per second: 162, episode reward: 0.793, mean reward: 0.016 [-0.007, 1.000], mean action: 1.157 [0.000, 2.000], mean observation: -0.144 [-0.701, 0.190], loss: 0.000102, mean_absolute_error: 0.466752, mean_q: 0.681206\n",
      " 36521/50000: episode: 719, duration: 0.314s, episode steps: 48, steps per second: 153, episode reward: 0.836, mean reward: 0.017 [-0.006, 1.000], mean action: 0.854 [0.000, 2.000], mean observation: 0.120 [-0.170, 0.595], loss: 0.000067, mean_absolute_error: 0.473510, mean_q: 0.693796\n",
      " 36548/50000: episode: 720, duration: 0.155s, episode steps: 27, steps per second: 175, episode reward: 0.942, mean reward: 0.035 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.064 [-0.120, 0.320], loss: 0.000149, mean_absolute_error: 0.475217, mean_q: 0.691319\n",
      " 36549/50000: episode: 721, duration: 0.016s, episode steps: 1, steps per second: 64, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.021 [-0.052, 0.010], loss: 0.000135, mean_absolute_error: 0.498733, mean_q: 0.729931\n",
      " 36550/50000: episode: 722, duration: 0.015s, episode steps: 1, steps per second: 68, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.004 [-0.019, 0.010], loss: 0.000066, mean_absolute_error: 0.464902, mean_q: 0.682803\n",
      " 36557/50000: episode: 723, duration: 0.047s, episode steps: 7, steps per second: 149, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.286 [0.000, 2.000], mean observation: 0.038 [-0.060, 0.124], loss: 0.000080, mean_absolute_error: 0.474890, mean_q: 0.693242\n",
      " 36586/50000: episode: 724, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 0.938, mean reward: 0.032 [-0.003, 1.000], mean action: 1.276 [0.000, 2.000], mean observation: -0.068 [-0.326, 0.130], loss: 0.000272, mean_absolute_error: 0.469023, mean_q: 0.683184\n",
      " 36606/50000: episode: 725, duration: 0.124s, episode steps: 20, steps per second: 161, episode reward: 0.965, mean reward: 0.048 [-0.002, 1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.051 [-0.245, 0.120], loss: 0.000058, mean_absolute_error: 0.473414, mean_q: 0.694517\n",
      " 36641/50000: episode: 726, duration: 0.201s, episode steps: 35, steps per second: 174, episode reward: 0.896, mean reward: 0.026 [-0.005, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.097 [-0.472, 0.170], loss: 0.000426, mean_absolute_error: 0.469037, mean_q: 0.684703\n",
      " 36679/50000: episode: 727, duration: 0.219s, episode steps: 38, steps per second: 173, episode reward: 0.900, mean reward: 0.024 [-0.004, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.097 [-0.140, 0.371], loss: 0.000135, mean_absolute_error: 0.466612, mean_q: 0.683243\n",
      " 36706/50000: episode: 728, duration: 0.167s, episode steps: 27, steps per second: 162, episode reward: 0.946, mean reward: 0.035 [-0.003, 1.000], mean action: 0.704 [0.000, 2.000], mean observation: 0.064 [-0.110, 0.299], loss: 0.000055, mean_absolute_error: 0.463285, mean_q: 0.679955\n",
      " 36751/50000: episode: 729, duration: 0.259s, episode steps: 45, steps per second: 174, episode reward: 0.853, mean reward: 0.019 [-0.006, 1.000], mean action: 0.844 [0.000, 2.000], mean observation: 0.114 [-0.140, 0.554], loss: 0.000046, mean_absolute_error: 0.467860, mean_q: 0.680890\n",
      " 36787/50000: episode: 730, duration: 0.227s, episode steps: 36, steps per second: 159, episode reward: 0.898, mean reward: 0.025 [-0.005, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.093 [-0.160, 0.455], loss: 0.000386, mean_absolute_error: 0.470574, mean_q: 0.685917\n",
      " 36817/50000: episode: 731, duration: 0.158s, episode steps: 30, steps per second: 190, episode reward: 0.943, mean reward: 0.031 [-0.003, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.070 [-0.251, 0.100], loss: 0.000440, mean_absolute_error: 0.470344, mean_q: 0.685635\n",
      " 36853/50000: episode: 732, duration: 0.202s, episode steps: 36, steps per second: 178, episode reward: 0.903, mean reward: 0.025 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.088 [-0.423, 0.140], loss: 0.000190, mean_absolute_error: 0.456323, mean_q: 0.667261\n",
      " 36935/50000: episode: 733, duration: 0.459s, episode steps: 82, steps per second: 179, episode reward: 0.782, mean reward: 0.010 [-0.006, 1.000], mean action: 1.098 [0.000, 2.000], mean observation: 0.015 [-0.210, 0.622], loss: 0.000243, mean_absolute_error: 0.467997, mean_q: 0.685309\n",
      " 36967/50000: episode: 734, duration: 0.266s, episode steps: 32, steps per second: 120, episode reward: 0.922, mean reward: 0.029 [-0.004, 1.000], mean action: 1.281 [0.000, 2.000], mean observation: -0.081 [-0.365, 0.140], loss: 0.000132, mean_absolute_error: 0.467260, mean_q: 0.681155\n",
      " 37019/50000: episode: 735, duration: 0.349s, episode steps: 52, steps per second: 149, episode reward: 0.806, mean reward: 0.015 [-0.006, 1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.132 [-0.630, 0.200], loss: 0.000125, mean_absolute_error: 0.470338, mean_q: 0.692638\n",
      " 37071/50000: episode: 736, duration: 0.289s, episode steps: 52, steps per second: 180, episode reward: 0.805, mean reward: 0.015 [-0.006, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.135 [-0.150, 0.633], loss: 0.000036, mean_absolute_error: 0.475194, mean_q: 0.703023\n",
      " 37135/50000: episode: 737, duration: 0.421s, episode steps: 64, steps per second: 152, episode reward: 0.652, mean reward: 0.010 [-0.009, 1.000], mean action: 0.906 [0.000, 2.000], mean observation: 0.207 [-0.250, 0.935], loss: 0.000421, mean_absolute_error: 0.471323, mean_q: 0.689515\n",
      " 37136/50000: episode: 738, duration: 0.014s, episode steps: 1, steps per second: 71, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.021 [-0.010, 0.051], loss: 0.000040, mean_absolute_error: 0.465744, mean_q: 0.683735\n",
      " 37167/50000: episode: 739, duration: 0.188s, episode steps: 31, steps per second: 165, episode reward: 0.927, mean reward: 0.030 [-0.004, 1.000], mean action: 1.226 [0.000, 2.000], mean observation: -0.078 [-0.355, 0.130], loss: 0.000043, mean_absolute_error: 0.461987, mean_q: 0.674881\n",
      " 37181/50000: episode: 740, duration: 0.121s, episode steps: 14, steps per second: 116, episode reward: 0.982, mean reward: 0.070 [-0.002, 1.000], mean action: 1.429 [0.000, 2.000], mean observation: -0.044 [-0.157, 0.070], loss: 0.000057, mean_absolute_error: 0.467194, mean_q: 0.690767\n",
      " 37235/50000: episode: 741, duration: 0.382s, episode steps: 54, steps per second: 141, episode reward: 0.767, mean reward: 0.014 [-0.007, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.158 [-0.728, 0.190], loss: 0.000252, mean_absolute_error: 0.471355, mean_q: 0.692990\n",
      " 37277/50000: episode: 742, duration: 0.309s, episode steps: 42, steps per second: 136, episode reward: 0.865, mean reward: 0.021 [-0.005, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.117 [-0.481, 0.170], loss: 0.000345, mean_absolute_error: 0.467559, mean_q: 0.682658\n",
      " 37309/50000: episode: 743, duration: 0.207s, episode steps: 32, steps per second: 154, episode reward: 0.924, mean reward: 0.029 [-0.004, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.080 [-0.110, 0.358], loss: 0.000039, mean_absolute_error: 0.469066, mean_q: 0.679463\n",
      " 37379/50000: episode: 744, duration: 0.447s, episode steps: 70, steps per second: 157, episode reward: 0.689, mean reward: 0.010 [-0.009, 1.000], mean action: 0.871 [0.000, 2.000], mean observation: 0.161 [-0.190, 0.882], loss: 0.000399, mean_absolute_error: 0.467076, mean_q: 0.683617\n",
      " 37380/50000: episode: 745, duration: 0.011s, episode steps: 1, steps per second: 89, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.033 [-0.076, 0.010], loss: 0.000233, mean_absolute_error: 0.460299, mean_q: 0.686983\n",
      " 37381/50000: episode: 746, duration: 0.013s, episode steps: 1, steps per second: 74, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.031 [0.010, 0.051], loss: 0.000060, mean_absolute_error: 0.473195, mean_q: 0.639131\n",
      " 37440/50000: episode: 747, duration: 0.395s, episode steps: 59, steps per second: 149, episode reward: 0.722, mean reward: 0.012 [-0.008, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.175 [-0.190, 0.814], loss: 0.000217, mean_absolute_error: 0.468976, mean_q: 0.688918\n",
      " 37487/50000: episode: 748, duration: 0.335s, episode steps: 47, steps per second: 140, episode reward: 0.873, mean reward: 0.019 [-0.005, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.084 [-0.160, 0.494], loss: 0.000372, mean_absolute_error: 0.476186, mean_q: 0.697647\n",
      " 37547/50000: episode: 749, duration: 0.314s, episode steps: 60, steps per second: 191, episode reward: 0.719, mean reward: 0.012 [-0.008, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.170 [-0.190, 0.844], loss: 0.000060, mean_absolute_error: 0.473011, mean_q: 0.693002\n",
      " 37612/50000: episode: 750, duration: 0.421s, episode steps: 65, steps per second: 155, episode reward: 0.655, mean reward: 0.010 [-0.009, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.204 [-0.170, 0.905], loss: 0.000119, mean_absolute_error: 0.462372, mean_q: 0.676036\n",
      " 37659/50000: episode: 751, duration: 0.295s, episode steps: 47, steps per second: 159, episode reward: 0.832, mean reward: 0.018 [-0.006, 1.000], mean action: 1.106 [0.000, 2.000], mean observation: -0.127 [-0.596, 0.170], loss: 0.000107, mean_absolute_error: 0.468892, mean_q: 0.684649\n",
      " 37677/50000: episode: 752, duration: 0.101s, episode steps: 18, steps per second: 178, episode reward: 0.972, mean reward: 0.054 [-0.002, 1.000], mean action: 1.444 [0.000, 2.000], mean observation: -0.049 [-0.212, 0.090], loss: 0.000060, mean_absolute_error: 0.474326, mean_q: 0.690164\n",
      " 37726/50000: episode: 753, duration: 0.255s, episode steps: 49, steps per second: 192, episode reward: 0.802, mean reward: 0.016 [-0.007, 1.000], mean action: 1.122 [0.000, 2.000], mean observation: -0.143 [-0.685, 0.190], loss: 0.000067, mean_absolute_error: 0.463136, mean_q: 0.677366\n",
      " 37727/50000: episode: 754, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.009 [-0.028, 0.010], loss: 0.000095, mean_absolute_error: 0.454329, mean_q: 0.672814\n",
      " 37728/50000: episode: 755, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.028 [-0.065, 0.010], loss: 0.000060, mean_absolute_error: 0.426179, mean_q: 0.632015\n",
      " 37735/50000: episode: 756, duration: 0.040s, episode steps: 7, steps per second: 176, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.286 [0.000, 2.000], mean observation: 0.039 [-0.050, 0.115], loss: 0.000050, mean_absolute_error: 0.476584, mean_q: 0.690329\n",
      " 37762/50000: episode: 757, duration: 0.143s, episode steps: 27, steps per second: 189, episode reward: 0.942, mean reward: 0.035 [-0.003, 1.000], mean action: 0.704 [0.000, 2.000], mean observation: 0.067 [-0.120, 0.319], loss: 0.000282, mean_absolute_error: 0.470577, mean_q: 0.690267\n",
      " 37819/50000: episode: 758, duration: 0.316s, episode steps: 57, steps per second: 180, episode reward: 0.728, mean reward: 0.013 [-0.008, 1.000], mean action: 0.842 [0.000, 2.000], mean observation: 0.170 [-0.190, 0.837], loss: 0.000426, mean_absolute_error: 0.474759, mean_q: 0.698392\n",
      " 37877/50000: episode: 759, duration: 0.500s, episode steps: 58, steps per second: 116, episode reward: 0.733, mean reward: 0.013 [-0.008, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.160 [-0.220, 0.836], loss: 0.000400, mean_absolute_error: 0.475344, mean_q: 0.697773\n",
      " 37878/50000: episode: 760, duration: 0.014s, episode steps: 1, steps per second: 70, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.010 [-0.031, 0.010], loss: 0.000110, mean_absolute_error: 0.436635, mean_q: 0.608142\n",
      " 37931/50000: episode: 761, duration: 0.392s, episode steps: 53, steps per second: 135, episode reward: 0.764, mean reward: 0.014 [-0.008, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.161 [-0.763, 0.200], loss: 0.000053, mean_absolute_error: 0.470704, mean_q: 0.685115\n",
      " 37966/50000: episode: 762, duration: 0.192s, episode steps: 35, steps per second: 182, episode reward: 0.898, mean reward: 0.026 [-0.005, 1.000], mean action: 0.771 [0.000, 2.000], mean observation: 0.095 [-0.180, 0.462], loss: 0.000047, mean_absolute_error: 0.467843, mean_q: 0.682776\n",
      " 38021/50000: episode: 763, duration: 0.295s, episode steps: 55, steps per second: 187, episode reward: 0.733, mean reward: 0.013 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.175 [-0.852, 0.220], loss: 0.000042, mean_absolute_error: 0.469625, mean_q: 0.690862\n",
      " 38065/50000: episode: 764, duration: 0.234s, episode steps: 44, steps per second: 188, episode reward: 0.840, mean reward: 0.019 [-0.006, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.125 [-0.150, 0.599], loss: 0.000118, mean_absolute_error: 0.467547, mean_q: 0.688515\n",
      " 38109/50000: episode: 765, duration: 0.232s, episode steps: 44, steps per second: 189, episode reward: 0.872, mean reward: 0.020 [-0.005, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.101 [-0.160, 0.484], loss: 0.000052, mean_absolute_error: 0.472157, mean_q: 0.692815\n",
      " 38156/50000: episode: 766, duration: 0.248s, episode steps: 47, steps per second: 189, episode reward: 0.797, mean reward: 0.017 [-0.007, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.150 [-0.721, 0.200], loss: 0.000314, mean_absolute_error: 0.466849, mean_q: 0.684403\n",
      " 38188/50000: episode: 767, duration: 0.172s, episode steps: 32, steps per second: 186, episode reward: 0.920, mean reward: 0.029 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.083 [-0.377, 0.120], loss: 0.000145, mean_absolute_error: 0.468346, mean_q: 0.685214\n",
      " 38248/50000: episode: 768, duration: 0.319s, episode steps: 60, steps per second: 188, episode reward: 0.725, mean reward: 0.012 [-0.008, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.166 [-0.220, 0.833], loss: 0.000654, mean_absolute_error: 0.464352, mean_q: 0.682530\n",
      " 38283/50000: episode: 769, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 0.903, mean reward: 0.026 [-0.004, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.096 [-0.406, 0.150], loss: 0.000088, mean_absolute_error: 0.468410, mean_q: 0.684937\n",
      " 38330/50000: episode: 770, duration: 0.248s, episode steps: 47, steps per second: 190, episode reward: 0.834, mean reward: 0.018 [-0.006, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.125 [-0.599, 0.170], loss: 0.000061, mean_absolute_error: 0.464802, mean_q: 0.680411\n",
      " 38376/50000: episode: 771, duration: 0.243s, episode steps: 46, steps per second: 189, episode reward: 0.818, mean reward: 0.018 [-0.007, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.135 [-0.674, 0.210], loss: 0.000041, mean_absolute_error: 0.460516, mean_q: 0.679054\n",
      " 38415/50000: episode: 772, duration: 0.212s, episode steps: 39, steps per second: 184, episode reward: 0.891, mean reward: 0.023 [-0.004, 1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.098 [-0.431, 0.150], loss: 0.000056, mean_absolute_error: 0.465198, mean_q: 0.678566\n",
      " 38446/50000: episode: 773, duration: 0.198s, episode steps: 31, steps per second: 157, episode reward: 0.926, mean reward: 0.030 [-0.004, 1.000], mean action: 1.258 [0.000, 2.000], mean observation: -0.076 [-0.370, 0.140], loss: 0.000230, mean_absolute_error: 0.464815, mean_q: 0.682102\n",
      " 38506/50000: episode: 774, duration: 0.323s, episode steps: 60, steps per second: 186, episode reward: 0.732, mean reward: 0.012 [-0.008, 1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.167 [-0.768, 0.180], loss: 0.000330, mean_absolute_error: 0.469817, mean_q: 0.689569\n",
      " 38546/50000: episode: 775, duration: 0.221s, episode steps: 40, steps per second: 181, episode reward: 0.869, mean reward: 0.022 [-0.005, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.107 [-0.180, 0.536], loss: 0.000046, mean_absolute_error: 0.462670, mean_q: 0.681057\n",
      " 38591/50000: episode: 776, duration: 0.238s, episode steps: 45, steps per second: 189, episode reward: 0.849, mean reward: 0.019 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.115 [-0.180, 0.560], loss: 0.000043, mean_absolute_error: 0.470806, mean_q: 0.697090\n",
      " 38649/50000: episode: 777, duration: 0.307s, episode steps: 58, steps per second: 189, episode reward: 0.679, mean reward: 0.012 [-0.010, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.197 [-0.240, 0.981], loss: 0.000085, mean_absolute_error: 0.467717, mean_q: 0.685343\n",
      " 38657/50000: episode: 778, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 0.992, mean reward: 0.124 [-0.001, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.037 [-0.070, 0.123], loss: 0.000039, mean_absolute_error: 0.459045, mean_q: 0.668501\n",
      " 38721/50000: episode: 779, duration: 0.340s, episode steps: 64, steps per second: 188, episode reward: 0.679, mean reward: 0.011 [-0.009, 1.000], mean action: 0.891 [0.000, 2.000], mean observation: 0.190 [-0.200, 0.883], loss: 0.000186, mean_absolute_error: 0.466246, mean_q: 0.684953\n",
      " 38784/50000: episode: 780, duration: 0.415s, episode steps: 63, steps per second: 152, episode reward: 0.703, mean reward: 0.011 [-0.008, 1.000], mean action: 1.095 [0.000, 2.000], mean observation: -0.177 [-0.839, 0.210], loss: 0.000046, mean_absolute_error: 0.471123, mean_q: 0.692087\n",
      " 38819/50000: episode: 781, duration: 0.195s, episode steps: 35, steps per second: 180, episode reward: 0.912, mean reward: 0.026 [-0.004, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.083 [-0.120, 0.393], loss: 0.000125, mean_absolute_error: 0.459759, mean_q: 0.676267\n",
      " 38865/50000: episode: 782, duration: 0.306s, episode steps: 46, steps per second: 150, episode reward: 0.832, mean reward: 0.018 [-0.006, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.125 [-0.170, 0.619], loss: 0.000319, mean_absolute_error: 0.464408, mean_q: 0.680265\n",
      " 38889/50000: episode: 783, duration: 0.158s, episode steps: 24, steps per second: 152, episode reward: 0.948, mean reward: 0.040 [-0.003, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.065 [-0.140, 0.309], loss: 0.000085, mean_absolute_error: 0.459800, mean_q: 0.676320\n",
      " 38910/50000: episode: 784, duration: 0.147s, episode steps: 21, steps per second: 143, episode reward: 0.967, mean reward: 0.046 [-0.002, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.040 [-0.130, 0.229], loss: 0.000203, mean_absolute_error: 0.467516, mean_q: 0.687197\n",
      " 38935/50000: episode: 785, duration: 0.173s, episode steps: 25, steps per second: 145, episode reward: 0.951, mean reward: 0.038 [-0.003, 1.000], mean action: 1.320 [0.000, 2.000], mean observation: -0.062 [-0.281, 0.110], loss: 0.000040, mean_absolute_error: 0.470632, mean_q: 0.693691\n",
      " 38936/50000: episode: 786, duration: 0.013s, episode steps: 1, steps per second: 77, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.020 [-0.010, 0.050], loss: 0.000022, mean_absolute_error: 0.442291, mean_q: 0.632503\n",
      " 38999/50000: episode: 787, duration: 0.395s, episode steps: 63, steps per second: 159, episode reward: 0.750, mean reward: 0.012 [-0.008, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.138 [-0.190, 0.765], loss: 0.000063, mean_absolute_error: 0.469907, mean_q: 0.694534\n",
      " 39061/50000: episode: 788, duration: 0.405s, episode steps: 62, steps per second: 153, episode reward: 0.684, mean reward: 0.011 [-0.009, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.191 [-0.891, 0.200], loss: 0.000046, mean_absolute_error: 0.469517, mean_q: 0.692355\n",
      " 39127/50000: episode: 789, duration: 0.437s, episode steps: 66, steps per second: 151, episode reward: 0.665, mean reward: 0.010 [-0.010, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.187 [-0.953, 0.210], loss: 0.000272, mean_absolute_error: 0.466333, mean_q: 0.682347\n",
      " 39183/50000: episode: 790, duration: 0.342s, episode steps: 56, steps per second: 164, episode reward: 0.701, mean reward: 0.013 [-0.009, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.195 [-0.240, 0.904], loss: 0.000327, mean_absolute_error: 0.471434, mean_q: 0.690019\n",
      " 39184/50000: episode: 791, duration: 0.010s, episode steps: 1, steps per second: 99, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.032 [-0.063, 0.000], loss: 0.000046, mean_absolute_error: 0.465479, mean_q: 0.682323\n",
      " 39225/50000: episode: 792, duration: 0.261s, episode steps: 41, steps per second: 157, episode reward: 0.880, mean reward: 0.021 [-0.005, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.102 [-0.140, 0.453], loss: 0.000065, mean_absolute_error: 0.469378, mean_q: 0.691951\n",
      " 39294/50000: episode: 793, duration: 0.480s, episode steps: 69, steps per second: 144, episode reward: 0.648, mean reward: 0.009 [-0.010, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.186 [-0.230, 0.978], loss: 0.000200, mean_absolute_error: 0.465370, mean_q: 0.679436\n",
      " 39313/50000: episode: 794, duration: 0.131s, episode steps: 19, steps per second: 146, episode reward: 0.970, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 2.000], mean observation: 0.046 [-0.110, 0.212], loss: 0.000197, mean_absolute_error: 0.476465, mean_q: 0.700924\n",
      " 39371/50000: episode: 795, duration: 0.380s, episode steps: 58, steps per second: 153, episode reward: 0.741, mean reward: 0.013 [-0.008, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.165 [-0.200, 0.774], loss: 0.000112, mean_absolute_error: 0.465175, mean_q: 0.678718\n",
      " 39417/50000: episode: 796, duration: 0.317s, episode steps: 46, steps per second: 145, episode reward: 0.851, mean reward: 0.019 [-0.005, 1.000], mean action: 1.152 [0.000, 2.000], mean observation: -0.117 [-0.512, 0.150], loss: 0.000035, mean_absolute_error: 0.468317, mean_q: 0.687096\n",
      " 39470/50000: episode: 797, duration: 0.325s, episode steps: 53, steps per second: 163, episode reward: 0.783, mean reward: 0.015 [-0.007, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.140 [-0.220, 0.748], loss: 0.000036, mean_absolute_error: 0.462616, mean_q: 0.677497\n",
      " 39516/50000: episode: 798, duration: 0.257s, episode steps: 46, steps per second: 179, episode reward: 0.849, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.112 [-0.562, 0.170], loss: 0.000176, mean_absolute_error: 0.467954, mean_q: 0.688703\n",
      " 39551/50000: episode: 799, duration: 0.211s, episode steps: 35, steps per second: 166, episode reward: 0.909, mean reward: 0.026 [-0.004, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.086 [-0.412, 0.140], loss: 0.000098, mean_absolute_error: 0.468191, mean_q: 0.686432\n",
      " 39572/50000: episode: 800, duration: 0.158s, episode steps: 21, steps per second: 133, episode reward: 0.961, mean reward: 0.046 [-0.003, 1.000], mean action: 0.571 [0.000, 2.000], mean observation: 0.055 [-0.120, 0.264], loss: 0.000051, mean_absolute_error: 0.470845, mean_q: 0.688275\n",
      " 39628/50000: episode: 801, duration: 0.406s, episode steps: 56, steps per second: 138, episode reward: 0.747, mean reward: 0.013 [-0.008, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.164 [-0.210, 0.806], loss: 0.000212, mean_absolute_error: 0.467030, mean_q: 0.679115\n",
      " 39650/50000: episode: 802, duration: 0.113s, episode steps: 22, steps per second: 194, episode reward: 0.963, mean reward: 0.044 [-0.002, 1.000], mean action: 1.182 [0.000, 2.000], mean observation: -0.053 [-0.241, 0.100], loss: 0.000077, mean_absolute_error: 0.470744, mean_q: 0.683312\n",
      " 39710/50000: episode: 803, duration: 0.332s, episode steps: 60, steps per second: 181, episode reward: 0.702, mean reward: 0.012 [-0.009, 1.000], mean action: 0.867 [0.000, 2.000], mean observation: 0.183 [-0.230, 0.888], loss: 0.000471, mean_absolute_error: 0.461219, mean_q: 0.674960\n",
      " 39845/50000: episode: 804, duration: 0.847s, episode steps: 135, steps per second: 159, episode reward: 0.637, mean reward: 0.005 [-0.006, 1.000], mean action: 0.933 [0.000, 2.000], mean observation: 0.032 [-0.644, 0.361], loss: 0.000352, mean_absolute_error: 0.475474, mean_q: 0.700326\n",
      " 39905/50000: episode: 805, duration: 0.351s, episode steps: 60, steps per second: 171, episode reward: 0.662, mean reward: 0.011 [-0.010, 1.000], mean action: 0.850 [0.000, 2.000], mean observation: 0.206 [-0.250, 0.997], loss: 0.000329, mean_absolute_error: 0.473700, mean_q: 0.696918\n",
      " 39964/50000: episode: 806, duration: 0.412s, episode steps: 59, steps per second: 143, episode reward: 0.756, mean reward: 0.013 [-0.008, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.143 [-0.220, 0.786], loss: 0.000045, mean_absolute_error: 0.463587, mean_q: 0.684925\n",
      " 39969/50000: episode: 807, duration: 0.029s, episode steps: 5, steps per second: 171, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.037 [-0.109, 0.050], loss: 0.000019, mean_absolute_error: 0.468877, mean_q: 0.697491\n",
      " 40031/50000: episode: 808, duration: 0.336s, episode steps: 62, steps per second: 185, episode reward: 0.745, mean reward: 0.012 [-0.008, 1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.148 [-0.773, 0.190], loss: 0.000137, mean_absolute_error: 0.467792, mean_q: 0.685570\n",
      " 40048/50000: episode: 809, duration: 0.118s, episode steps: 17, steps per second: 144, episode reward: 0.984, mean reward: 0.058 [-0.001, 1.000], mean action: 0.471 [0.000, 2.000], mean observation: 0.010 [-0.110, 0.148], loss: 0.000034, mean_absolute_error: 0.476746, mean_q: 0.708603\n",
      " 40106/50000: episode: 810, duration: 0.384s, episode steps: 58, steps per second: 151, episode reward: 0.699, mean reward: 0.012 [-0.009, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.187 [-0.240, 0.927], loss: 0.000098, mean_absolute_error: 0.467935, mean_q: 0.686603\n",
      " 40173/50000: episode: 811, duration: 0.361s, episode steps: 67, steps per second: 186, episode reward: 0.640, mean reward: 0.010 [-0.010, 1.000], mean action: 1.134 [0.000, 2.000], mean observation: -0.204 [-0.973, 0.220], loss: 0.000036, mean_absolute_error: 0.471965, mean_q: 0.693160\n",
      " 40216/50000: episode: 812, duration: 0.250s, episode steps: 43, steps per second: 172, episode reward: 0.861, mean reward: 0.020 [-0.005, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.115 [-0.160, 0.493], loss: 0.000067, mean_absolute_error: 0.472906, mean_q: 0.694472\n",
      " 40243/50000: episode: 813, duration: 0.175s, episode steps: 27, steps per second: 155, episode reward: 0.963, mean reward: 0.036 [-0.002, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.024 [-0.120, 0.235], loss: 0.000224, mean_absolute_error: 0.471112, mean_q: 0.697157\n",
      " 40284/50000: episode: 814, duration: 0.218s, episode steps: 41, steps per second: 188, episode reward: 0.874, mean reward: 0.021 [-0.005, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.106 [-0.150, 0.494], loss: 0.000041, mean_absolute_error: 0.473382, mean_q: 0.696797\n",
      " 40353/50000: episode: 815, duration: 0.369s, episode steps: 69, steps per second: 187, episode reward: 0.642, mean reward: 0.009 [-0.010, 1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.197 [-0.954, 0.210], loss: 0.000158, mean_absolute_error: 0.474601, mean_q: 0.695899\n",
      " 40390/50000: episode: 816, duration: 0.220s, episode steps: 37, steps per second: 168, episode reward: 0.905, mean reward: 0.024 [-0.004, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.078 [-0.130, 0.426], loss: 0.000041, mean_absolute_error: 0.474266, mean_q: 0.700889\n",
      " 40435/50000: episode: 817, duration: 0.244s, episode steps: 45, steps per second: 185, episode reward: 0.847, mean reward: 0.019 [-0.006, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.121 [-0.150, 0.552], loss: 0.000036, mean_absolute_error: 0.472432, mean_q: 0.697499\n",
      " 40474/50000: episode: 818, duration: 0.210s, episode steps: 39, steps per second: 186, episode reward: 0.922, mean reward: 0.024 [-0.003, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.058 [-0.110, 0.349], loss: 0.000584, mean_absolute_error: 0.475310, mean_q: 0.695440\n",
      " 40494/50000: episode: 819, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 0.966, mean reward: 0.048 [-0.002, 1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.053 [-0.237, 0.100], loss: 0.000038, mean_absolute_error: 0.474214, mean_q: 0.704016\n",
      " 40495/50000: episode: 820, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.019 [0.010, 0.027], loss: 0.000066, mean_absolute_error: 0.531002, mean_q: 0.802884\n",
      " 40504/50000: episode: 821, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.090, 0.139], loss: 0.000064, mean_absolute_error: 0.484179, mean_q: 0.713242\n",
      " 40520/50000: episode: 822, duration: 0.091s, episode steps: 16, steps per second: 176, episode reward: 0.977, mean reward: 0.061 [-0.002, 1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.048 [-0.182, 0.090], loss: 0.000053, mean_absolute_error: 0.481152, mean_q: 0.714138\n",
      " 40584/50000: episode: 823, duration: 0.392s, episode steps: 64, steps per second: 163, episode reward: 0.694, mean reward: 0.011 [-0.009, 1.000], mean action: 1.109 [0.000, 2.000], mean observation: -0.180 [-0.864, 0.200], loss: 0.000133, mean_absolute_error: 0.477019, mean_q: 0.706202\n",
      " 40643/50000: episode: 824, duration: 0.316s, episode steps: 59, steps per second: 187, episode reward: 0.762, mean reward: 0.013 [-0.008, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.137 [-0.190, 0.781], loss: 0.000315, mean_absolute_error: 0.468954, mean_q: 0.690510\n",
      " 40699/50000: episode: 825, duration: 0.308s, episode steps: 56, steps per second: 182, episode reward: 0.758, mean reward: 0.014 [-0.008, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.153 [-0.220, 0.781], loss: 0.000037, mean_absolute_error: 0.470354, mean_q: 0.686497\n",
      " 40712/50000: episode: 826, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 0.985, mean reward: 0.076 [-0.001, 1.000], mean action: 1.462 [0.000, 2.000], mean observation: -0.048 [-0.138, 0.070], loss: 0.000039, mean_absolute_error: 0.470789, mean_q: 0.693707\n",
      " 40776/50000: episode: 827, duration: 0.353s, episode steps: 64, steps per second: 181, episode reward: 0.704, mean reward: 0.011 [-0.008, 1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.175 [-0.824, 0.190], loss: 0.000033, mean_absolute_error: 0.477046, mean_q: 0.703352\n",
      " 40777/50000: episode: 828, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.018 [0.010, 0.027], loss: 0.000015, mean_absolute_error: 0.453174, mean_q: 0.607998\n",
      " 40807/50000: episode: 829, duration: 0.165s, episode steps: 30, steps per second: 182, episode reward: 0.928, mean reward: 0.031 [-0.004, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.077 [-0.360, 0.120], loss: 0.000025, mean_absolute_error: 0.471162, mean_q: 0.693163\n",
      " 40866/50000: episode: 830, duration: 0.315s, episode steps: 59, steps per second: 187, episode reward: 0.760, mean reward: 0.013 [-0.007, 1.000], mean action: 1.119 [0.000, 2.000], mean observation: -0.149 [-0.748, 0.190], loss: 0.000060, mean_absolute_error: 0.473844, mean_q: 0.695476\n",
      " 40893/50000: episode: 831, duration: 0.147s, episode steps: 27, steps per second: 184, episode reward: 0.960, mean reward: 0.036 [-0.002, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.033 [-0.110, 0.243], loss: 0.000140, mean_absolute_error: 0.468956, mean_q: 0.684619\n",
      " 40894/50000: episode: 832, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.031 [-0.072, 0.010], loss: 0.000022, mean_absolute_error: 0.455765, mean_q: 0.691408\n",
      " 40946/50000: episode: 833, duration: 0.281s, episode steps: 52, steps per second: 185, episode reward: 0.823, mean reward: 0.016 [-0.006, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.121 [-0.130, 0.582], loss: 0.000118, mean_absolute_error: 0.474639, mean_q: 0.702566\n",
      " 40986/50000: episode: 834, duration: 0.217s, episode steps: 40, steps per second: 184, episode reward: 0.897, mean reward: 0.022 [-0.004, 1.000], mean action: 0.775 [0.000, 2.000], mean observation: 0.084 [-0.140, 0.419], loss: 0.000157, mean_absolute_error: 0.468828, mean_q: 0.685632\n",
      " 41017/50000: episode: 835, duration: 0.171s, episode steps: 31, steps per second: 182, episode reward: 0.929, mean reward: 0.030 [-0.004, 1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.067 [-0.361, 0.140], loss: 0.000496, mean_absolute_error: 0.469635, mean_q: 0.688139\n",
      " 41174/50000: episode: 836, duration: 0.833s, episode steps: 157, steps per second: 189, episode reward: 0.270, mean reward: 0.002 [-0.010, 1.000], mean action: 0.949 [0.000, 2.000], mean observation: 0.229 [-0.240, 0.957], loss: 0.000145, mean_absolute_error: 0.471776, mean_q: 0.695604\n",
      " 41236/50000: episode: 837, duration: 0.339s, episode steps: 62, steps per second: 183, episode reward: 0.693, mean reward: 0.011 [-0.008, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.185 [-0.230, 0.831], loss: 0.000230, mean_absolute_error: 0.470746, mean_q: 0.690455\n",
      " 41286/50000: episode: 838, duration: 0.274s, episode steps: 50, steps per second: 182, episode reward: 0.795, mean reward: 0.016 [-0.007, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.142 [-0.210, 0.721], loss: 0.000345, mean_absolute_error: 0.479517, mean_q: 0.708919\n",
      " 41287/50000: episode: 839, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.020 [-0.049, 0.010], loss: 0.000022, mean_absolute_error: 0.443836, mean_q: 0.668745\n",
      " 41307/50000: episode: 840, duration: 0.114s, episode steps: 20, steps per second: 175, episode reward: 0.971, mean reward: 0.049 [-0.002, 1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.052 [-0.190, 0.070], loss: 0.000045, mean_absolute_error: 0.471568, mean_q: 0.692532\n",
      " 41348/50000: episode: 841, duration: 0.220s, episode steps: 41, steps per second: 186, episode reward: 0.875, mean reward: 0.021 [-0.005, 1.000], mean action: 0.780 [0.000, 2.000], mean observation: 0.100 [-0.140, 0.502], loss: 0.000033, mean_absolute_error: 0.471266, mean_q: 0.690660\n",
      " 41391/50000: episode: 842, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 0.864, mean reward: 0.020 [-0.005, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.103 [-0.170, 0.544], loss: 0.000115, mean_absolute_error: 0.473193, mean_q: 0.697192\n",
      " 41428/50000: episode: 843, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 0.926, mean reward: 0.025 [-0.004, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.052 [-0.130, 0.363], loss: 0.000033, mean_absolute_error: 0.466593, mean_q: 0.683960\n",
      " 41429/50000: episode: 844, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.009 [-0.028, 0.010], loss: 0.000102, mean_absolute_error: 0.536540, mean_q: 0.808459\n",
      " 41467/50000: episode: 845, duration: 0.206s, episode steps: 38, steps per second: 185, episode reward: 0.892, mean reward: 0.023 [-0.005, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.094 [-0.140, 0.452], loss: 0.000076, mean_absolute_error: 0.473704, mean_q: 0.700132\n",
      " 41481/50000: episode: 846, duration: 0.078s, episode steps: 14, steps per second: 179, episode reward: 0.985, mean reward: 0.070 [-0.002, 1.000], mean action: 0.357 [0.000, 2.000], mean observation: 0.022 [-0.110, 0.150], loss: 0.000033, mean_absolute_error: 0.452663, mean_q: 0.657651\n",
      " 41518/50000: episode: 847, duration: 0.201s, episode steps: 37, steps per second: 184, episode reward: 0.895, mean reward: 0.024 [-0.004, 1.000], mean action: 1.216 [0.000, 2.000], mean observation: -0.097 [-0.441, 0.130], loss: 0.000042, mean_absolute_error: 0.474471, mean_q: 0.699764\n",
      " 41587/50000: episode: 848, duration: 0.411s, episode steps: 69, steps per second: 168, episode reward: 0.757, mean reward: 0.011 [-0.008, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.115 [-0.220, 0.766], loss: 0.000110, mean_absolute_error: 0.472913, mean_q: 0.692009\n",
      " 41601/50000: episode: 849, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 0.982, mean reward: 0.070 [-0.002, 1.000], mean action: 1.643 [0.000, 2.000], mean observation: -0.042 [-0.156, 0.100], loss: 0.000043, mean_absolute_error: 0.483201, mean_q: 0.716123\n",
      " 41670/50000: episode: 850, duration: 0.358s, episode steps: 69, steps per second: 193, episode reward: 0.633, mean reward: 0.009 [-0.010, 1.000], mean action: 1.101 [0.000, 2.000], mean observation: -0.203 [-0.977, 0.220], loss: 0.000037, mean_absolute_error: 0.474746, mean_q: 0.695320\n",
      " 41707/50000: episode: 851, duration: 0.258s, episode steps: 37, steps per second: 144, episode reward: 0.908, mean reward: 0.025 [-0.004, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.075 [-0.408, 0.130], loss: 0.000410, mean_absolute_error: 0.472487, mean_q: 0.701052\n",
      " 41768/50000: episode: 852, duration: 0.330s, episode steps: 61, steps per second: 185, episode reward: 0.720, mean reward: 0.012 [-0.009, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.160 [-0.868, 0.220], loss: 0.000358, mean_absolute_error: 0.470991, mean_q: 0.692372\n",
      " 41830/50000: episode: 853, duration: 0.335s, episode steps: 62, steps per second: 185, episode reward: 0.712, mean reward: 0.011 [-0.008, 1.000], mean action: 1.097 [0.000, 2.000], mean observation: -0.173 [-0.843, 0.200], loss: 0.000067, mean_absolute_error: 0.472786, mean_q: 0.690745\n",
      " 41865/50000: episode: 854, duration: 0.211s, episode steps: 35, steps per second: 166, episode reward: 0.921, mean reward: 0.026 [-0.004, 1.000], mean action: 0.743 [0.000, 2.000], mean observation: 0.065 [-0.120, 0.382], loss: 0.000038, mean_absolute_error: 0.480779, mean_q: 0.714552\n",
      " 41913/50000: episode: 855, duration: 0.290s, episode steps: 48, steps per second: 166, episode reward: 0.872, mean reward: 0.018 [-0.005, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.079 [-0.510, 0.150], loss: 0.000510, mean_absolute_error: 0.475794, mean_q: 0.701141\n",
      " 41939/50000: episode: 856, duration: 0.169s, episode steps: 26, steps per second: 154, episode reward: 0.946, mean reward: 0.036 [-0.003, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.065 [-0.120, 0.303], loss: 0.000069, mean_absolute_error: 0.465996, mean_q: 0.683156\n",
      " 42005/50000: episode: 857, duration: 0.362s, episode steps: 66, steps per second: 182, episode reward: 0.643, mean reward: 0.010 [-0.010, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.205 [-0.220, 0.971], loss: 0.000211, mean_absolute_error: 0.472490, mean_q: 0.696317\n",
      " 42055/50000: episode: 858, duration: 0.268s, episode steps: 50, steps per second: 187, episode reward: 0.835, mean reward: 0.017 [-0.006, 1.000], mean action: 0.820 [0.000, 2.000], mean observation: 0.113 [-0.160, 0.574], loss: 0.000039, mean_absolute_error: 0.478907, mean_q: 0.711971\n",
      " 42109/50000: episode: 859, duration: 0.299s, episode steps: 54, steps per second: 181, episode reward: 0.701, mean reward: 0.013 [-0.010, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.198 [-0.965, 0.280], loss: 0.000039, mean_absolute_error: 0.473360, mean_q: 0.694138\n",
      " 42112/50000: episode: 860, duration: 0.021s, episode steps: 3, steps per second: 141, episode reward: 0.998, mean reward: 0.333 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.041 [-0.103, 0.030], loss: 0.000052, mean_absolute_error: 0.488268, mean_q: 0.698746\n",
      " 42171/50000: episode: 861, duration: 0.320s, episode steps: 59, steps per second: 185, episode reward: 0.738, mean reward: 0.013 [-0.008, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.163 [-0.797, 0.190], loss: 0.000037, mean_absolute_error: 0.475675, mean_q: 0.698710\n",
      " 42182/50000: episode: 862, duration: 0.068s, episode steps: 11, steps per second: 161, episode reward: 0.987, mean reward: 0.090 [-0.001, 1.000], mean action: 0.182 [0.000, 2.000], mean observation: 0.041 [-0.090, 0.139], loss: 0.000216, mean_absolute_error: 0.460643, mean_q: 0.675811\n",
      " 42248/50000: episode: 863, duration: 0.424s, episode steps: 66, steps per second: 156, episode reward: 0.682, mean reward: 0.010 [-0.009, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.166 [-0.948, 0.240], loss: 0.000236, mean_absolute_error: 0.473175, mean_q: 0.698334\n",
      " 42304/50000: episode: 864, duration: 0.295s, episode steps: 56, steps per second: 190, episode reward: 0.768, mean reward: 0.014 [-0.007, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.150 [-0.744, 0.180], loss: 0.000040, mean_absolute_error: 0.477531, mean_q: 0.703205\n",
      " 42354/50000: episode: 865, duration: 0.258s, episode steps: 50, steps per second: 194, episode reward: 0.805, mean reward: 0.016 [-0.007, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.139 [-0.654, 0.170], loss: 0.000074, mean_absolute_error: 0.476946, mean_q: 0.696198\n",
      " 42380/50000: episode: 866, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 0.948, mean reward: 0.036 [-0.003, 1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.064 [-0.284, 0.100], loss: 0.000030, mean_absolute_error: 0.469084, mean_q: 0.690350\n",
      " 42443/50000: episode: 867, duration: 0.459s, episode steps: 63, steps per second: 137, episode reward: 0.666, mean reward: 0.011 [-0.010, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.195 [-0.989, 0.260], loss: 0.000234, mean_absolute_error: 0.481730, mean_q: 0.713433\n",
      " 42471/50000: episode: 868, duration: 0.172s, episode steps: 28, steps per second: 163, episode reward: 0.969, mean reward: 0.035 [-0.002, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.013 [-0.110, 0.212], loss: 0.000066, mean_absolute_error: 0.480812, mean_q: 0.707713\n",
      " 42490/50000: episode: 869, duration: 0.116s, episode steps: 19, steps per second: 164, episode reward: 0.978, mean reward: 0.051 [-0.002, 1.000], mean action: 0.526 [0.000, 2.000], mean observation: 0.021 [-0.110, 0.181], loss: 0.000046, mean_absolute_error: 0.475749, mean_q: 0.704942\n",
      " 42533/50000: episode: 870, duration: 0.314s, episode steps: 43, steps per second: 137, episode reward: 0.883, mean reward: 0.021 [-0.004, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.097 [-0.140, 0.420], loss: 0.000431, mean_absolute_error: 0.474817, mean_q: 0.699602\n",
      " 42611/50000: episode: 871, duration: 0.542s, episode steps: 78, steps per second: 144, episode reward: 0.815, mean reward: 0.010 [-0.006, 1.000], mean action: 0.897 [0.000, 2.000], mean observation: -0.032 [-0.599, 0.190], loss: 0.000249, mean_absolute_error: 0.477521, mean_q: 0.704848\n",
      " 42662/50000: episode: 872, duration: 0.448s, episode steps: 51, steps per second: 114, episode reward: 0.741, mean reward: 0.015 [-0.009, 1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.178 [-0.876, 0.250], loss: 0.000133, mean_absolute_error: 0.464161, mean_q: 0.684388\n",
      " 42717/50000: episode: 873, duration: 0.293s, episode steps: 55, steps per second: 187, episode reward: 0.780, mean reward: 0.014 [-0.007, 1.000], mean action: 0.836 [0.000, 2.000], mean observation: 0.140 [-0.170, 0.721], loss: 0.000052, mean_absolute_error: 0.475610, mean_q: 0.701774\n",
      " 42744/50000: episode: 874, duration: 0.157s, episode steps: 27, steps per second: 172, episode reward: 0.951, mean reward: 0.035 [-0.003, 1.000], mean action: 1.222 [0.000, 2.000], mean observation: -0.060 [-0.267, 0.090], loss: 0.000039, mean_absolute_error: 0.477617, mean_q: 0.701618\n",
      " 42789/50000: episode: 875, duration: 0.329s, episode steps: 45, steps per second: 137, episode reward: 0.841, mean reward: 0.019 [-0.006, 1.000], mean action: 1.156 [0.000, 2.000], mean observation: -0.122 [-0.599, 0.180], loss: 0.000037, mean_absolute_error: 0.472732, mean_q: 0.699842\n",
      " 42790/50000: episode: 876, duration: 0.017s, episode steps: 1, steps per second: 59, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.024 [-0.010, 0.059], loss: 0.000097, mean_absolute_error: 0.510401, mean_q: 0.768777\n",
      " 42827/50000: episode: 877, duration: 0.218s, episode steps: 37, steps per second: 169, episode reward: 0.896, mean reward: 0.024 [-0.004, 1.000], mean action: 1.162 [0.000, 2.000], mean observation: -0.096 [-0.434, 0.140], loss: 0.000085, mean_absolute_error: 0.481097, mean_q: 0.712493\n",
      " 42889/50000: episode: 878, duration: 0.370s, episode steps: 62, steps per second: 167, episode reward: 0.724, mean reward: 0.012 [-0.008, 1.000], mean action: 1.097 [0.000, 2.000], mean observation: -0.165 [-0.823, 0.200], loss: 0.000041, mean_absolute_error: 0.474649, mean_q: 0.704270\n",
      " 42898/50000: episode: 879, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 0.991, mean reward: 0.110 [-0.001, 1.000], mean action: 0.222 [0.000, 2.000], mean observation: 0.042 [-0.070, 0.125], loss: 0.000042, mean_absolute_error: 0.466900, mean_q: 0.691961\n",
      " 42953/50000: episode: 880, duration: 0.325s, episode steps: 55, steps per second: 169, episode reward: 0.787, mean reward: 0.014 [-0.007, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.139 [-0.703, 0.190], loss: 0.000050, mean_absolute_error: 0.472388, mean_q: 0.694243\n",
      " 42978/50000: episode: 881, duration: 0.146s, episode steps: 25, steps per second: 171, episode reward: 0.952, mean reward: 0.038 [-0.003, 1.000], mean action: 1.320 [0.000, 2.000], mean observation: -0.062 [-0.276, 0.100], loss: 0.000039, mean_absolute_error: 0.482858, mean_q: 0.716177\n",
      " 43041/50000: episode: 882, duration: 0.389s, episode steps: 63, steps per second: 162, episode reward: 0.763, mean reward: 0.012 [-0.007, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.134 [-0.180, 0.705], loss: 0.000049, mean_absolute_error: 0.475910, mean_q: 0.703384\n",
      " 43100/50000: episode: 883, duration: 0.345s, episode steps: 59, steps per second: 171, episode reward: 0.730, mean reward: 0.012 [-0.008, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.166 [-0.190, 0.816], loss: 0.000089, mean_absolute_error: 0.476988, mean_q: 0.705249\n",
      " 43151/50000: episode: 884, duration: 0.331s, episode steps: 51, steps per second: 154, episode reward: 0.792, mean reward: 0.016 [-0.007, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.151 [-0.654, 0.170], loss: 0.000040, mean_absolute_error: 0.472627, mean_q: 0.694433\n",
      " 43209/50000: episode: 885, duration: 0.334s, episode steps: 58, steps per second: 174, episode reward: 0.672, mean reward: 0.012 [-0.010, 1.000], mean action: 1.121 [0.000, 2.000], mean observation: -0.206 [-0.992, 0.260], loss: 0.000036, mean_absolute_error: 0.475300, mean_q: 0.700653\n",
      " 43236/50000: episode: 886, duration: 0.171s, episode steps: 27, steps per second: 158, episode reward: 0.958, mean reward: 0.035 [-0.003, 1.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.030 [-0.150, 0.270], loss: 0.000123, mean_absolute_error: 0.479995, mean_q: 0.706483\n",
      " 43253/50000: episode: 887, duration: 0.095s, episode steps: 17, steps per second: 179, episode reward: 0.982, mean reward: 0.058 [-0.002, 1.000], mean action: 0.471 [0.000, 2.000], mean observation: 0.014 [-0.120, 0.159], loss: 0.000802, mean_absolute_error: 0.490907, mean_q: 0.731543\n",
      " 43260/50000: episode: 888, duration: 0.042s, episode steps: 7, steps per second: 167, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.122, 0.070], loss: 0.000073, mean_absolute_error: 0.487739, mean_q: 0.729570\n",
      " 43306/50000: episode: 889, duration: 0.258s, episode steps: 46, steps per second: 179, episode reward: 0.848, mean reward: 0.018 [-0.006, 1.000], mean action: 1.174 [0.000, 2.000], mean observation: -0.113 [-0.572, 0.190], loss: 0.000038, mean_absolute_error: 0.474687, mean_q: 0.702315\n",
      " 43368/50000: episode: 890, duration: 0.351s, episode steps: 62, steps per second: 176, episode reward: 0.779, mean reward: 0.013 [-0.007, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.123 [-0.190, 0.682], loss: 0.000065, mean_absolute_error: 0.479891, mean_q: 0.713066\n",
      " 43419/50000: episode: 891, duration: 0.276s, episode steps: 51, steps per second: 185, episode reward: 0.734, mean reward: 0.014 [-0.009, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.180 [-0.897, 0.270], loss: 0.000042, mean_absolute_error: 0.481802, mean_q: 0.715905\n",
      " 43463/50000: episode: 892, duration: 0.240s, episode steps: 44, steps per second: 183, episode reward: 0.871, mean reward: 0.020 [-0.005, 1.000], mean action: 1.159 [0.000, 2.000], mean observation: -0.103 [-0.489, 0.130], loss: 0.000074, mean_absolute_error: 0.476018, mean_q: 0.704589\n",
      " 43494/50000: episode: 893, duration: 0.169s, episode steps: 31, steps per second: 184, episode reward: 0.921, mean reward: 0.030 [-0.004, 1.000], mean action: 1.258 [0.000, 2.000], mean observation: -0.082 [-0.381, 0.140], loss: 0.000045, mean_absolute_error: 0.483134, mean_q: 0.719219\n",
      " 43523/50000: episode: 894, duration: 0.227s, episode steps: 29, steps per second: 128, episode reward: 0.945, mean reward: 0.033 [-0.003, 1.000], mean action: 1.069 [0.000, 2.000], mean observation: -0.062 [-0.299, 0.130], loss: 0.000458, mean_absolute_error: 0.480558, mean_q: 0.710156\n",
      " 43581/50000: episode: 895, duration: 0.315s, episode steps: 58, steps per second: 184, episode reward: 0.773, mean reward: 0.013 [-0.007, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.138 [-0.170, 0.716], loss: 0.000043, mean_absolute_error: 0.478056, mean_q: 0.710415\n",
      " 43635/50000: episode: 896, duration: 0.290s, episode steps: 54, steps per second: 186, episode reward: 0.816, mean reward: 0.015 [-0.006, 1.000], mean action: 1.093 [0.000, 2.000], mean observation: -0.123 [-0.620, 0.170], loss: 0.000278, mean_absolute_error: 0.477185, mean_q: 0.706072\n",
      " 43667/50000: episode: 897, duration: 0.171s, episode steps: 32, steps per second: 187, episode reward: 0.925, mean reward: 0.029 [-0.004, 1.000], mean action: 1.219 [0.000, 2.000], mean observation: -0.077 [-0.362, 0.120], loss: 0.000040, mean_absolute_error: 0.475724, mean_q: 0.700976\n",
      " 43723/50000: episode: 898, duration: 0.370s, episode steps: 56, steps per second: 152, episode reward: 0.813, mean reward: 0.015 [-0.006, 1.000], mean action: 0.839 [0.000, 2.000], mean observation: 0.112 [-0.160, 0.624], loss: 0.000050, mean_absolute_error: 0.475572, mean_q: 0.711129\n",
      " 43724/50000: episode: 899, duration: 0.011s, episode steps: 1, steps per second: 95, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.032 [-0.010, 0.075], loss: 0.000033, mean_absolute_error: 0.453293, mean_q: 0.667607\n",
      " 43750/50000: episode: 900, duration: 0.142s, episode steps: 26, steps per second: 183, episode reward: 0.946, mean reward: 0.036 [-0.003, 1.000], mean action: 1.269 [0.000, 2.000], mean observation: -0.065 [-0.310, 0.120], loss: 0.000035, mean_absolute_error: 0.481640, mean_q: 0.716611\n",
      " 43802/50000: episode: 901, duration: 0.282s, episode steps: 52, steps per second: 184, episode reward: 0.805, mean reward: 0.015 [-0.006, 1.000], mean action: 0.827 [0.000, 2.000], mean observation: 0.135 [-0.160, 0.650], loss: 0.000041, mean_absolute_error: 0.479371, mean_q: 0.714015\n",
      " 43808/50000: episode: 902, duration: 0.038s, episode steps: 6, steps per second: 156, episode reward: 0.994, mean reward: 0.166 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.036 [-0.060, 0.116], loss: 0.000032, mean_absolute_error: 0.474855, mean_q: 0.709606\n",
      " 43836/50000: episode: 903, duration: 0.178s, episode steps: 28, steps per second: 158, episode reward: 0.935, mean reward: 0.033 [-0.003, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.072 [-0.130, 0.346], loss: 0.000058, mean_absolute_error: 0.473207, mean_q: 0.703821\n",
      " 43874/50000: episode: 904, duration: 0.230s, episode steps: 38, steps per second: 165, episode reward: 0.940, mean reward: 0.025 [-0.003, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.030 [-0.140, 0.288], loss: 0.000342, mean_absolute_error: 0.471425, mean_q: 0.696435\n",
      " 43919/50000: episode: 905, duration: 0.323s, episode steps: 45, steps per second: 139, episode reward: 0.852, mean reward: 0.019 [-0.006, 1.000], mean action: 1.178 [0.000, 2.000], mean observation: -0.115 [-0.550, 0.150], loss: 0.000093, mean_absolute_error: 0.479550, mean_q: 0.710556\n",
      " 43951/50000: episode: 906, duration: 0.197s, episode steps: 32, steps per second: 163, episode reward: 0.921, mean reward: 0.029 [-0.004, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.083 [-0.120, 0.368], loss: 0.000056, mean_absolute_error: 0.471574, mean_q: 0.692708\n",
      " 43991/50000: episode: 907, duration: 0.262s, episode steps: 40, steps per second: 152, episode reward: 0.880, mean reward: 0.022 [-0.005, 1.000], mean action: 1.175 [0.000, 2.000], mean observation: -0.105 [-0.477, 0.180], loss: 0.000066, mean_absolute_error: 0.472920, mean_q: 0.703244\n",
      " 44036/50000: episode: 908, duration: 0.248s, episode steps: 45, steps per second: 182, episode reward: 0.820, mean reward: 0.018 [-0.007, 1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.123 [-0.707, 0.260], loss: 0.000397, mean_absolute_error: 0.480884, mean_q: 0.711295\n",
      " 44105/50000: episode: 909, duration: 0.403s, episode steps: 69, steps per second: 171, episode reward: 0.685, mean reward: 0.010 [-0.009, 1.000], mean action: 0.870 [0.000, 2.000], mean observation: 0.167 [-0.230, 0.886], loss: 0.000073, mean_absolute_error: 0.472245, mean_q: 0.701982\n",
      " 44150/50000: episode: 910, duration: 0.300s, episode steps: 45, steps per second: 150, episode reward: 0.859, mean reward: 0.019 [-0.005, 1.000], mean action: 1.156 [0.000, 2.000], mean observation: -0.109 [-0.535, 0.170], loss: 0.000077, mean_absolute_error: 0.470111, mean_q: 0.698114\n",
      " 44161/50000: episode: 911, duration: 0.070s, episode steps: 11, steps per second: 157, episode reward: 0.988, mean reward: 0.090 [-0.001, 1.000], mean action: 1.455 [0.000, 2.000], mean observation: -0.040 [-0.142, 0.060], loss: 0.000042, mean_absolute_error: 0.470241, mean_q: 0.697075\n",
      " 44176/50000: episode: 912, duration: 0.087s, episode steps: 15, steps per second: 172, episode reward: 0.980, mean reward: 0.065 [-0.002, 1.000], mean action: 1.600 [0.000, 2.000], mean observation: -0.037 [-0.183, 0.100], loss: 0.000044, mean_absolute_error: 0.471664, mean_q: 0.701726\n",
      " 44203/50000: episode: 913, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 0.946, mean reward: 0.035 [-0.003, 1.000], mean action: 1.259 [0.000, 2.000], mean observation: -0.065 [-0.294, 0.100], loss: 0.000068, mean_absolute_error: 0.479899, mean_q: 0.716830\n",
      " 44204/50000: episode: 914, duration: 0.010s, episode steps: 1, steps per second: 100, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.010 [-0.010, -0.010], loss: 0.000071, mean_absolute_error: 0.467597, mean_q: 0.696408\n",
      " 44243/50000: episode: 915, duration: 0.237s, episode steps: 39, steps per second: 164, episode reward: 0.887, mean reward: 0.023 [-0.005, 1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.100 [-0.464, 0.140], loss: 0.000056, mean_absolute_error: 0.470555, mean_q: 0.700774\n",
      " 44298/50000: episode: 916, duration: 0.422s, episode steps: 55, steps per second: 130, episode reward: 0.701, mean reward: 0.013 [-0.009, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.195 [-0.943, 0.260], loss: 0.000043, mean_absolute_error: 0.475966, mean_q: 0.710159\n",
      " 44309/50000: episode: 917, duration: 0.075s, episode steps: 11, steps per second: 146, episode reward: 0.987, mean reward: 0.090 [-0.002, 1.000], mean action: 1.455 [0.000, 2.000], mean observation: -0.040 [-0.153, 0.080], loss: 0.000048, mean_absolute_error: 0.482393, mean_q: 0.721688\n",
      " 44326/50000: episode: 918, duration: 0.150s, episode steps: 17, steps per second: 113, episode reward: 0.974, mean reward: 0.057 [-0.002, 1.000], mean action: 1.529 [0.000, 2.000], mean observation: -0.047 [-0.197, 0.110], loss: 0.000042, mean_absolute_error: 0.481651, mean_q: 0.716713\n",
      " 44327/50000: episode: 919, duration: 0.012s, episode steps: 1, steps per second: 87, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.082, 0.010], loss: 0.000020, mean_absolute_error: 0.458964, mean_q: 0.693113\n",
      " 44360/50000: episode: 920, duration: 0.229s, episode steps: 33, steps per second: 144, episode reward: 0.921, mean reward: 0.028 [-0.004, 1.000], mean action: 0.848 [0.000, 2.000], mean observation: 0.081 [-0.140, 0.365], loss: 0.000292, mean_absolute_error: 0.470540, mean_q: 0.701745\n",
      " 44373/50000: episode: 921, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 0.983, mean reward: 0.076 [-0.002, 1.000], mean action: 1.538 [1.000, 2.000], mean observation: -0.043 [-0.169, 0.070], loss: 0.000725, mean_absolute_error: 0.480514, mean_q: 0.715453\n",
      " 44434/50000: episode: 922, duration: 0.448s, episode steps: 61, steps per second: 136, episode reward: 0.764, mean reward: 0.013 [-0.008, 1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.121 [-0.769, 0.240], loss: 0.000358, mean_absolute_error: 0.471943, mean_q: 0.700212\n",
      " 44478/50000: episode: 923, duration: 0.583s, episode steps: 44, steps per second: 75, episode reward: 0.873, mean reward: 0.020 [-0.005, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.095 [-0.140, 0.491], loss: 0.000074, mean_absolute_error: 0.476053, mean_q: 0.709013\n",
      " 44506/50000: episode: 924, duration: 0.394s, episode steps: 28, steps per second: 71, episode reward: 0.938, mean reward: 0.033 [-0.003, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.065 [-0.140, 0.342], loss: 0.000071, mean_absolute_error: 0.478333, mean_q: 0.710030\n",
      " 44549/50000: episode: 925, duration: 0.382s, episode steps: 43, steps per second: 113, episode reward: 0.855, mean reward: 0.020 [-0.006, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.117 [-0.160, 0.551], loss: 0.000043, mean_absolute_error: 0.477096, mean_q: 0.712795\n",
      " 44558/50000: episode: 926, duration: 0.055s, episode steps: 9, steps per second: 164, episode reward: 0.990, mean reward: 0.110 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.035 [-0.090, 0.137], loss: 0.000029, mean_absolute_error: 0.473196, mean_q: 0.704971\n",
      " 44566/50000: episode: 927, duration: 0.051s, episode steps: 8, steps per second: 156, episode reward: 0.991, mean reward: 0.124 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [-0.080, 0.133], loss: 0.000035, mean_absolute_error: 0.460984, mean_q: 0.687169\n",
      " 44620/50000: episode: 928, duration: 0.360s, episode steps: 54, steps per second: 150, episode reward: 0.774, mean reward: 0.014 [-0.008, 1.000], mean action: 1.056 [0.000, 2.000], mean observation: -0.148 [-0.769, 0.240], loss: 0.000051, mean_absolute_error: 0.481590, mean_q: 0.716478\n",
      " 44649/50000: episode: 929, duration: 0.157s, episode steps: 29, steps per second: 185, episode reward: 0.948, mean reward: 0.033 [-0.002, 1.000], mean action: 1.207 [0.000, 2.000], mean observation: -0.068 [-0.236, 0.090], loss: 0.000446, mean_absolute_error: 0.476817, mean_q: 0.711039\n",
      " 44702/50000: episode: 930, duration: 0.279s, episode steps: 53, steps per second: 190, episode reward: 0.794, mean reward: 0.015 [-0.007, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.141 [-0.170, 0.671], loss: 0.000111, mean_absolute_error: 0.479311, mean_q: 0.713758\n",
      " 44745/50000: episode: 931, duration: 0.250s, episode steps: 43, steps per second: 172, episode reward: 0.880, mean reward: 0.020 [-0.005, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.087 [-0.150, 0.487], loss: 0.000106, mean_absolute_error: 0.474966, mean_q: 0.706255\n",
      " 44757/50000: episode: 932, duration: 0.073s, episode steps: 12, steps per second: 163, episode reward: 0.985, mean reward: 0.082 [-0.002, 1.000], mean action: 1.667 [0.000, 2.000], mean observation: -0.039 [-0.157, 0.080], loss: 0.000059, mean_absolute_error: 0.481414, mean_q: 0.724352\n",
      " 44808/50000: episode: 933, duration: 0.336s, episode steps: 51, steps per second: 152, episode reward: 0.750, mean reward: 0.015 [-0.009, 1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.165 [-0.861, 0.260], loss: 0.000254, mean_absolute_error: 0.472441, mean_q: 0.700563\n",
      " 44861/50000: episode: 934, duration: 0.304s, episode steps: 53, steps per second: 174, episode reward: 0.703, mean reward: 0.013 [-0.009, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.201 [-0.938, 0.260], loss: 0.000060, mean_absolute_error: 0.468488, mean_q: 0.695388\n",
      " 44912/50000: episode: 935, duration: 0.277s, episode steps: 51, steps per second: 184, episode reward: 0.818, mean reward: 0.016 [-0.006, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.126 [-0.180, 0.608], loss: 0.000048, mean_absolute_error: 0.473673, mean_q: 0.700888\n",
      " 44967/50000: episode: 936, duration: 0.301s, episode steps: 55, steps per second: 183, episode reward: 0.767, mean reward: 0.014 [-0.007, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.153 [-0.746, 0.210], loss: 0.000047, mean_absolute_error: 0.475020, mean_q: 0.705841\n",
      " 44977/50000: episode: 937, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 0.989, mean reward: 0.099 [-0.001, 1.000], mean action: 0.300 [0.000, 2.000], mean observation: 0.043 [-0.070, 0.125], loss: 0.000065, mean_absolute_error: 0.468780, mean_q: 0.699110\n",
      " 45011/50000: episode: 938, duration: 0.246s, episode steps: 34, steps per second: 138, episode reward: 0.917, mean reward: 0.027 [-0.004, 1.000], mean action: 0.735 [0.000, 2.000], mean observation: 0.074 [-0.140, 0.395], loss: 0.000052, mean_absolute_error: 0.475998, mean_q: 0.705538\n",
      " 45056/50000: episode: 939, duration: 0.263s, episode steps: 45, steps per second: 171, episode reward: 0.861, mean reward: 0.019 [-0.005, 1.000], mean action: 1.111 [0.000, 2.000], mean observation: -0.108 [-0.531, 0.150], loss: 0.000080, mean_absolute_error: 0.476438, mean_q: 0.705866\n",
      " 45112/50000: episode: 940, duration: 0.325s, episode steps: 56, steps per second: 172, episode reward: 0.835, mean reward: 0.015 [-0.006, 1.000], mean action: 1.054 [0.000, 2.000], mean observation: -0.107 [-0.557, 0.150], loss: 0.000050, mean_absolute_error: 0.474461, mean_q: 0.702718\n",
      " 45113/50000: episode: 941, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.006 [-0.023, 0.010], loss: 0.000057, mean_absolute_error: 0.487823, mean_q: 0.735552\n",
      " 45152/50000: episode: 942, duration: 0.290s, episode steps: 39, steps per second: 135, episode reward: 0.889, mean reward: 0.023 [-0.005, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.097 [-0.140, 0.459], loss: 0.000294, mean_absolute_error: 0.473275, mean_q: 0.702756\n",
      " 45188/50000: episode: 943, duration: 0.306s, episode steps: 36, steps per second: 118, episode reward: 0.927, mean reward: 0.026 [-0.004, 1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.034 [-0.382, 0.210], loss: 0.000308, mean_absolute_error: 0.477795, mean_q: 0.709866\n",
      " 45203/50000: episode: 944, duration: 0.110s, episode steps: 15, steps per second: 137, episode reward: 0.979, mean reward: 0.065 [-0.002, 1.000], mean action: 1.600 [0.000, 2.000], mean observation: -0.041 [-0.183, 0.090], loss: 0.000067, mean_absolute_error: 0.469202, mean_q: 0.692910\n",
      " 45258/50000: episode: 945, duration: 0.436s, episode steps: 55, steps per second: 126, episode reward: 0.767, mean reward: 0.014 [-0.008, 1.000], mean action: 1.073 [0.000, 2.000], mean observation: -0.152 [-0.774, 0.230], loss: 0.000050, mean_absolute_error: 0.477410, mean_q: 0.710288\n",
      " 45299/50000: episode: 946, duration: 0.227s, episode steps: 41, steps per second: 180, episode reward: 0.881, mean reward: 0.021 [-0.005, 1.000], mean action: 0.805 [0.000, 2.000], mean observation: 0.101 [-0.120, 0.471], loss: 0.000053, mean_absolute_error: 0.467728, mean_q: 0.690090\n",
      " 45350/50000: episode: 947, duration: 0.272s, episode steps: 51, steps per second: 188, episode reward: 0.801, mean reward: 0.016 [-0.007, 1.000], mean action: 1.157 [0.000, 2.000], mean observation: -0.136 [-0.704, 0.230], loss: 0.000111, mean_absolute_error: 0.470422, mean_q: 0.697380\n",
      " 45397/50000: episode: 948, duration: 0.261s, episode steps: 47, steps per second: 180, episode reward: 0.849, mean reward: 0.018 [-0.006, 1.000], mean action: 1.085 [0.000, 2.000], mean observation: -0.112 [-0.562, 0.160], loss: 0.000091, mean_absolute_error: 0.480450, mean_q: 0.711434\n",
      " 45457/50000: episode: 949, duration: 0.323s, episode steps: 60, steps per second: 186, episode reward: 0.757, mean reward: 0.013 [-0.007, 1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.151 [-0.718, 0.210], loss: 0.000052, mean_absolute_error: 0.474102, mean_q: 0.705748\n",
      " 45507/50000: episode: 950, duration: 0.281s, episode steps: 50, steps per second: 178, episode reward: 0.832, mean reward: 0.017 [-0.006, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.120 [-0.568, 0.140], loss: 0.000236, mean_absolute_error: 0.479641, mean_q: 0.714220\n",
      " 45566/50000: episode: 951, duration: 0.324s, episode steps: 59, steps per second: 182, episode reward: 0.738, mean reward: 0.013 [-0.008, 1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.149 [-0.839, 0.260], loss: 0.000214, mean_absolute_error: 0.477994, mean_q: 0.711859\n",
      " 45573/50000: episode: 952, duration: 0.049s, episode steps: 7, steps per second: 142, episode reward: 0.993, mean reward: 0.142 [-0.001, 1.000], mean action: 0.286 [0.000, 2.000], mean observation: 0.038 [-0.060, 0.123], loss: 0.000042, mean_absolute_error: 0.484715, mean_q: 0.728536\n",
      " 45611/50000: episode: 953, duration: 0.343s, episode steps: 38, steps per second: 111, episode reward: 0.900, mean reward: 0.024 [-0.004, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.089 [-0.170, 0.392], loss: 0.000095, mean_absolute_error: 0.468184, mean_q: 0.692554\n",
      " 45649/50000: episode: 954, duration: 0.274s, episode steps: 38, steps per second: 139, episode reward: 0.899, mean reward: 0.024 [-0.004, 1.000], mean action: 0.763 [0.000, 2.000], mean observation: 0.099 [-0.150, 0.364], loss: 0.000331, mean_absolute_error: 0.467379, mean_q: 0.693175\n",
      " 45705/50000: episode: 955, duration: 0.324s, episode steps: 56, steps per second: 173, episode reward: 0.696, mean reward: 0.012 [-0.010, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.186 [-0.975, 0.290], loss: 0.000110, mean_absolute_error: 0.465090, mean_q: 0.688226\n",
      " 45715/50000: episode: 956, duration: 0.079s, episode steps: 10, steps per second: 127, episode reward: 0.990, mean reward: 0.099 [-0.001, 1.000], mean action: 0.300 [0.000, 2.000], mean observation: 0.040 [-0.070, 0.125], loss: 0.000048, mean_absolute_error: 0.470335, mean_q: 0.702933\n",
      " 45762/50000: episode: 957, duration: 0.277s, episode steps: 47, steps per second: 170, episode reward: 0.838, mean reward: 0.018 [-0.006, 1.000], mean action: 1.128 [0.000, 2.000], mean observation: -0.120 [-0.592, 0.150], loss: 0.000255, mean_absolute_error: 0.470378, mean_q: 0.698066\n",
      " 45782/50000: episode: 958, duration: 0.108s, episode steps: 20, steps per second: 185, episode reward: 0.969, mean reward: 0.048 [-0.002, 1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.038 [-0.230, 0.120], loss: 0.000483, mean_absolute_error: 0.473208, mean_q: 0.701727\n",
      " 45829/50000: episode: 959, duration: 0.259s, episode steps: 47, steps per second: 181, episode reward: 0.840, mean reward: 0.018 [-0.006, 1.000], mean action: 0.809 [0.000, 2.000], mean observation: 0.121 [-0.150, 0.570], loss: 0.000064, mean_absolute_error: 0.471582, mean_q: 0.700332\n",
      " 45855/50000: episode: 960, duration: 0.134s, episode steps: 26, steps per second: 195, episode reward: 0.950, mean reward: 0.037 [-0.003, 1.000], mean action: 0.654 [0.000, 2.000], mean observation: 0.060 [-0.110, 0.272], loss: 0.000053, mean_absolute_error: 0.478778, mean_q: 0.712877\n",
      " 45918/50000: episode: 961, duration: 0.336s, episode steps: 63, steps per second: 187, episode reward: 0.661, mean reward: 0.010 [-0.010, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.201 [-0.260, 0.958], loss: 0.000058, mean_absolute_error: 0.467693, mean_q: 0.695112\n",
      " 45930/50000: episode: 962, duration: 0.074s, episode steps: 12, steps per second: 163, episode reward: 0.984, mean reward: 0.082 [-0.002, 1.000], mean action: 0.250 [0.000, 2.000], mean observation: 0.040 [-0.100, 0.166], loss: 0.000058, mean_absolute_error: 0.477220, mean_q: 0.704429\n",
      " 45993/50000: episode: 963, duration: 0.343s, episode steps: 63, steps per second: 184, episode reward: 0.688, mean reward: 0.011 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.186 [-0.190, 0.879], loss: 0.000059, mean_absolute_error: 0.471590, mean_q: 0.701562\n",
      " 46015/50000: episode: 964, duration: 0.125s, episode steps: 22, steps per second: 176, episode reward: 0.958, mean reward: 0.044 [-0.003, 1.000], mean action: 0.591 [0.000, 2.000], mean observation: 0.056 [-0.130, 0.274], loss: 0.000043, mean_absolute_error: 0.474074, mean_q: 0.707430\n",
      " 46023/50000: episode: 965, duration: 0.035s, episode steps: 8, steps per second: 231, episode reward: 0.991, mean reward: 0.124 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.037 [-0.134, 0.080], loss: 0.000075, mean_absolute_error: 0.465054, mean_q: 0.685501\n",
      " 46079/50000: episode: 966, duration: 0.381s, episode steps: 56, steps per second: 147, episode reward: 0.732, mean reward: 0.013 [-0.009, 1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.150 [-0.920, 0.290], loss: 0.000048, mean_absolute_error: 0.465771, mean_q: 0.692565\n",
      " 46141/50000: episode: 967, duration: 0.362s, episode steps: 62, steps per second: 171, episode reward: 0.760, mean reward: 0.012 [-0.007, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.140 [-0.190, 0.712], loss: 0.000272, mean_absolute_error: 0.466491, mean_q: 0.692304\n",
      " 46142/50000: episode: 968, duration: 0.010s, episode steps: 1, steps per second: 102, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.021 [-0.010, 0.053], loss: 0.000082, mean_absolute_error: 0.473844, mean_q: 0.706109\n",
      " 46190/50000: episode: 969, duration: 0.254s, episode steps: 48, steps per second: 189, episode reward: 0.858, mean reward: 0.018 [-0.005, 1.000], mean action: 1.146 [0.000, 2.000], mean observation: -0.106 [-0.497, 0.140], loss: 0.000055, mean_absolute_error: 0.470212, mean_q: 0.701986\n",
      " 46191/50000: episode: 970, duration: 0.010s, episode steps: 1, steps per second: 95, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.039 [0.010, 0.068], loss: 0.000046, mean_absolute_error: 0.497043, mean_q: 0.741690\n",
      " 46234/50000: episode: 971, duration: 0.231s, episode steps: 43, steps per second: 186, episode reward: 0.871, mean reward: 0.020 [-0.005, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.103 [-0.140, 0.494], loss: 0.000335, mean_absolute_error: 0.471950, mean_q: 0.699550\n",
      " 46289/50000: episode: 972, duration: 0.292s, episode steps: 55, steps per second: 188, episode reward: 0.783, mean reward: 0.014 [-0.007, 1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.139 [-0.711, 0.200], loss: 0.000232, mean_absolute_error: 0.461906, mean_q: 0.683661\n",
      " 46350/50000: episode: 973, duration: 0.397s, episode steps: 61, steps per second: 154, episode reward: 0.736, mean reward: 0.012 [-0.008, 1.000], mean action: 1.131 [0.000, 2.000], mean observation: -0.160 [-0.787, 0.220], loss: 0.000048, mean_absolute_error: 0.471969, mean_q: 0.703118\n",
      " 46379/50000: episode: 974, duration: 0.177s, episode steps: 29, steps per second: 164, episode reward: 0.942, mean reward: 0.032 [-0.003, 1.000], mean action: 1.241 [0.000, 2.000], mean observation: -0.068 [-0.293, 0.100], loss: 0.000052, mean_absolute_error: 0.466291, mean_q: 0.693776\n",
      " 46380/50000: episode: 975, duration: 0.011s, episode steps: 1, steps per second: 91, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.035 [-0.010, 0.080], loss: 0.000071, mean_absolute_error: 0.456062, mean_q: 0.686412\n",
      " 46408/50000: episode: 976, duration: 0.147s, episode steps: 28, steps per second: 190, episode reward: 0.946, mean reward: 0.034 [-0.003, 1.000], mean action: 0.679 [0.000, 2.000], mean observation: 0.065 [-0.100, 0.269], loss: 0.000050, mean_absolute_error: 0.475140, mean_q: 0.705694\n",
      " 46468/50000: episode: 977, duration: 0.333s, episode steps: 60, steps per second: 180, episode reward: 0.690, mean reward: 0.011 [-0.009, 1.000], mean action: 1.133 [0.000, 2.000], mean observation: -0.188 [-0.942, 0.250], loss: 0.000042, mean_absolute_error: 0.468034, mean_q: 0.695104\n",
      " 46517/50000: episode: 978, duration: 0.267s, episode steps: 49, steps per second: 184, episode reward: 0.841, mean reward: 0.017 [-0.006, 1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.115 [-0.570, 0.160], loss: 0.000063, mean_absolute_error: 0.466043, mean_q: 0.692542\n",
      " 46583/50000: episode: 979, duration: 0.347s, episode steps: 66, steps per second: 190, episode reward: 0.745, mean reward: 0.011 [-0.007, 1.000], mean action: 1.061 [0.000, 2.000], mean observation: -0.145 [-0.738, 0.190], loss: 0.000049, mean_absolute_error: 0.470597, mean_q: 0.699797\n",
      " 46637/50000: episode: 980, duration: 0.304s, episode steps: 54, steps per second: 178, episode reward: 0.818, mean reward: 0.015 [-0.006, 1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.125 [-0.579, 0.150], loss: 0.000311, mean_absolute_error: 0.469328, mean_q: 0.693394\n",
      " 46670/50000: episode: 981, duration: 0.176s, episode steps: 33, steps per second: 188, episode reward: 0.928, mean reward: 0.028 [-0.003, 1.000], mean action: 1.212 [0.000, 2.000], mean observation: -0.074 [-0.341, 0.110], loss: 0.000075, mean_absolute_error: 0.476391, mean_q: 0.706090\n",
      " 46675/50000: episode: 982, duration: 0.060s, episode steps: 5, steps per second: 83, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.037 [-0.050, 0.109], loss: 0.000056, mean_absolute_error: 0.453563, mean_q: 0.679639\n",
      " 46729/50000: episode: 983, duration: 0.326s, episode steps: 54, steps per second: 165, episode reward: 0.819, mean reward: 0.015 [-0.006, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.123 [-0.584, 0.140], loss: 0.000164, mean_absolute_error: 0.470004, mean_q: 0.696012\n",
      " 46780/50000: episode: 984, duration: 0.307s, episode steps: 51, steps per second: 166, episode reward: 0.744, mean reward: 0.015 [-0.009, 1.000], mean action: 1.157 [0.000, 2.000], mean observation: -0.177 [-0.858, 0.240], loss: 0.000051, mean_absolute_error: 0.470965, mean_q: 0.700074\n",
      " 46824/50000: episode: 985, duration: 0.301s, episode steps: 44, steps per second: 146, episode reward: 0.843, mean reward: 0.019 [-0.006, 1.000], mean action: 0.795 [0.000, 2.000], mean observation: 0.122 [-0.200, 0.603], loss: 0.000040, mean_absolute_error: 0.466827, mean_q: 0.696639\n",
      " 46825/50000: episode: 986, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.012 [-0.010, 0.034], loss: 0.000046, mean_absolute_error: 0.479859, mean_q: 0.724622\n",
      " 46883/50000: episode: 987, duration: 0.363s, episode steps: 58, steps per second: 160, episode reward: 0.733, mean reward: 0.013 [-0.008, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.167 [-0.220, 0.831], loss: 0.000053, mean_absolute_error: 0.467783, mean_q: 0.697266\n",
      " 46911/50000: episode: 988, duration: 0.207s, episode steps: 28, steps per second: 135, episode reward: 0.935, mean reward: 0.033 [-0.003, 1.000], mean action: 0.714 [0.000, 2.000], mean observation: 0.073 [-0.120, 0.346], loss: 0.000042, mean_absolute_error: 0.461344, mean_q: 0.682958\n",
      " 46955/50000: episode: 989, duration: 0.397s, episode steps: 44, steps per second: 111, episode reward: 0.863, mean reward: 0.020 [-0.005, 1.000], mean action: 0.841 [0.000, 2.000], mean observation: 0.109 [-0.140, 0.515], loss: 0.000103, mean_absolute_error: 0.477408, mean_q: 0.708555\n",
      " 46983/50000: episode: 990, duration: 0.161s, episode steps: 28, steps per second: 174, episode reward: 0.944, mean reward: 0.034 [-0.003, 1.000], mean action: 1.286 [0.000, 2.000], mean observation: -0.066 [-0.296, 0.100], loss: 0.000042, mean_absolute_error: 0.477243, mean_q: 0.711140\n",
      " 47021/50000: episode: 991, duration: 0.211s, episode steps: 38, steps per second: 180, episode reward: 0.909, mean reward: 0.024 [-0.004, 1.000], mean action: 1.211 [0.000, 2.000], mean observation: -0.085 [-0.370, 0.100], loss: 0.000050, mean_absolute_error: 0.468300, mean_q: 0.697519\n",
      " 47027/50000: episode: 992, duration: 0.044s, episode steps: 6, steps per second: 136, episode reward: 0.995, mean reward: 0.166 [-0.001, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.038 [-0.050, 0.111], loss: 0.000039, mean_absolute_error: 0.462630, mean_q: 0.695804\n",
      " 47078/50000: episode: 993, duration: 0.283s, episode steps: 51, steps per second: 180, episode reward: 0.806, mean reward: 0.016 [-0.007, 1.000], mean action: 1.157 [0.000, 2.000], mean observation: -0.133 [-0.679, 0.210], loss: 0.000217, mean_absolute_error: 0.473685, mean_q: 0.703992\n",
      " 47251/50000: episode: 994, duration: 0.947s, episode steps: 173, steps per second: 183, episode reward: 0.034, mean reward: 0.000 [-0.010, 1.000], mean action: 0.948 [0.000, 2.000], mean observation: 0.152 [-0.862, 1.000], loss: 0.000132, mean_absolute_error: 0.471484, mean_q: 0.700770\n",
      " 47297/50000: episode: 995, duration: 0.258s, episode steps: 46, steps per second: 178, episode reward: 0.853, mean reward: 0.019 [-0.005, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.115 [-0.140, 0.514], loss: 0.000045, mean_absolute_error: 0.475514, mean_q: 0.707834\n",
      " 47303/50000: episode: 996, duration: 0.035s, episode steps: 6, steps per second: 169, episode reward: 0.995, mean reward: 0.166 [-0.001, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.042 [-0.030, 0.109], loss: 0.000032, mean_absolute_error: 0.473054, mean_q: 0.705844\n",
      " 47358/50000: episode: 997, duration: 0.319s, episode steps: 55, steps per second: 173, episode reward: 0.795, mean reward: 0.014 [-0.007, 1.000], mean action: 1.109 [0.000, 2.000], mean observation: -0.135 [-0.668, 0.180], loss: 0.000038, mean_absolute_error: 0.472106, mean_q: 0.702858\n",
      " 47416/50000: episode: 998, duration: 0.352s, episode steps: 58, steps per second: 165, episode reward: 0.728, mean reward: 0.013 [-0.008, 1.000], mean action: 0.845 [0.000, 2.000], mean observation: 0.172 [-0.230, 0.836], loss: 0.000290, mean_absolute_error: 0.466849, mean_q: 0.691355\n",
      " 47475/50000: episode: 999, duration: 0.370s, episode steps: 59, steps per second: 159, episode reward: 0.686, mean reward: 0.012 [-0.009, 1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.195 [-0.945, 0.280], loss: 0.000040, mean_absolute_error: 0.467108, mean_q: 0.695954\n",
      " 47489/50000: episode: 1000, duration: 0.085s, episode steps: 14, steps per second: 164, episode reward: 0.980, mean reward: 0.070 [-0.002, 1.000], mean action: 0.500 [0.000, 2.000], mean observation: 0.041 [-0.100, 0.183], loss: 0.000153, mean_absolute_error: 0.469682, mean_q: 0.690507\n",
      " 47490/50000: episode: 1001, duration: 0.009s, episode steps: 1, steps per second: 117, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [-0.010, 0.061], loss: 0.000042, mean_absolute_error: 0.492575, mean_q: 0.744536\n",
      " 47545/50000: episode: 1002, duration: 0.278s, episode steps: 55, steps per second: 198, episode reward: 0.823, mean reward: 0.015 [-0.006, 1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.119 [-0.570, 0.140], loss: 0.000036, mean_absolute_error: 0.467609, mean_q: 0.695530\n",
      " 47567/50000: episode: 1003, duration: 0.133s, episode steps: 22, steps per second: 166, episode reward: 0.964, mean reward: 0.044 [-0.002, 1.000], mean action: 1.364 [0.000, 2.000], mean observation: -0.053 [-0.231, 0.090], loss: 0.000260, mean_absolute_error: 0.461228, mean_q: 0.685352\n",
      " 47618/50000: episode: 1004, duration: 0.292s, episode steps: 51, steps per second: 175, episode reward: 0.774, mean reward: 0.015 [-0.008, 1.000], mean action: 0.824 [0.000, 2.000], mean observation: 0.156 [-0.220, 0.763], loss: 0.000110, mean_absolute_error: 0.476533, mean_q: 0.709305\n",
      " 47665/50000: episode: 1005, duration: 0.279s, episode steps: 47, steps per second: 168, episode reward: 0.860, mean reward: 0.018 [-0.005, 1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.107 [-0.499, 0.130], loss: 0.000041, mean_absolute_error: 0.468267, mean_q: 0.701649\n",
      " 47688/50000: episode: 1006, duration: 0.137s, episode steps: 23, steps per second: 168, episode reward: 0.957, mean reward: 0.042 [-0.003, 1.000], mean action: 1.391 [0.000, 2.000], mean observation: -0.057 [-0.262, 0.100], loss: 0.000047, mean_absolute_error: 0.467050, mean_q: 0.698850\n",
      " 47754/50000: episode: 1007, duration: 0.376s, episode steps: 66, steps per second: 176, episode reward: 0.693, mean reward: 0.010 [-0.008, 1.000], mean action: 0.864 [0.000, 2.000], mean observation: 0.180 [-0.170, 0.804], loss: 0.000110, mean_absolute_error: 0.465857, mean_q: 0.694147\n",
      " 47755/50000: episode: 1008, duration: 0.011s, episode steps: 1, steps per second: 87, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.014 [-0.010, 0.039], loss: 0.000022, mean_absolute_error: 0.450046, mean_q: 0.682412\n",
      " 47809/50000: episode: 1009, duration: 0.308s, episode steps: 54, steps per second: 175, episode reward: 0.762, mean reward: 0.014 [-0.008, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.158 [-0.773, 0.230], loss: 0.000029, mean_absolute_error: 0.468674, mean_q: 0.695240\n",
      " 47865/50000: episode: 1010, duration: 0.316s, episode steps: 56, steps per second: 177, episode reward: 0.737, mean reward: 0.013 [-0.008, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.171 [-0.230, 0.829], loss: 0.000115, mean_absolute_error: 0.466287, mean_q: 0.689567\n",
      " 47866/50000: episode: 1011, duration: 0.011s, episode steps: 1, steps per second: 90, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.003 [-0.010, 0.016], loss: 0.000039, mean_absolute_error: 0.465920, mean_q: 0.701562\n",
      " 47867/50000: episode: 1012, duration: 0.010s, episode steps: 1, steps per second: 96, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.044 [-0.010, 0.097], loss: 0.000025, mean_absolute_error: 0.462383, mean_q: 0.682270\n",
      " 47924/50000: episode: 1013, duration: 0.300s, episode steps: 57, steps per second: 190, episode reward: 0.738, mean reward: 0.013 [-0.008, 1.000], mean action: 1.123 [0.000, 2.000], mean observation: -0.167 [-0.826, 0.220], loss: 0.000058, mean_absolute_error: 0.464157, mean_q: 0.690248\n",
      " 47937/50000: episode: 1014, duration: 0.077s, episode steps: 13, steps per second: 170, episode reward: 0.983, mean reward: 0.076 [-0.002, 1.000], mean action: 1.538 [0.000, 2.000], mean observation: -0.042 [-0.170, 0.080], loss: 0.000040, mean_absolute_error: 0.472905, mean_q: 0.705899\n",
      " 47987/50000: episode: 1015, duration: 0.278s, episode steps: 50, steps per second: 180, episode reward: 0.825, mean reward: 0.017 [-0.006, 1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.123 [-0.611, 0.160], loss: 0.000031, mean_absolute_error: 0.471085, mean_q: 0.704076\n",
      " 48046/50000: episode: 1016, duration: 0.318s, episode steps: 59, steps per second: 186, episode reward: 0.725, mean reward: 0.012 [-0.008, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.169 [-0.240, 0.838], loss: 0.000036, mean_absolute_error: 0.465506, mean_q: 0.692749\n",
      " 48067/50000: episode: 1017, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 0.961, mean reward: 0.046 [-0.003, 1.000], mean action: 0.619 [0.000, 2.000], mean observation: 0.055 [-0.120, 0.261], loss: 0.000025, mean_absolute_error: 0.463677, mean_q: 0.693765\n",
      " 48099/50000: episode: 1018, duration: 0.177s, episode steps: 32, steps per second: 181, episode reward: 0.914, mean reward: 0.029 [-0.004, 1.000], mean action: 0.719 [0.000, 2.000], mean observation: 0.086 [-0.150, 0.413], loss: 0.000043, mean_absolute_error: 0.466202, mean_q: 0.697743\n",
      " 48152/50000: episode: 1019, duration: 0.299s, episode steps: 53, steps per second: 177, episode reward: 0.708, mean reward: 0.013 [-0.009, 1.000], mean action: 1.151 [0.000, 2.000], mean observation: -0.201 [-0.900, 0.270], loss: 0.000031, mean_absolute_error: 0.469286, mean_q: 0.701952\n",
      " 48186/50000: episode: 1020, duration: 0.193s, episode steps: 34, steps per second: 176, episode reward: 0.907, mean reward: 0.027 [-0.004, 1.000], mean action: 0.765 [0.000, 2.000], mean observation: 0.089 [-0.150, 0.429], loss: 0.000025, mean_absolute_error: 0.472407, mean_q: 0.709899\n",
      " 48218/50000: episode: 1021, duration: 0.183s, episode steps: 32, steps per second: 175, episode reward: 0.927, mean reward: 0.029 [-0.004, 1.000], mean action: 0.812 [0.000, 2.000], mean observation: 0.075 [-0.140, 0.363], loss: 0.000176, mean_absolute_error: 0.464475, mean_q: 0.692117\n",
      " 48257/50000: episode: 1022, duration: 0.224s, episode steps: 39, steps per second: 174, episode reward: 0.868, mean reward: 0.022 [-0.005, 1.000], mean action: 0.769 [0.000, 2.000], mean observation: 0.112 [-0.190, 0.550], loss: 0.000036, mean_absolute_error: 0.468892, mean_q: 0.699424\n",
      " 48300/50000: episode: 1023, duration: 0.248s, episode steps: 43, steps per second: 173, episode reward: 0.861, mean reward: 0.020 [-0.005, 1.000], mean action: 0.791 [0.000, 2.000], mean observation: 0.110 [-0.190, 0.537], loss: 0.000038, mean_absolute_error: 0.469990, mean_q: 0.699876\n",
      " 48355/50000: episode: 1024, duration: 0.303s, episode steps: 55, steps per second: 182, episode reward: 0.743, mean reward: 0.014 [-0.008, 1.000], mean action: 0.855 [0.000, 2.000], mean observation: 0.168 [-0.230, 0.828], loss: 0.000027, mean_absolute_error: 0.472041, mean_q: 0.702309\n",
      " 48414/50000: episode: 1025, duration: 0.322s, episode steps: 59, steps per second: 183, episode reward: 0.708, mean reward: 0.012 [-0.009, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.183 [-0.240, 0.863], loss: 0.000272, mean_absolute_error: 0.469989, mean_q: 0.699877\n",
      " 48468/50000: episode: 1026, duration: 0.290s, episode steps: 54, steps per second: 186, episode reward: 0.783, mean reward: 0.015 [-0.007, 1.000], mean action: 0.852 [0.000, 2.000], mean observation: 0.150 [-0.200, 0.654], loss: 0.000124, mean_absolute_error: 0.474663, mean_q: 0.708057\n",
      " 48508/50000: episode: 1027, duration: 0.231s, episode steps: 40, steps per second: 173, episode reward: 0.887, mean reward: 0.022 [-0.004, 1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.099 [-0.435, 0.140], loss: 0.000035, mean_absolute_error: 0.467223, mean_q: 0.700827\n",
      " 48555/50000: episode: 1028, duration: 0.262s, episode steps: 47, steps per second: 180, episode reward: 0.863, mean reward: 0.018 [-0.005, 1.000], mean action: 1.191 [0.000, 2.000], mean observation: -0.089 [-0.540, 0.170], loss: 0.000253, mean_absolute_error: 0.463968, mean_q: 0.691376\n",
      " 48592/50000: episode: 1029, duration: 0.221s, episode steps: 37, steps per second: 168, episode reward: 0.905, mean reward: 0.024 [-0.004, 1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.083 [-0.410, 0.150], loss: 0.000058, mean_absolute_error: 0.469066, mean_q: 0.699538\n",
      " 48646/50000: episode: 1030, duration: 0.305s, episode steps: 54, steps per second: 177, episode reward: 0.731, mean reward: 0.014 [-0.009, 1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.179 [-0.861, 0.260], loss: 0.000035, mean_absolute_error: 0.467903, mean_q: 0.698259\n",
      " 48679/50000: episode: 1031, duration: 0.190s, episode steps: 33, steps per second: 174, episode reward: 0.920, mean reward: 0.028 [-0.004, 1.000], mean action: 1.242 [0.000, 2.000], mean observation: -0.079 [-0.377, 0.120], loss: 0.000036, mean_absolute_error: 0.470562, mean_q: 0.700644\n",
      " 48725/50000: episode: 1032, duration: 0.274s, episode steps: 46, steps per second: 168, episode reward: 0.835, mean reward: 0.018 [-0.006, 1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.125 [-0.595, 0.150], loss: 0.000332, mean_absolute_error: 0.469227, mean_q: 0.696903\n",
      " 48774/50000: episode: 1033, duration: 0.283s, episode steps: 49, steps per second: 173, episode reward: 0.831, mean reward: 0.017 [-0.006, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.124 [-0.150, 0.571], loss: 0.000045, mean_absolute_error: 0.472373, mean_q: 0.703152\n",
      " 48837/50000: episode: 1034, duration: 0.347s, episode steps: 63, steps per second: 182, episode reward: 0.668, mean reward: 0.011 [-0.009, 1.000], mean action: 0.857 [0.000, 2.000], mean observation: 0.200 [-0.240, 0.906], loss: 0.000029, mean_absolute_error: 0.470385, mean_q: 0.699218\n",
      " 48870/50000: episode: 1035, duration: 0.194s, episode steps: 33, steps per second: 170, episode reward: 0.913, mean reward: 0.028 [-0.004, 1.000], mean action: 0.758 [0.000, 2.000], mean observation: 0.085 [-0.150, 0.410], loss: 0.000030, mean_absolute_error: 0.465608, mean_q: 0.696104\n",
      " 48929/50000: episode: 1036, duration: 0.331s, episode steps: 59, steps per second: 179, episode reward: 0.717, mean reward: 0.012 [-0.008, 1.000], mean action: 0.847 [0.000, 2.000], mean observation: 0.182 [-0.220, 0.773], loss: 0.000302, mean_absolute_error: 0.466237, mean_q: 0.694362\n",
      " 48930/50000: episode: 1037, duration: 0.011s, episode steps: 1, steps per second: 93, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.034 [-0.077, 0.010], loss: 0.000030, mean_absolute_error: 0.470938, mean_q: 0.714128\n",
      " 48978/50000: episode: 1038, duration: 0.282s, episode steps: 48, steps per second: 170, episode reward: 0.819, mean reward: 0.017 [-0.007, 1.000], mean action: 0.833 [0.000, 2.000], mean observation: 0.131 [-0.200, 0.654], loss: 0.000031, mean_absolute_error: 0.467363, mean_q: 0.698127\n",
      " 48979/50000: episode: 1039, duration: 0.013s, episode steps: 1, steps per second: 75, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.025 [-0.010, 0.059], loss: 0.000018, mean_absolute_error: 0.480573, mean_q: 0.710567\n",
      " 49010/50000: episode: 1040, duration: 0.180s, episode steps: 31, steps per second: 172, episode reward: 0.938, mean reward: 0.030 [-0.003, 1.000], mean action: 0.710 [0.000, 2.000], mean observation: 0.067 [-0.110, 0.304], loss: 0.000509, mean_absolute_error: 0.469010, mean_q: 0.700012\n",
      " 49064/50000: episode: 1041, duration: 0.321s, episode steps: 54, steps per second: 168, episode reward: 0.749, mean reward: 0.014 [-0.008, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.164 [-0.813, 0.240], loss: 0.000039, mean_absolute_error: 0.469610, mean_q: 0.698825\n",
      " 49099/50000: episode: 1042, duration: 0.219s, episode steps: 35, steps per second: 160, episode reward: 0.912, mean reward: 0.026 [-0.004, 1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.078 [-0.409, 0.140], loss: 0.000031, mean_absolute_error: 0.464819, mean_q: 0.689819\n",
      " 49145/50000: episode: 1043, duration: 0.351s, episode steps: 46, steps per second: 131, episode reward: 0.822, mean reward: 0.018 [-0.006, 1.000], mean action: 0.804 [0.000, 2.000], mean observation: 0.136 [-0.190, 0.631], loss: 0.000081, mean_absolute_error: 0.472599, mean_q: 0.704887\n",
      " 49146/50000: episode: 1044, duration: 0.017s, episode steps: 1, steps per second: 58, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.008 [0.006, 0.010], loss: 0.000080, mean_absolute_error: 0.474708, mean_q: 0.687338\n",
      " 49183/50000: episode: 1045, duration: 0.220s, episode steps: 37, steps per second: 168, episode reward: 0.902, mean reward: 0.024 [-0.004, 1.000], mean action: 1.189 [0.000, 2.000], mean observation: -0.091 [-0.414, 0.110], loss: 0.000342, mean_absolute_error: 0.464322, mean_q: 0.689714\n",
      " 49237/50000: episode: 1046, duration: 0.369s, episode steps: 54, steps per second: 146, episode reward: 0.737, mean reward: 0.014 [-0.008, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.173 [-0.849, 0.260], loss: 0.000033, mean_absolute_error: 0.471696, mean_q: 0.704563\n",
      " 49275/50000: episode: 1047, duration: 0.228s, episode steps: 38, steps per second: 167, episode reward: 0.893, mean reward: 0.023 [-0.004, 1.000], mean action: 1.237 [0.000, 2.000], mean observation: -0.098 [-0.430, 0.130], loss: 0.000046, mean_absolute_error: 0.474314, mean_q: 0.709104\n",
      " 49302/50000: episode: 1048, duration: 0.157s, episode steps: 27, steps per second: 171, episode reward: 0.940, mean reward: 0.035 [-0.003, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.068 [-0.329, 0.130], loss: 0.000041, mean_absolute_error: 0.471302, mean_q: 0.699906\n",
      " 49303/50000: episode: 1049, duration: 0.011s, episode steps: 1, steps per second: 88, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.005 [-0.010, 0.020], loss: 0.000011, mean_absolute_error: 0.454660, mean_q: 0.671219\n",
      " 49308/50000: episode: 1050, duration: 0.032s, episode steps: 5, steps per second: 154, episode reward: 0.996, mean reward: 0.199 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.037 [-0.109, 0.050], loss: 0.000046, mean_absolute_error: 0.483400, mean_q: 0.727931\n",
      " 49340/50000: episode: 1051, duration: 0.204s, episode steps: 32, steps per second: 157, episode reward: 0.926, mean reward: 0.029 [-0.004, 1.000], mean action: 1.281 [0.000, 2.000], mean observation: -0.073 [-0.359, 0.120], loss: 0.000024, mean_absolute_error: 0.467931, mean_q: 0.697085\n",
      " 49398/50000: episode: 1052, duration: 0.355s, episode steps: 58, steps per second: 164, episode reward: 0.698, mean reward: 0.012 [-0.009, 1.000], mean action: 0.862 [0.000, 2.000], mean observation: 0.192 [-0.240, 0.908], loss: 0.000210, mean_absolute_error: 0.474266, mean_q: 0.706542\n",
      " 49441/50000: episode: 1053, duration: 0.304s, episode steps: 43, steps per second: 142, episode reward: 0.854, mean reward: 0.020 [-0.006, 1.000], mean action: 0.814 [0.000, 2.000], mean observation: 0.118 [-0.190, 0.555], loss: 0.000031, mean_absolute_error: 0.474871, mean_q: 0.710270\n",
      " 49489/50000: episode: 1054, duration: 0.320s, episode steps: 48, steps per second: 150, episode reward: 0.817, mean reward: 0.017 [-0.007, 1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.131 [-0.663, 0.190], loss: 0.000261, mean_absolute_error: 0.474611, mean_q: 0.708606\n",
      " 49490/50000: episode: 1055, duration: 0.014s, episode steps: 1, steps per second: 72, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.001 [0.000, 0.002], loss: 0.000015, mean_absolute_error: 0.458583, mean_q: 0.696217\n",
      " 49540/50000: episode: 1056, duration: 0.439s, episode steps: 50, steps per second: 114, episode reward: 0.776, mean reward: 0.016 [-0.008, 1.000], mean action: 0.840 [0.000, 2.000], mean observation: 0.158 [-0.210, 0.763], loss: 0.000066, mean_absolute_error: 0.468793, mean_q: 0.699006\n",
      " 49593/50000: episode: 1057, duration: 0.319s, episode steps: 53, steps per second: 166, episode reward: 0.745, mean reward: 0.014 [-0.009, 1.000], mean action: 1.151 [0.000, 2.000], mean observation: -0.170 [-0.851, 0.260], loss: 0.000033, mean_absolute_error: 0.476903, mean_q: 0.712201\n",
      " 49647/50000: episode: 1058, duration: 0.324s, episode steps: 54, steps per second: 167, episode reward: 0.742, mean reward: 0.014 [-0.008, 1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.170 [-0.849, 0.260], loss: 0.000036, mean_absolute_error: 0.478300, mean_q: 0.714986\n",
      " 49679/50000: episode: 1059, duration: 0.169s, episode steps: 32, steps per second: 190, episode reward: 0.919, mean reward: 0.029 [-0.004, 1.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.083 [-0.140, 0.382], loss: 0.000080, mean_absolute_error: 0.486332, mean_q: 0.719967\n",
      " 49731/50000: episode: 1060, duration: 0.335s, episode steps: 52, steps per second: 155, episode reward: 0.781, mean reward: 0.015 [-0.007, 1.000], mean action: 0.846 [0.000, 2.000], mean observation: 0.151 [-0.200, 0.730], loss: 0.000032, mean_absolute_error: 0.473857, mean_q: 0.709622\n",
      " 49771/50000: episode: 1061, duration: 0.216s, episode steps: 40, steps per second: 185, episode reward: 0.880, mean reward: 0.022 [-0.005, 1.000], mean action: 0.800 [0.000, 2.000], mean observation: 0.102 [-0.170, 0.499], loss: 0.000033, mean_absolute_error: 0.475999, mean_q: 0.711412\n",
      " 49812/50000: episode: 1062, duration: 0.276s, episode steps: 41, steps per second: 149, episode reward: 0.858, mean reward: 0.021 [-0.006, 1.000], mean action: 0.805 [0.000, 2.000], mean observation: 0.117 [-0.190, 0.571], loss: 0.000060, mean_absolute_error: 0.472891, mean_q: 0.705809\n",
      " 49841/50000: episode: 1063, duration: 0.195s, episode steps: 29, steps per second: 148, episode reward: 0.940, mean reward: 0.032 [-0.003, 1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.061 [-0.319, 0.120], loss: 0.000050, mean_absolute_error: 0.477726, mean_q: 0.711820\n",
      " 49847/50000: episode: 1064, duration: 0.040s, episode steps: 6, steps per second: 150, episode reward: 0.995, mean reward: 0.166 [-0.001, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: -0.036 [-0.116, 0.060], loss: 0.000025, mean_absolute_error: 0.469607, mean_q: 0.700135\n",
      " 49900/50000: episode: 1065, duration: 0.352s, episode steps: 53, steps per second: 151, episode reward: 0.761, mean reward: 0.014 [-0.007, 1.000], mean action: 0.830 [0.000, 2.000], mean observation: 0.164 [-0.220, 0.748], loss: 0.000023, mean_absolute_error: 0.476829, mean_q: 0.709886\n",
      " 49924/50000: episode: 1066, duration: 0.159s, episode steps: 24, steps per second: 151, episode reward: 0.953, mean reward: 0.040 [-0.003, 1.000], mean action: 0.625 [0.000, 2.000], mean observation: 0.060 [-0.120, 0.273], loss: 0.000028, mean_absolute_error: 0.483743, mean_q: 0.723598\n",
      " 49961/50000: episode: 1067, duration: 0.319s, episode steps: 37, steps per second: 116, episode reward: 0.896, mean reward: 0.024 [-0.005, 1.000], mean action: 0.757 [0.000, 2.000], mean observation: 0.091 [-0.170, 0.462], loss: 0.000023, mean_absolute_error: 0.482058, mean_q: 0.721956\n",
      " 49962/50000: episode: 1068, duration: 0.015s, episode steps: 1, steps per second: 66, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.040 [-0.010, 0.090], loss: 0.000020, mean_absolute_error: 0.479001, mean_q: 0.723698\n",
      " 49990/50000: episode: 1069, duration: 0.214s, episode steps: 28, steps per second: 131, episode reward: 0.937, mean reward: 0.033 [-0.003, 1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.076 [-0.303, 0.130], loss: 0.000028, mean_absolute_error: 0.479183, mean_q: 0.716749\n",
      "done, took 285.673 seconds\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "env = PointOnLine()\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# DQNのネットワーク定義\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# experience replay用のmemory\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "# 行動方策はオーソドックスなepsilon-greedy。ほかに、各行動のQ値によって確率を決定するBoltzmannQPolicyが利用可能\n",
    "policy = EpsGreedyQPolicy(eps=0.1) \n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2, nb_max_episode_steps=300)\n",
    "#学習の様子を描画したいときは、Envに_render()を実装して、visualize=True にします,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 0.995, steps: 6\n",
      "Episode 2: reward: 0.715, steps: 60\n",
      "Episode 3: reward: 0.778, steps: 53\n",
      "Episode 4: reward: 0.839, steps: 41\n",
      "Episode 5: reward: 0.844, steps: 46\n",
      "Episode 6: reward: 0.678, steps: 62\n",
      "Episode 7: reward: 0.906, steps: 33\n",
      "Episode 8: reward: 1.000, steps: 1\n",
      "Episode 9: reward: 0.889, steps: 39\n",
      "Episode 10: reward: 0.958, steps: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f5072be7e10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAF5CAYAAACxwgF3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5072883470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import rl.callbacks\n",
    "class EpisodeLogger(rl.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.observations = {}\n",
    "        self.rewards = {}\n",
    "        self.actions = {}\n",
    "\n",
    "    def on_episode_begin(self, episode, logs):\n",
    "        self.observations[episode] = []\n",
    "        self.rewards[episode] = []\n",
    "        self.actions[episode] = []\n",
    "\n",
    "    def on_step_end(self, step, logs):\n",
    "        episode = logs['episode']\n",
    "        self.observations[episode].append(logs['observation'])\n",
    "        self.rewards[episode].append(logs['reward'])\n",
    "        self.actions[episode].append(logs['action'])\n",
    "\n",
    "cb_ep = EpisodeLogger()\n",
    "dqn.test(env, nb_episodes=10, visualize=False, callbacks=[cb_ep])\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for obs in cb_ep.observations.values():\n",
    "    plt.plot([o[0] for o in obs])\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
